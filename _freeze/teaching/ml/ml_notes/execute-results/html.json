{
  "hash": "03d4622ee24a0f7e95d69f6b28e74e10",
  "result": {
    "markdown": "---\ntitle: 'Workshop on ML for materials science'\ndescription: <i>Some notes to accompany the ML workshop.</i>\nimage: \"\"\nsidebar: false\nexecute:\n  freeze: true\n---\n\n## Motivation \n\nTo design new materials, we need to know their properties. \nThere are two main routes to get the properties of a material: \n\n1. Perform an experiment to measure them \n2. Perform a simulation to “measure” them in silico \n\nIn many cases, performing an experiment is time-consuming and, hence, expensive. Also high-fidelity simulations can be very costly. \n[Fidelity expresses the exactness with which a surrogate represents the truth. In the context of ML you might also see the term multi-fidelity, which means that the approach uses multiple approximations with different levels of fidelity, e.g. density-functional theory and coupled cluster theory]{.aside}\n\nTherefore, there is a need for methods that can help us to predict the properties of materials with high fidelity and low cost. In this lecture, we will see that _supervised machine learning_ (ML) is a powerful tool to achieve this goal.\n\nInterestingly, this tool can be used in many different ways.\n\n### Where does ML fit in the design process?\n\nMachine learning can be used in multiple ways to make high-fidelity predictions of materials less expensive.\n[Note that reducing the cost has been a challenge for chemists and material scientists for a long time. Dirac famously said \"The fundamental laws necessary for the mathematical treatment of a large part of physics and the whole of chemistry are thus completely known, and the difficulty lies only in the fact that application of these laws leads to equations that are too complex to be solved. [...] approximate practical methods of applying quantum mechanics should be developed, which can lead to an explanation of the main features of complex atomic systems without too much computation\"]{.aside}\n\n![Machine learning (green boxes) can be used at multiple places in the material design process.](images/ml_design_process.png)\n\n1. _Replace expensive evaluation of the potential energy surface $U(\\mathbf{X}, \\{\\mathbf{Z}\\})$_: Quantum chemistry as a field is concerned with the prediction of the potential energy surface $U(\\mathbf{X}, \\{\\mathbf{Z}\\})$ of a system of atoms of types $\\mathbf{Z}$ at positions $\\mathbf{X}$. Quantum chemists have developed different approximations to this problem. However, since they are all kinds of functions that map positions of atoms (and atom types, and in some cases electron densities/coordinates) to energies, we can learn those functions with ML.\n\n    Note that once we have done that, we generally still need to perform simulations to extract the properties of interest (e.g. as ensemble averages).\n\n    There are many good review articles about this. For example, see [this one by Unke et al.](https://pubs.acs.org/doi/10.1021/acs.chemrev.0c01111) as well as the ones by [Deringer et al.](https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00022) and [Behler](https://pubs.acs.org/doi/10.1021/acs.chemrev.0c00868) in the same [issue of Chemical Reviews](https://pubs.acs.org/toc/chreay/121/16).\n\n2. _Directly predict the properties of interest_ Instead of computing the properties of interest using a molecular simulations, we can build models that learn the $f(\\mathrm{structure}) \\to \\mathrm{property}$ mapping directly. The basis for this mapping might be experimental data or high-fidelity computational data.\n\n    Also about this approach, there are many review articles. I also wrote [one](https://pubs.acs.org/doi/10.1021/acs.chemrev.0c00004), focussing on porous materials.\n\nNote that in the context of using ML for molecular simulations, it can also be used to address sampling problems. \nWe will not cover this in detail in this lecture. For a good introduction, see the [seminal paper by Noe](https://www.science.org/doi/10.1126/science.aaw1147) and a [piece about it by Tuckerman](https://www.science.org/doi/10.1126/science.aay2568).\n\n\n## Supervised ML workflow\n\n![The supervised ML workflow.](images/ml_workflow.png)\n\nFor the main part of this lecture, we will assume that we use models that consume so-called tabular data, i.e. data that is stored in a table (feature matrix $\\mathbf{X}$ and target/label vector/matrix $\\mathbf{Y}$), where each row corresponds to a material and each of the $p$ columns corresponds to a so-called feature. We wil later see that this is not the only way to use ML for materials science, but it is the most common one. We will also explore in more detail how we obtain the features.\n\nWe will use some data $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$ to train a model $f(\\mathbf{x}) \\to y$ that can predict the target $y$ for a new structure described with the feature vector $\\mathbf{x}^*$.\n\n## Feeding structures into models \n\n### Incorporating symmetries/invariances/equivariances\n\n#### Learning a very simple force field \n\nTo understand what it takes to feed structures into ML models, let us try to build a very simple force field. To make things simple and fast, we will just attempt to predict the energies of different conformers of the same molecule.\n\nWe will create some data using [RDkit](https://rdkit.readthedocs.io) and then use [scikit-learn](https://scikit-learn.org/stable/) to train a model. \n\n##### Generating data\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pymatviz.parity import density_scatter_with_hist\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, PyMol\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport matplotx\nplt.style.use(['science', 'nature', matplotx.styles.dufte])\n\ndef gen_conformers(mol, numConfs=10_000, maxAttempts=1000, \n    pruneRmsThresh=0.2, useExpTorsionAnglePrefs=True, \n    useBasicKnowledge=True, enforceChirality=True):\n    \"\"\"Use RDkit to generate conformers for a molecule.\"\"\"\n    ids = AllChem.EmbedMultipleConfs(mol, numConfs=numConfs, maxAttempts=maxAttempts, pruneRmsThresh=pruneRmsThresh, useExpTorsionAnglePrefs=useExpTorsionAnglePrefs, useBasicKnowledge=useBasicKnowledge, enforceChirality=enforceChirality, numThreads=0)\n    return list(ids)\n\ndef calc_energy(mol, conformer_id, iterations=0):\n    \"\"\"Calculate the energy of a conformer using the Merck Molecular Force Field.\"\"\"\n    ff = AllChem.MMFFGetMoleculeForceField(mol, AllChem.MMFFGetMoleculeProperties(mol), confId=conformer_id)\n    ff.Initialize()\n    ff.CalcEnergy()\n    results = {}\n    if iterations > 0:\n        results[\"converged\"] = ff.Minimize(maxIts=iterations)\n    results[\"energy_abs\"] = ff.CalcEnergy()\n    return results\n\n# create a molecule\nmol = Chem.AddHs(Chem.MolFromSmiles('CC(CCC)CC(C)(CCCC)O'))\n\n# visualize some conformers using PyMol\nconformer_ids = gen_conformers(mol)\nv= PyMol.MolViewer()\nv.DeleteAll()\nfor cid in conformer_ids[:50]: \n    v.ShowMol(mol,confId=cid,name='Conf-%d'%cid,showOnly=False)\nv.server.do('set grid_mode, on')\nv.server.do('ray')\nv.GetPNG()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](ml_notes_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nFor those conformers, we can now retrieve the positions and energies and save them in a pandas dataframe. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# make column names\ncoordinate_names = sum([[f'x_{n}',f'y_{n}', f'z_{n}'] for n in range(mol.GetNumAtoms())], []) \n\n# make a dataframe\ndata = []\nfor conformer_id in conformer_ids:\n    energy = calc_energy(mol, conformer_id)['energy_abs']\n    positions = mol.GetConformer(conformer_id).GetPositions().flatten()\n    position_dict = dict(zip(coordinate_names, positions))\n    position_dict['energy'] = energy\n    data.append(position_dict)\ndata = pd.DataFrame(data).sample(len(data))\ndata\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_0</th>\n      <th>y_0</th>\n      <th>z_0</th>\n      <th>x_1</th>\n      <th>y_1</th>\n      <th>z_1</th>\n      <th>x_2</th>\n      <th>y_2</th>\n      <th>z_2</th>\n      <th>x_3</th>\n      <th>...</th>\n      <th>x_36</th>\n      <th>y_36</th>\n      <th>z_36</th>\n      <th>x_37</th>\n      <th>y_37</th>\n      <th>z_37</th>\n      <th>x_38</th>\n      <th>y_38</th>\n      <th>z_38</th>\n      <th>energy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2350</th>\n      <td>2.082377</td>\n      <td>-2.384621</td>\n      <td>-0.886825</td>\n      <td>1.878611</td>\n      <td>-1.063902</td>\n      <td>-0.149281</td>\n      <td>3.062111</td>\n      <td>-0.226795</td>\n      <td>-0.498702</td>\n      <td>3.164710</td>\n      <td>...</td>\n      <td>-4.783486</td>\n      <td>2.805272</td>\n      <td>0.453616</td>\n      <td>-3.121490</td>\n      <td>3.344990</td>\n      <td>0.081068</td>\n      <td>-1.268761</td>\n      <td>-3.134410</td>\n      <td>-0.056116</td>\n      <td>50.527359</td>\n    </tr>\n    <tr>\n      <th>1031</th>\n      <td>2.077103</td>\n      <td>-1.521307</td>\n      <td>-1.978108</td>\n      <td>1.380593</td>\n      <td>-0.800183</td>\n      <td>-0.726914</td>\n      <td>2.472656</td>\n      <td>-0.385603</td>\n      <td>0.144759</td>\n      <td>2.231073</td>\n      <td>...</td>\n      <td>-4.911553</td>\n      <td>-0.809955</td>\n      <td>1.899992</td>\n      <td>-3.547584</td>\n      <td>0.108867</td>\n      <td>2.562697</td>\n      <td>0.556878</td>\n      <td>2.723737</td>\n      <td>-0.847688</td>\n      <td>61.708833</td>\n    </tr>\n    <tr>\n      <th>2043</th>\n      <td>-1.562612</td>\n      <td>-2.510985</td>\n      <td>-1.091513</td>\n      <td>-1.538347</td>\n      <td>-1.446391</td>\n      <td>0.001791</td>\n      <td>-2.874616</td>\n      <td>-0.790099</td>\n      <td>-0.074631</td>\n      <td>-3.214859</td>\n      <td>...</td>\n      <td>3.185514</td>\n      <td>2.119784</td>\n      <td>-2.037756</td>\n      <td>3.116718</td>\n      <td>3.713313</td>\n      <td>-1.258657</td>\n      <td>1.010212</td>\n      <td>-1.434941</td>\n      <td>-2.027757</td>\n      <td>54.934918</td>\n    </tr>\n    <tr>\n      <th>488</th>\n      <td>-2.622666</td>\n      <td>-0.984364</td>\n      <td>-1.736397</td>\n      <td>-1.552771</td>\n      <td>-0.325951</td>\n      <td>-0.876648</td>\n      <td>-2.211194</td>\n      <td>0.177802</td>\n      <td>0.351123</td>\n      <td>-3.280326</td>\n      <td>...</td>\n      <td>4.928752</td>\n      <td>1.771672</td>\n      <td>-1.560194</td>\n      <td>3.312213</td>\n      <td>1.208019</td>\n      <td>-2.103907</td>\n      <td>0.947618</td>\n      <td>-1.027059</td>\n      <td>2.062733</td>\n      <td>46.566148</td>\n    </tr>\n    <tr>\n      <th>3182</th>\n      <td>-2.622062</td>\n      <td>-1.702202</td>\n      <td>-0.649370</td>\n      <td>-1.699629</td>\n      <td>-0.442711</td>\n      <td>-0.448595</td>\n      <td>-2.041035</td>\n      <td>0.022269</td>\n      <td>0.917023</td>\n      <td>-3.557981</td>\n      <td>...</td>\n      <td>4.663731</td>\n      <td>-0.690552</td>\n      <td>1.900821</td>\n      <td>5.054191</td>\n      <td>-1.298227</td>\n      <td>0.261869</td>\n      <td>1.695958</td>\n      <td>-1.012533</td>\n      <td>-2.171839</td>\n      <td>58.263424</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1695</th>\n      <td>1.545078</td>\n      <td>-1.676186</td>\n      <td>-0.579824</td>\n      <td>1.733232</td>\n      <td>-0.758289</td>\n      <td>0.569323</td>\n      <td>2.313415</td>\n      <td>0.545339</td>\n      <td>0.010601</td>\n      <td>3.660710</td>\n      <td>...</td>\n      <td>-4.968294</td>\n      <td>-0.564927</td>\n      <td>-1.690845</td>\n      <td>-5.102694</td>\n      <td>1.251015</td>\n      <td>-1.373149</td>\n      <td>-1.049935</td>\n      <td>0.839526</td>\n      <td>2.870953</td>\n      <td>65.543487</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>-2.078717</td>\n      <td>2.181547</td>\n      <td>1.377408</td>\n      <td>-1.922400</td>\n      <td>0.699183</td>\n      <td>1.110325</td>\n      <td>-2.860042</td>\n      <td>0.256573</td>\n      <td>0.041219</td>\n      <td>-2.749035</td>\n      <td>...</td>\n      <td>3.568744</td>\n      <td>-2.309582</td>\n      <td>0.450033</td>\n      <td>3.535776</td>\n      <td>-1.985371</td>\n      <td>-1.363970</td>\n      <td>1.251191</td>\n      <td>2.215607</td>\n      <td>1.256163</td>\n      <td>53.297442</td>\n    </tr>\n    <tr>\n      <th>433</th>\n      <td>2.872947</td>\n      <td>-1.275055</td>\n      <td>1.324504</td>\n      <td>1.803448</td>\n      <td>-1.107335</td>\n      <td>0.204128</td>\n      <td>2.282703</td>\n      <td>0.009835</td>\n      <td>-0.633029</td>\n      <td>2.427814</td>\n      <td>...</td>\n      <td>-4.479378</td>\n      <td>0.508470</td>\n      <td>1.416176</td>\n      <td>-4.317413</td>\n      <td>2.044153</td>\n      <td>0.469898</td>\n      <td>-1.669397</td>\n      <td>-2.149311</td>\n      <td>1.292007</td>\n      <td>54.034508</td>\n    </tr>\n    <tr>\n      <th>1870</th>\n      <td>2.664259</td>\n      <td>2.034236</td>\n      <td>0.575131</td>\n      <td>1.761238</td>\n      <td>0.833087</td>\n      <td>0.858491</td>\n      <td>2.505428</td>\n      <td>-0.422292</td>\n      <td>0.967442</td>\n      <td>3.344749</td>\n      <td>...</td>\n      <td>-5.091052</td>\n      <td>-1.406824</td>\n      <td>0.155182</td>\n      <td>-6.283711</td>\n      <td>-0.587390</td>\n      <td>-0.984725</td>\n      <td>-1.040008</td>\n      <td>0.992988</td>\n      <td>1.719511</td>\n      <td>66.927399</td>\n    </tr>\n    <tr>\n      <th>1366</th>\n      <td>-2.664309</td>\n      <td>-0.816584</td>\n      <td>-1.457318</td>\n      <td>-1.533768</td>\n      <td>-0.292689</td>\n      <td>-0.538467</td>\n      <td>-2.304004</td>\n      <td>0.230210</td>\n      <td>0.679246</td>\n      <td>-3.268371</td>\n      <td>...</td>\n      <td>3.204639</td>\n      <td>2.086104</td>\n      <td>-1.194536</td>\n      <td>2.268599</td>\n      <td>2.114207</td>\n      <td>0.313189</td>\n      <td>0.522256</td>\n      <td>0.077753</td>\n      <td>2.072437</td>\n      <td>56.919313</td>\n    </tr>\n  </tbody>\n</table>\n<p>3214 rows × 118 columns</p>\n</div>\n```\n:::\n:::\n\n\nGiven this data, we can build a model. We will use a gradient boosting regressor from scikit-learn. \nWe will also split the data into a training and a test set. In later sections, we will see why this is important.  But for now, let us us just appreciate that a test set---conformers we did not train on---will give us a measure of how well our model will perform on new, unseen, conformers.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\npositions = data[coordinate_names] # X\nenergies = data['energy'] # y\n\n# split into training and test set\ntrain_points = 3000\ntrain_positions = positions[:train_points]\ntest_positions = positions[train_points:]\ntrain_energies = energies[:train_points]\ntest_energies = energies[train_points:]\n\n# train a model\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nmodel = HistGradientBoostingRegressor()\nmodel.fit(train_positions, train_energies)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;HistGradientBoostingRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html\">?<span>Documentation for HistGradientBoostingRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>HistGradientBoostingRegressor()</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\nOnce we have trained a model, we can use it to predict the energies of new conformers. Let's first see how well it does on the data it was trained on.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntrain_predictions = model.predict(train_positions)\n\n\ndensity_scatter_with_hist(train_energies.values, train_predictions, xlabel='True energy', ylabel='Predicted energy')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<Axes: xlabel='True energy', ylabel='Predicted energy'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ml_notes_files/figure-html/cell-5-output-2.png){}\n:::\n:::\n\n\nThis looks pretty good. But how well does it do on new conformers? Let's see. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntest_predictions = model.predict(test_positions)\n\ndensity_scatter_with_hist(test_energies.values, test_predictions, xlabel='True energy', ylabel='Predicted energy')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<Axes: xlabel='True energy', ylabel='Predicted energy'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ml_notes_files/figure-html/cell-6-output-2.png){}\n:::\n:::\n\n\nFrom physics we know that (without external field) the energy of a molecule does not depend on where in space it is. That is, if we translate a molecule along $[1, 1, 1]$, the energy should not change. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# translate the molecule along [1, 1, 1]\ntranslated_positions = train_positions + 1\ntranslated_predictions = model.predict(translated_positions)\ndensity_scatter_with_hist(train_energies.values, translated_predictions)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<Axes: xlabel='Actual', ylabel='Predicted'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ml_notes_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\nThis is not what we expect. Our model shows completly unphysical behavior and predicts a different energy for the same conformers in different positions in space.\n\nTo fix this, and related problems, we need to use a more elaborate approach to building a model.\n\n#### Mmaking predictions invariant/equivariant to transformations\n[Invariance and equivariance are terms that have become very relevant in ML. It is always important to mention with respect to what operation something is invariant and equivariant; if people don't mention this they often refer to the symmetry operations of the Euclidean group which comprises all translations, rotations, and reflection.  Invariant means that the property of interest does not change under those operations. Equivariant means that it changes in the same way. The energy, for example, is invariant and the forces are equivariant.]{.aside}\n\n##### What are symmetries we would like to respect?\nBefore we can talk about how to build a model that respects symmetries, we need to know what symmetries we would like to respect.\n\nIn the case of molecules, we would like to respect the following symmetries:\n\n- translation: that is, if we move a molecule along a vector, the energy should not change (see above)\n- rotation: that is, if we rotate a molecule, the energy should not change\n- permutation of atoms: that is the order with which we put the atoms in the model does not matter\n\nFor crystals, we additionally need to respect periodicity. That is, for intensive properties, there should be no difference between using a unit cell or a super cell of that unit cell as input for a model.\n\n\nBroadly speaking, there are three different ways to build models that respect symmetries.\n\n1. _Data augmentation_: This is the most straightforward approach. We can generate new data points by applying the symmetries to the existing data points. For example, we can generate new conformers by rotating the existing conformers. This approach is very simple to implement, but it can be very expensive. For example, if we want to generate new conformers by rotating the existing conformers, we need to generate a new conformer for every rotation.\nThis approach is often used for computer vision pipelines in which you might want to detect a cat in an image independent of the orientation. In this case, you can generate new images by rotating the existing images.\n2. _Features that are invariant/equivariant_ : This approach is more sophisticated. We can build features that are invariant/equivariant to the symmetries we want to respect. For example, we can build features that are invariant to rotation. In the case of force field such features are bond lengths and angles. This is approach is widely used in ML for chemistry and materials science.\n3. _Models that are invariant/equivariant_: Alternatively, one can build special models that can consume point clouds as inputs and are equivariant to the symmetries we want to respect. We will not discuss this in detail, but you can find starting points in this [perspective by Tess Smidt](https://www.sciencedirect.com/science/article/pii/S2589597420302641).\n\n##### Invariant/equivariant features\n\n###### Symmetry functions \n\n###### Fingerprints \n\n###### Correlation functions\n\n###### Symmetry functions \n\n###### Cheaper computations \n\n\n## Training a model\n\n### How to know if a model is good?\n\nBefore we can proceed to building models, we need to estabilsh a way to measure how good a model is.\n\nInterestingly, this is not as trivial as it may sound. To realize this, it is useful to formally write down what we mean by a good model.\n\n#### Empirical risk minimization\n\nLet's assume we have some input space $\\mathcal{X}$ and some output space $\\mathcal{Y}$. We can think of $\\mathcal{X}$ as the space of all possible inputs and $\\mathcal{Y}$ as the space of all possible outputs. For example, $\\mathcal{X}$ could be the space of all possible molecules and $\\mathcal{Y}$ could be the space of all possible energies.\nWe want to learn a function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ that maps inputs to outputs. We can think of $f$ as a model that we want to train.\n\nTo build this models we have samples of the joint distribution $p(x, y)$, where $x$ is an input and $y$ is the corresponding output. We can think of this as a set of data points $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}$.\n\nIf we now define a loss function $L$ we can compute the risk, which is the expected value of the loss function:\n\n$$\nR(h)={\\mathbf  {E}}[L(f(x),y)]=\\int L(f(x),y)\\,dP(x,y).\n$$\n\nour goal is to find a model $f$ that minimizes the risk:\n\n$$\n{\\displaystyle h^{*}={\\underset {h\\in {\\mathcal {H}}}{\\operatorname {arg\\,min} }}\\,{R(h)}.}\n$$\n\nIn practice we cannot compute this. \nThe reason is that we do not have access to the joint distribution $p(x, y)$, but only to a finite set of samples $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}$.\n\n### Linear regression \n\n```python\nimport jax.numpy as jnp\n\ndef linear_regression(x, w, b):\n    return jnp.dot(x, w) + b\n```\n\n\n```python\ndef loss(w, b):\n    prediction = linear_regression(x, w, b)\n    return jnp.mean((prediction - y) ** 2)\n```\n\n\n```python\ndef init_params(num_feat):\n    return np.random.normal(size=(num_feat,)), 0.0\n```\n\n```python \nloss_grad = jax.grad(loss, argnums=(0, 1))\n```\n\n```python\nlearning_rate = 1e-6\nnum_epochs = 1000\n```\n\n\n## Bias-variance trade-off\n\n## Hyperparameters \n\n\n## Kernel trick\n\n![Kernel-based machine learning can be thought of expressing the property of interest via an expansion in a basis spanned by the structures in the training set. Figure taken from M. A. Caro, _Arkhimedes_ *2018*, 3, 21.](images/kt_visual.png)\n\n## Feature importance\n\n### Permutation feature importance\n\n## Feature selection \n\n### Curse of dimensionality \n\nFor understanding the curse of dimensionality, it is useful to consider a very simple ML model, the $k$-nearest neighbors model. In this model, we have a set of training points $\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}$, where $x_i$ is a vector of features and $y_i$ is the corresponding label. To make a prediction, we compute the distance between the input and all training points and return the mode of the labels of the $k$ closest training points.\n\nClearly, in this algorithm it is important to find the nearest neighbor. In general, this is important in many algorithms, for instance also in kernel-based learning.\n\nLet's now ask ourself what part of the space we need to find the nearest neighbors.\n\n<!-- https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html -->\n\nFor this, let's start considering a unit cube $[0,1]^d$ and $n$ data points $x_i$ sampled uniformly from this cube.\n\nThe smallest hypercube that contains $k$ out of the $n$ points has the following edge length \n\n$$\nl^d = \\frac{k}{n} \\quad \\Rightarrow \\quad l = \\left(\\frac{k}{n}\\right)^{1/d} \n$$\n\nIf we plot this for different values of $d$ we get the following plot:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np \n\ndef length(d, k=5, n=10_000):\n    return (k/n)**(1/d)\n\nd = np.arange(1, 1000)\n\nplt.plot(d, length(d))\nplt.xlabel('numbr of dimensions')\nplt.ylabel('length of hypercube that contains k neighbors')\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nText(0, 0.5, 'length of hypercube that contains k neighbors')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ml_notes_files/figure-html/cell-8-output-2.png){}\n:::\n:::\n\n\nClearly, for large $d$ the length approaches 1---which means that all points are now almost equally far apart and comparing distances no longer makes much sense.\n\nWe can also check this by performing a simulation: Generating random $d$ dimensional points and computing the distance between them. We can then plot the distribution of distances.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom scipy.spatial import distance_matrix\ndimensions = [2, 5, 10, 100, 10_000]\nnum_points = 1000\n\nfig, axes = plt.subplots(1, len(dimensions), sharey='all')\n\ndef get_distances(d, num_points):\n    points = np.random.uniform(size=(num_points, d))\n    distances = distance_matrix(points, points)\n    return np.array(distances).flatten()\n\nfor d, ax in zip(dimensions, axes):\n    distances = get_distances(d, num_points)\n    ax.hist(distances, bins=20)\n    ax.set_title(f'd={d} \\n cv={distances.std()/distances.mean():.2f}')\n```\n\n::: {.cell-output .cell-output-display}\n![](ml_notes_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nClearly, for large $d$ the distances are almost the same (the histograms are much more peaked). \nWe can also see this in terms of the coefficient of variation (cv), which is the standard deviation divided by the mean. For large $d$ the cv is very small, which means that the distances are very similar.\n\n### Feature selection approaches\n\n## Feature projection \n\n### Principal component analysis\n\n### t-distributed stochastic neighbor embedding\n\n\n## Feature learning \n\n",
    "supporting": [
      "ml_notes_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}