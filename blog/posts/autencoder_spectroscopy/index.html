<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-04-23">

<title>Autoencoders as Digital Archaeologists for Spectroscopic Data – Kevin’s Homepage</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-3a751e4d48817b0aacecf974f477ff73.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-2f37bee87c45ee91f2836123699e7799.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-S9W9LVHXJK"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-S9W9LVHXJK', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Autoencoders as Digital Archaeologists for Spectroscopic Data – Kevin’s Homepage">
<meta property="og:description" content="An archaeological journey through molecular and material landscapes">
<meta property="og:image" content="https://kjablonka.com/blog/posts/autencoder_spectroscopy/cover.png">
<meta property="og:site_name" content="Kevin's Homepage">
<meta property="og:image:height" content="1024">
<meta property="og:image:width" content="1536">
<meta name="twitter:title" content="Autoencoders as Digital Archaeologists for Spectroscopic Data – Kevin’s Homepage">
<meta name="twitter:description" content="An archaeological journey through molecular and material landscapes">
<meta name="twitter:image" content="https://kjablonka.com/blog/posts/autencoder_spectroscopy/cover.png">
<meta name="twitter:creator" content="@kmjablonka">
<meta name="twitter:image-height" content="1024">
<meta name="twitter:image-width" content="1536">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> <i class="bi bi-home" role="img">
</i> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#the-digital-archaeologists-toolkit" id="toc-the-digital-archaeologists-toolkit" class="nav-link" data-scroll-target="#the-digital-archaeologists-toolkit">The Digital Archaeologist’s Toolkit</a>
  <ul class="collapse">
  <li><a href="#phase-1-the-excavation-encoding" id="toc-phase-1-the-excavation-encoding" class="nav-link" data-scroll-target="#phase-1-the-excavation-encoding">Phase 1: The Excavation (Encoding)</a></li>
  <li><a href="#phase-2-the-latent-space" id="toc-phase-2-the-latent-space" class="nav-link" data-scroll-target="#phase-2-the-latent-space">Phase 2: The latent space</a></li>
  <li><a href="#phase-3-bringing-it-back-together-reconstruction" id="toc-phase-3-bringing-it-back-together-reconstruction" class="nav-link" data-scroll-target="#phase-3-bringing-it-back-together-reconstruction">Phase 3: Bringing it back together (Reconstruction)</a></li>
  </ul></li>
  <li><a href="#the-mathematics-of-archaeological-documentation" id="toc-the-mathematics-of-archaeological-documentation" class="nav-link" data-scroll-target="#the-mathematics-of-archaeological-documentation">The Mathematics of Archaeological Documentation</a></li>
  <li><a href="#the-manifold-hypothesis-why-this-archaeological-dig-makes-sense-at-all" id="toc-the-manifold-hypothesis-why-this-archaeological-dig-makes-sense-at-all" class="nav-link" data-scroll-target="#the-manifold-hypothesis-why-this-archaeological-dig-makes-sense-at-all">The Manifold Hypothesis: Why This Archaeological Dig Makes Sense At All</a></li>
  <li><a href="#from-classical-to-neural-the-connection-between-pca-and-linear-autoencoders" id="toc-from-classical-to-neural-the-connection-between-pca-and-linear-autoencoders" class="nav-link" data-scroll-target="#from-classical-to-neural-the-connection-between-pca-and-linear-autoencoders">From Classical to Neural: The Connection Between PCA and Linear Autoencoders</a>
  <ul class="collapse">
  <li><a href="#a-practical-example-finding-the-redundant-dimension" id="toc-a-practical-example-finding-the-redundant-dimension" class="nav-link" data-scroll-target="#a-practical-example-finding-the-redundant-dimension">A Practical Example: Finding the Redundant Dimension</a></li>
  </ul></li>
  <li><a href="#beyond-linear-maps-where-neural-networks-actually-shine" id="toc-beyond-linear-maps-where-neural-networks-actually-shine" class="nav-link" data-scroll-target="#beyond-linear-maps-where-neural-networks-actually-shine">Beyond Linear Maps: Where Neural Networks Actually Shine</a>
  <ul class="collapse">
  <li><a href="#the-nonlinear-archaeologists-advantage" id="toc-the-nonlinear-archaeologists-advantage" class="nav-link" data-scroll-target="#the-nonlinear-archaeologists-advantage">The Nonlinear Archaeologist’s Advantage</a></li>
  </ul></li>
  <li><a href="#the-probabilistic-excavation-variational-autoencoders" id="toc-the-probabilistic-excavation-variational-autoencoders" class="nav-link" data-scroll-target="#the-probabilistic-excavation-variational-autoencoders">The Probabilistic Excavation: Variational Autoencoders</a>
  <ul class="collapse">
  <li><a href="#manifold-cartography-the-kl-divergence-as-map-making" id="toc-manifold-cartography-the-kl-divergence-as-map-making" class="nav-link" data-scroll-target="#manifold-cartography-the-kl-divergence-as-map-making">Manifold Cartography: The KL Divergence as Map-Making</a></li>
  </ul></li>
  <li><a href="#conclusion-the-journey-continues" id="toc-conclusion-the-journey-continues" class="nav-link" data-scroll-target="#conclusion-the-journey-continues">Conclusion: The Journey Continues</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/kjappelbaum/kjappelbaum.github.io/edit/main/blog/posts/autencoder_spectroscopy/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Autoencoders as Digital Archaeologists for Spectroscopic Data</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">An archaeological journey through molecular and material landscapes</p>
  <div class="quarto-categories">
    <div class="quarto-category">machine-learning</div>
    <div class="quarto-category">teaching</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 23, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Picture this: An archaeologist stands at a dig site, surrounded by layers of earth that haven’t seen sunlight since dinosaurs were a hot new trend. With painstaking care, they brush away dirt and sediment, revealing pottery shards, and that one graduate student who fell asleep on the job.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="archeology_process.png" class="img-fluid figure-img"></p>
<figcaption>The process of archeology</figcaption>
</figure>
</div>
<p>Now imagine replacing dirt with noise, shards with molecular signatures, and the reconstructed vase with a clean spectrum. Welcome to the world of spectroscopic data analysis using autoencoders: where we excavate molecular treasures from layers of noise and complexity through what the fancy folks call “unsupervised representation learning” (which is “teaching computers to find patterns without telling them what patterns to find”).</p>
<p>Every spectroscopic measurement is like an archaeological dig, except instead of finding ancient coins, we’re finding molecular transitions between energy states.</p>
<p>But just as ancient artifacts come to us covered in dirt and damaged by time (and occasionally by that one archaeologist who thought dynamite was a good excavation tool), our spectroscopic data arrives buried under multiple layers of contamination: such as</p>
<ul>
<li><em>noise</em> due to random fluctuations (“electrons having a dance party”)</li>
<li><em>environmental interference</em>: for example, Water vapor and CO₂ absorption bands</li>
<li><em>instrumental artifacts</em>: baseline drift (“detector getting tired”)</li>
<li><em>physical degradation</em>: sample fluorescence and aging effects</li>
</ul>
<p>Traditional smoothing techniques are the equivalent of using a bulldozer to dust off a delicate vase. Sure, you’ll remove the dirt, but you might also remove, well, everything else. One usecase of autoencoder is to do this in a better way.</p>
</section>
<section id="the-digital-archaeologists-toolkit" class="level2">
<h2 class="anchored" data-anchor-id="the-digital-archaeologists-toolkit">The Digital Archaeologist’s Toolkit</h2>
<p>In our metaphor, an autoencoder is like a three-phase archaeological expedition.</p>
<section id="phase-1-the-excavation-encoding" class="level3">
<h3 class="anchored" data-anchor-id="phase-1-the-excavation-encoding">Phase 1: The Excavation (Encoding)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="excavation_comp.png" class="img-fluid figure-img"></p>
<figcaption>Comparison of archeological and spectroscopic excavations.</figcaption>
</figure>
</div>
<p>The first phase in our archeology mission begins by cleaning the artifacts we find: removing the sand, cleaning them. We can think of it as removing the unnecessary. Similarly, our spectroscopic application starts with removing the unnecessary and “compressing” the spectrum to its essentials.</p>
<p>In the simplest form this can be done using a sequence of linear layers:</p>
<div id="c1267b7f" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExcavationBrush(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, spectral_channels<span class="op">=</span><span class="dv">1000</span>, artifact_dimensions<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compressor <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>            nn.Linear(spectral_channels, <span class="dv">512</span>),</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">512</span>),</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>),</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>),</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, artifact_dimensions)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, buried_spectrum):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.compressor(buried_spectrum)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The encoder takes our high-dimensional spectrum and compresses it into something more manageable. But this isn’t a simple compression algorithm: the model learns which features matter most, like an experienced archaeologist who can tell the difference between “priceless artifact” and “rock that looks vaguely interesting.”</p>
</section>
<section id="phase-2-the-latent-space" class="level3">
<h3 class="anchored" data-anchor-id="phase-2-the-latent-space">Phase 2: The latent space</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="latent_space_comp.png" class="img-fluid figure-img"></p>
<figcaption>The archeological vs.&nbsp;spectroscopic latent space</figcaption>
</figure>
</div>
<p>The latent space is our archaeological museum’s storage room—not the fancy public galleries with mood lighting and gift shops, but the back room where things actually get done. Here, each spectrum becomes a neat little index card with just the essential information.</p>
<div id="0664b30b" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In our latent space, each spectrum becomes coordinates on an ancient map</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># It's like Google Maps, but for molecules</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_spectrum(encoder, noisy_spectrum):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    latent_artifacts <span class="op">=</span> encoder(noisy_spectrum)  <span class="co"># Returns a point in hyperspace</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> latent_artifacts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>But this is no ordinary storage room. It’s a magical space where similar artifacts naturally cluster together, like teenagers at a high school cafeteria. Polymers hang out in their valley, ceramics claim the mountain peaks, and metal oxides spread across their plains like they own the place.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Math Behind Clustering
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The clustering emerges from what we call the <a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">manifold hypothesis</a>—the idea that high-dimensional data actually lives on a lower-dimensional surface.</p>
<p>Mathematically, our encoder learns a mapping: <span class="math display">\[
f_\phi: \mathcal{X} \rightarrow \mathcal{Z}
\]</span> Where <span class="math inline">\(\mathcal{X} \subset \mathbb{R}^n\)</span> is where our data lives (the messy real world) and <span class="math inline">\(\mathcal{Z} \subset \mathbb{R}^m\)</span> is our nice, clean latent space. This mapping preserves important properties:</p>
<p><strong>Distance preservation</strong>: Similar inputs map to nearby points <span class="math display">\[
d_{\mathcal{Z}}(f_\phi(x_i), f_\phi(x_j)) \approx d_{\mathcal{X}}(x_i, x_j)
\]</span> <strong>Continuity</strong>: Small changes in input create small changes in output</p>
<p><span class="math display">\[
\|f_\phi(x_1) - f_\phi(x_2)\| \leq L\|x_1 - x_2\|
\]</span></p>
<p>(The Lipschitz condition.)</p>
<p>So materials with similar spectra end up as neighbors in latent space, forming these natural clusters. It’s like chemical social networking!</p>
</div>
</div>
</div>
</section>
<section id="phase-3-bringing-it-back-together-reconstruction" class="level3">
<h3 class="anchored" data-anchor-id="phase-3-bringing-it-back-together-reconstruction">Phase 3: Bringing it back together (Reconstruction)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="reconstruction.png" class="img-fluid figure-img"></p>
<figcaption>Archeological vs.&nbsp;spectroscopic reconstruction.</figcaption>
</figure>
</div>
<p>Using only our compressed representation (those index cards), we attempt to reconstruct the original spectrum. It’s like trying to rebuild a dinosaur from a few bones and a lot of imagination, except our imagination is constrained by mathematics.</p>
<div id="26d710c3" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Reconstructor(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, artifact_dimensions<span class="op">=</span><span class="dv">32</span>, spectral_channels<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reconstruction_process <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>            nn.Linear(artifact_dimensions, <span class="dv">256</span>),</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>),</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>),</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">512</span>),</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, spectral_channels),</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid() </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, artifact_description):</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.reconstruction_process(artifact_description)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The decoder takes our compressed representation and attempts to rebuild the original spectrum. If we’ve done our job right (and haven’t accidentally trained our network to just output pictures of cats), the reconstruction should be faithful to the original.</p>
</section>
</section>
<section id="the-mathematics-of-archaeological-documentation" class="level2">
<h2 class="anchored" data-anchor-id="the-mathematics-of-archaeological-documentation">The Mathematics of Archaeological Documentation</h2>
<p>Just as physical conservation laws govern the preservation of matter and energy (thanks, Emmy Noether!), information theory dictates how we can compress and reconstruct data without turning it into digital gibberish.</p>
<p>The fundamental equation governing our autoencoder is the reconstruction loss:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{reconstruction}} = \|x - \hat{x}\|^2
\]</span></p>
<p>Where <span class="math inline">\((x)\)</span> is our original spectrum (the truth, the whole truth, and nothing but the truth) and <span class="math inline">\((\hat{x})\)</span> is our reconstruction.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why MSE Makes Statistical Sense
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let me tell you a tale about why MSE and Gaussian noise are BFFs.</p>
<p>If we assume our noise is Gaussian with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
p(x|z) = \mathcal{N}(x; f_\theta(z), \sigma^2I)
\]</span></p>
<p>The likelihood for a single data point becomes: <span class="math display">\[
p(x|z) = \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{\|x - f_\theta(z)\|^2}{2\sigma^2}\right)
\]</span></p>
<p>Taking the negative log-likelihood:</p>
<p><span class="math display">\[
-\log p(x|z) = \frac{n}{2}\log(2\pi\sigma^2) + \frac{\|x - f_\theta(z)\|^2}{2\sigma^2}
\]</span></p>
<p>Since the first term is constant w.r.t. θ (our parameters), minimizing negative log-likelihood is equivalent to minimizing:</p>
<p><span class="math display">\[
\frac{\|x - f_\theta(z)\|^2}{2\sigma^2}
\]</span></p>
<p>Which is just MSE in a fancy hat! So when you use MSE loss, you’re implicitly assuming Gaussian noise.</p>
</div>
</div>
</div>
<p>But we can be fancier with a composite loss function:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{total}} = \underbrace{\|x - \hat{x}\|^2}_{\text{Be accurate}} + \lambda_1 \underbrace{\|\nabla x - \nabla \hat{x}\|^2}_{\text{Be smooth}} + \lambda_2 \underbrace{\sum_{p \in \text{peaks}} |x_p - \hat{x_p}|}_{\text{Don't mess up the peaks}} + \lambda_3 \underbrace{\mathcal{R}(\phi, \theta)}_{\text{Don't go crazy}}
\]</span></p>
<p>Each term has a job:</p>
<ul>
<li>Fidelity term: “Make it look like the original”</li>
<li>Gradient penalty: “Keep it smooth, no sudden jumps”</li>
<li>Feature preservation: “Those peaks are important, don’t lose them!”</li>
<li>Regularization: “Stay humble, don’t overfit”</li>
</ul>
</section>
<section id="the-manifold-hypothesis-why-this-archaeological-dig-makes-sense-at-all" class="level2">
<h2 class="anchored" data-anchor-id="the-manifold-hypothesis-why-this-archaeological-dig-makes-sense-at-all">The Manifold Hypothesis: Why This Archaeological Dig Makes Sense At All</h2>
<p>Let’s address a fundamental question: why should this even work? Shouldn’t compressing our beautiful high-dimensional spectrum lose valuable information? Welcome to the manifold hypothesis, the reason dimensionality reduction isn’t just mathematical vandalism.</p>
<p>The manifold hypothesis suggests that high-dimensional data (like our spectroscopic signals) aren’t actually using all those dimensions effectively. Instead, the data lies on or near a lower-dimensional surface (a manifold) embedded in that high-dimensional space. It’s like discovering that what looks like a complex 3D sculpture is actually just a cleverly folded 2D sheet of paper.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why spectroscopic data probably lives on a manifold
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Spectroscopic data is fundamentally constrained by:</p>
<ul>
<li><strong>Physics</strong>: Certain combinations of absorption bands are physically impossible due to quantum mechanical selection rules. You can’t just have arbitrary patterns of peaks!</li>
<li><strong>Chemistry</strong>: Molecular structures create specific patterns of vibrations, rotations, and electronic transitions. A carbonyl group will always give you that telltale peak around 1700 cm⁻¹ in IR spectroscopy. And the space of possible chemicals is constrained (you cannot combine all atoms in all possible ways)</li>
<li><strong>Instrumental limitations</strong>: Your spectrometer has a specific resolution and response function, further constraining the space of possible measurements.</li>
</ul>
<p>These constraints mean that despite having thousands of wavelength points, your spectrum is likely determined by a much smaller number of underlying variables—chemical compositions, molecular structures, temperature, etc.</p>
<p>Mathematically, if your spectral data points <span class="math inline">\(\{x_1, x_2, \dots, x_n\} \in \mathbb{R}^d\)</span> (where d might be thousands of wavelengths), they likely lie on or near a <span class="math inline">\(k\)</span>-dimensional manifold <span class="math inline">\(\mathcal{M} \subset \mathbb{R}^d\)</span> where <span class="math inline">\(k\ll d\)</span>.</p>
<p>The goal of our autoencoder is to learn this manifold—the archaeological site map, if you will.</p>
</div>
</div>
</div>
<p>To visualize this, imagine our spectra are actually faces of ancient masks (stay with me here). Each mask has thousands of pixels (dimensions), but you could describe any mask with far fewer parameters: eye size, mouth width, nose shape, etc. That’s your manifold! Autoencoders discover these “facial features” of spectra automatically. ![You might be familiar with <a href="https://en.wikipedia.org/wiki/Eigenface">eigenfaces</a>, which are “basis vectors” of human faces one can derive with PCA.]</p>
<div id="cell-fig-manifold" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="fig-manifold" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-manifold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-manifold-output-1.png" width="921" height="469" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-manifold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Illustration of how data might lie on a lower-dimensional manifold in a higher-dimensional space.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="from-classical-to-neural-the-connection-between-pca-and-linear-autoencoders" class="level2">
<h2 class="anchored" data-anchor-id="from-classical-to-neural-the-connection-between-pca-and-linear-autoencoders">From Classical to Neural: The Connection Between PCA and Linear Autoencoders</h2>
<p>Long before neural networks were cool, archaeologists (well, statisticians) had their own dimensionality reduction technique: Principal Component Analysis, or PCA.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Mathematical Connection Between PCA and Linear Autoencoders
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let’s consider a linear autoencoder with:</p>
<ul>
<li>Input dimension <span class="math inline">\(d\)</span></li>
<li>Latent dimension <span class="math inline">\(k\)</span> (where <span class="math inline">\(k &lt; d\)</span>)</li>
<li>Encoder weight matrix <span class="math inline">\(W_1 \in  \mathbb{R}^{k \times d}\)</span></li>
<li>Decoder weight matrix <span class="math inline">\(W_2 \in  \mathbb{R}^{d \times k}\)</span></li>
<li>No biases or activation functions</li>
</ul>
<p>For an input <span class="math inline">\(x \in \mathbb{R}^d\)</span>, the encoding and reconstruction process is:</p>
<ol type="1">
<li>Encode: <span class="math inline">\(z = W_1 x\)</span> (where <span class="math inline">\(z \in \mathbb{R}^k\)</span>)</li>
<li>Decode: <span class="math inline">\(\hat{x} = W_2 z = W_2W_1x\)</span></li>
</ol>
<p>The reconstruction error we minimize is: <span class="math display">\[
\mathcal{L} = \|x - \hat{x}\|^2 = \|x - W_2W_1x\|^2
\]</span></p>
<p>Under the constraint that <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> minimize this reconstruction error, the optimal solution has the following properties:</p>
<ul>
<li><span class="math inline">\(W_2 = W_1^T\)</span> (the decoder is the transpose of the encoder)</li>
<li>The rows of <span class="math inline">\(W_1\)</span> are the first k principal components of the data</li>
</ul>
<p>To see why, let’s decompose our data matrix <span class="math inline">\(X\)</span> using SVD: <span class="math display">\[
X = U\Sigma V^T
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(U\)</span> contains the left singular vectors</li>
<li><span class="math inline">\(\Sigma\)</span> contains the singular values on its diagonal</li>
<li><span class="math inline">\(V^T\)</span> contains the right singular vectors</li>
</ul>
<p>The optimal linear projection to <span class="math inline">\(k\)</span> dimensions is given by:</p>
<p><span class="math display">\[
W_1 = U_k^T
\]</span></p>
<p>Where <span class="math inline">\(U_k\)</span> contains the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(U\)</span> (corresponding to the <span class="math inline">\(k\)</span> largest singular values).</p>
<p>And the optimal reconstruction matrix is: <span class="math display">\[
W_2 = U_k
\]</span></p>
<p>Which is exactly <span class="math inline">\(W_1^T\)</span>.</p>
<p>Therefore, our reconstructed data is: <span class="math display">\[
\hat{X} = W_2W_1X = U_kU_k^TX
\]</span></p>
<p>Which is precisely the reconstruction you’d get from projecting X onto the first k principal components and back.</p>
<p>This means our linear autoencoder will learn the same subspace as PCA, just with more computational effort and the possibility of getting stuck in local minima. It’s like taking a road trip to your neighbor’s house—you’ll get there, but was the scenic route necessary?</p>
</div>
</div>
</div>
<section id="a-practical-example-finding-the-redundant-dimension" class="level3">
<h3 class="anchored" data-anchor-id="a-practical-example-finding-the-redundant-dimension">A Practical Example: Finding the Redundant Dimension</h3>
<p>Let’s make this concrete with an example. Imagine we have a spectrum where two neighboring wavelengths always vary together—perhaps due to a broad absorption band or some instrumental correlation.</p>
<div id="83060c40" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dataset with a redundant dimension</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_redundant_spectrum(num_samples<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Independent features</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    independent_features <span class="op">=</span> np.random.randn(num_samples, <span class="dv">3</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a 5D spectrum where dimensions 2 and 3 are correlated</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    spectra <span class="op">=</span> np.zeros((num_samples, <span class="dv">5</span>))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">0</span>] <span class="op">=</span> independent_features[:, <span class="dv">0</span>]  <span class="co"># Independent</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">1</span>] <span class="op">=</span> independent_features[:, <span class="dv">1</span>]  <span class="co"># Independent</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">2</span>] <span class="op">=</span> independent_features[:, <span class="dv">2</span>]  <span class="co"># Independent</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">3</span>] <span class="op">=</span> <span class="fl">0.95</span> <span class="op">*</span> independent_features[:, <span class="dv">2</span>] <span class="op">+</span> <span class="fl">0.05</span> <span class="op">*</span> np.random.randn(num_samples)  <span class="co"># Correlated with dim 2</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">4</span>] <span class="op">=</span> independent_features[:, <span class="dv">0</span>] <span class="op">-</span> independent_features[:, <span class="dv">1</span>]  <span class="co"># Another linear combination</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> spectra</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a linear autoencoder</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearAutoencoder(nn.Module):</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim<span class="op">=</span><span class="dv">5</span>, latent_dim<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Linear(input_dim, latent_dim, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># No bias</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Linear(latent_dim, input_dim, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># No bias</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        latent <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(latent)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tie_weights(<span class="va">self</span>):</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This enforces W_2 = W_1^T </span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder.weight.data <span class="op">=</span> <span class="va">self</span>.encoder.weight.data.t()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When we train this model, it should learn to identify dimension 3 as redundant (since it’s nearly identical to dimension 2). Also dimension 4 is only a linear combination of other dimensions. A 3-dimensional latent space will capture all the variance in the 5-dimensional input.</p>
<div id="cell-fig-pca" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate redundant spectrum</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>spectra <span class="op">=</span> create_redundant_spectrum()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply PCA</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">5</span>)  <span class="co"># Get all components to see variance</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>spectra_reduced <span class="op">=</span> pca.fit_transform(spectra)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>pca_three <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>spectra_reduced_three <span class="op">=</span> pca_three.fit_transform(spectra)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>spectra_reconstructed <span class="op">=</span> pca_three.inverse_transform(spectra_reduced_three)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate reconstruction error</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>reconstruction_error <span class="op">=</span> np.mean((spectra <span class="op">-</span> spectra_reconstructed) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the explained variance</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>), pca.explained_variance_ratio_)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Principal Component"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Explained Variance Ratio"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"PCA Explained Variance (Reconstruction Error: </span><span class="sc">{</span>reconstruction_error<span class="sc">:.6f}</span><span class="ss">)"</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>))</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pca" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-pca-output-1.png" width="950" height="469" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: PCA analysis of the redundant spectrum data, showing the explained variance ratio.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In the real world, spectroscopic data often has many such redundancies. Neighboring wavelengths are correlated, certain patterns of peaks occur together, and baseline effects introduce further correlations. These redundancies are exactly what autoencoders exploit—the manifold structure of our data.</p>
<p>The difference is that nonlinear autoencoders can capture more complex manifolds that PCA misses. It’s like upgrading from a 2D map to a 3D hologram of your archaeological site.</p>
</section>
</section>
<section id="beyond-linear-maps-where-neural-networks-actually-shine" class="level2">
<h2 class="anchored" data-anchor-id="beyond-linear-maps-where-neural-networks-actually-shine">Beyond Linear Maps: Where Neural Networks Actually Shine</h2>
<p>Now we’ve seen that linear autoencoders are just PCA in disguise, let’s talk about why we still bother with neural networks.</p>
<p>The magic happens when we add nonlinearities: those lovely activation functions like ReLU, sigmoid, or tanh. These allow autoencoders to learn complex, curved manifolds that PCA could never dream of capturing.</p>
<div id="e3132a8c" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NonlinearArchaeologist(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim<span class="op">=</span><span class="dv">1000</span>, latent_dim<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Now with extra nonlinear goodness!</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_dim, <span class="dv">512</span>),</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),  <span class="co"># This is where the magic happens</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>),</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),  <span class="co"># More magic!</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, latent_dim)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            nn.Linear(latent_dim, <span class="dv">256</span>),</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>),</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, input_dim)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Power of Nonlinearity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider a simple nonlinear manifold: data points lying on a curved surface, like a <a href="https://en.wikipedia.org/wiki/Swiss_roll">swiss roll</a> or a spiral. Linear methods like PCA can only find a flat subspace that minimizes the average distance to all points.</p>
<p>But with nonlinear transformations, we can “unroll” or “straighten” the manifold.</p>
<p>For autoencoders, this means:</p>
<ul>
<li>The encoder can learn a function <span class="math inline">\(f: \mathbb{R}^d \to \mathbb{R}^m\)</span> that maps the curved manifold to a flat latent space</li>
<li>The decoder learns the inverse mapping <span class="math inline">\(g: \mathbb{R}^m \to \mathbb{R}^d\)</span> to bring it back</li>
</ul>
<p>The nonlinear functions effectively learn to “straighten” the manifold in latent space, making it more amenable to analysis and visualization.</p>
<p>It’s like being able to translate an ancient text written on a curved vase simply by “unwrapping” it digitally!</p>
</div>
</div>
</div>
<section id="the-nonlinear-archaeologists-advantage" class="level3">
<h3 class="anchored" data-anchor-id="the-nonlinear-archaeologists-advantage">The Nonlinear Archaeologist’s Advantage</h3>
<p>Imagine two archaeological sites with similar artifacts. A traditional archaeologist might classify them identically based on simple metrics. But our advanced neural archaeologist notices subtle nonlinear patterns.</p>
<p>Similarly, nonlinear autoencoders can distinguish between spectral patterns that would be indistinguishable to linear methods. They can capture:</p>
<ul>
<li><strong>Peak shifting</strong> - When peaks move slightly based on local environment</li>
<li><strong>Multiplicative interactions</strong> - When components don’t just add linearly</li>
<li><strong>Complex baselines</strong> - When background signals have complicated, nonlinear forms</li>
</ul>
<p>This is why, despite the elegance and interpretability of PCA, we still train these complex nonlinear beasts for real spectroscopic data. The archaeology of molecules is rarely a linear affair!</p>
</section>
</section>
<section id="the-probabilistic-excavation-variational-autoencoders" class="level2">
<h2 class="anchored" data-anchor-id="the-probabilistic-excavation-variational-autoencoders">The Probabilistic Excavation: Variational Autoencoders</h2>
<p>What if our archaeologist isn’t completely certain about what they’ve found? Enter <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">the Variational Autoencoder (VAE)</a>—the probabilistic archaeologist who deals in uncertainties rather than absolutes.</p>
<div id="44e752b4" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ProbabilisticArchaeologist(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim<span class="op">=</span><span class="dv">1000</span>, latent_dim<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encoder produces distribution parameters</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_base <span class="op">=</span> nn.Sequential(</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_dim, <span class="dv">512</span>),</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>),</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Two outputs: mean and log-variance</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_mu <span class="op">=</span> nn.Linear(<span class="dv">256</span>, latent_dim)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_logvar <span class="op">=</span> nn.Linear(<span class="dv">256</span>, latent_dim)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Decoder reconstructs from samples</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(latent_dim, <span class="dv">256</span>),</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>),</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, input_dim)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x):</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder_base(x)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> <span class="va">self</span>.fc_mu(h)         <span class="co"># "I think the artifact is here"</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        logvar <span class="op">=</span> <span class="va">self</span>.fc_logvar(h)  <span class="co"># "But I could be wrong by this much"</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu, logvar</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reparameterize(<span class="va">self</span>, mu, logvar):</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The famous reparameterization trick</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> logvar)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> torch.randn_like(std)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu <span class="op">+</span> eps <span class="op">*</span> std</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        mu, logvar <span class="op">=</span> <span class="va">self</span>.encode(x)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.reparameterize(mu, logvar)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(z), mu, logvar</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="manifold-cartography-the-kl-divergence-as-map-making" class="level3">
<h3 class="anchored" data-anchor-id="manifold-cartography-the-kl-divergence-as-map-making">Manifold Cartography: The KL Divergence as Map-Making</h3>
<p>Here’s where the VAE truly shines: it doesn’t just learn the manifold, it learns a <em>probabilistic</em> manifold with a well-behaved coordinate system. The VAE loss function has two terms:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{VAE}} = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction: Make it look right}} - \underbrace{D_{\text{KL}}(q_\phi(z|x) \| p(z))}_{\text{KL divergence: Keep it reasonable}}
\]</span></p>
<p>The first part is our familiar reconstruction loss - “make the reconstruction look like the input.”</p>
<p>The second part is the Kullback-Leibler divergence, which measures how much our learned distribution <span class="math inline">\(q_\phi(z|x)\)</span> differs from a prior distribution <span class="math inline">\(p(z)\)</span> (typically a standard normal distribution).</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why the KL Term Matters for the Manifold
</div>
</div>
<div class="callout-body-container callout-body">
<p>The KL divergence term in VAEs serves multiple crucial purposes that make it perfect for learning manifolds:</p>
<ol type="1">
<li><p><strong>It creates a continuous latent space</strong>: By encouraging overlap between the distributions of similar data points, the KL term ensures that nearby points in input space map to overlapping regions in latent space. This creates a smooth manifold where interpolation makes sense.</p></li>
<li><p><strong>It regularizes the coordinate system</strong>: Without the KL term, the autoencoder could learn any arbitrary mapping that preserves information. The KL term acts like a cartographer imposing a standard coordinate system on a newly discovered land.</p></li>
<li><p><strong>It enables generative sampling</strong>: By forcing the aggregate posterior to match the prior distribution, we can sample from the prior and generate new data points that lie on the learned manifold - essentially “discovering” new artifacts that could plausibly exist.</p></li>
<li><p><strong>It prevents overfitting</strong>: The KL term acts as a complexity penalty that prevents the model from learning an overly complex mapping that might not generalize well.</p></li>
</ol>
</div>
</div>
<p>When applied to spectroscopic data, this is particularly powerful because:</p>
<ol type="1">
<li>We can generate new realistic spectra by sampling from the latent space</li>
<li>We can perform meaningful interpolation between spectra</li>
<li>We can quantify uncertainty in our representations</li>
</ol>
<div id="23c122c7" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vae_loss(reconstruction, x, mu, logvar, beta<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate the VAE loss with reconstruction and KL terms"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reconstruction loss (how well does the output match the input?)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    recon_loss <span class="op">=</span> F.mse_loss(reconstruction, x, reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># KL divergence (how much does our distribution differ from the prior?)</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For the standard normal prior, this has a nice closed form</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    kl_loss <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(<span class="dv">1</span> <span class="op">+</span> logvar <span class="op">-</span> mu.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="op">-</span> logvar.exp())</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Total loss with β weighting</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> recon_loss <span class="op">+</span> beta <span class="op">*</span> kl_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>By adjusting the β parameter, we can control the trade-off between reconstruction quality and the “niceness” of our latent space. Higher β values force the latent space to be more like a standard normal distribution, while lower values prioritize reconstruction accuracy.</p>
<p>This gives us a powerful tool for exploring the manifold of spectroscopic data - not just finding it, but mapping it in a way that makes it useful for generation, interpolation, and understanding the underlying physical parameters.</p>
</section>
</section>
<section id="conclusion-the-journey-continues" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-the-journey-continues">Conclusion: The Journey Continues</h2>
<p>Our archaeological expedition through the world of autoencoders has revealed powerful tools for uncovering the hidden structure in spectroscopic data. We’ve seen how:</p>
<ol type="1">
<li>Linear autoencoders connect to classical methods like PCA</li>
<li>Nonlinear autoencoders can capture complex manifold structures</li>
<li>Variational autoencoders add a probabilistic perspective that enables generation and interpolation</li>
</ol>
<p>Just as archaeologists piece together ancient civilizations from fragments, we can piece together the underlying molecular and material properties from noisy, complex spectral data.</p>
<p>And just like archaeology, the field continues to evolve with new techniques and approaches. From graph neural networks to attention mechanisms to diffusion models, the tools for spectroscopic data analysis keep getting more sophisticated - allowing us to uncover ever more subtle patterns and relationships in our molecular artifacts.</p>
<p>So grab your digital trowel and start digging!</p>
<div id="cell-fig-vae" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div id="fig-vae" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-vae-output-1.png" width="1142" height="565" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Visualization of latent space sampling in a VAE, showing how we can generate new spectra.
</figcaption>
</figure>
</div>
</div>
</div>
<p>You can find a short lecture on this on <a href="https://youtu.be/fibGQX3nlM0?si=cWmN3VQnBLtEu5j2">YouTube</a>. <iframe width="560" height="315" src="https://www.youtube.com/embed/fibGQX3nlM0?si=sXc7Ne5f7mBSMITn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kjablonka\.com\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb9" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Autoencoders as Digital Archaeologists for Spectroscopic Data"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "An archaeological journey through molecular and material landscapes"</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "cover.png"</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co">  "4/23/2025"</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="an">sidebar:</span><span class="co"> false</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - machine-learning</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - teaching</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: setup</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction </span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>Picture this: An archaeologist stands at a dig site, surrounded by layers of earth that haven't seen sunlight since dinosaurs were a hot new trend. With painstaking care, they brush away dirt and sediment, revealing pottery shards, and that one graduate student who fell asleep on the job.</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="al">![The process of archeology](archeology_process.png)</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>Now imagine replacing dirt with noise, shards with molecular signatures, and the reconstructed vase with a clean spectrum. Welcome to the world of spectroscopic data analysis using autoencoders: where we excavate molecular treasures from layers of noise and complexity through what the fancy folks call "unsupervised representation learning" (which is "teaching computers to find patterns without telling them what patterns to find").</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>Every spectroscopic measurement is like an archaeological dig, except instead of finding ancient coins, we're finding molecular transitions between energy states.</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>But just as ancient artifacts come to us covered in dirt and damaged by time (and occasionally by that one archaeologist who thought dynamite was a good excavation tool), our spectroscopic data arrives buried under multiple layers of contamination: such as </span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*noise* due to random fluctuations ("electrons having a dance party")</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*environmental interference*: for example, Water vapor and CO₂ absorption bands</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*instrumental artifacts*: baseline drift ("detector getting tired") </span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*physical degradation*: sample fluorescence and aging effects</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>Traditional smoothing techniques are the equivalent of using a bulldozer to dust off a delicate vase. Sure, you'll remove the dirt, but you might also remove, well, everything else. One usecase of autoencoder is to do this in a better way. </span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Digital Archaeologist's Toolkit</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>In our metaphor, an autoencoder is like a three-phase archaeological expedition.</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="fu">### Phase 1: The Excavation (Encoding)</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a><span class="al">![Comparison of archeological and spectroscopic excavations.](excavation_comp.png)</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>The first phase in our archeology mission begins by cleaning the artifacts we find: removing the sand, cleaning them. We can think of it as removing the unnecessary. Similarly, our spectroscopic application starts with removing the unnecessary and "compressing" the spectrum to its essentials. </span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>In the simplest form this can be done using a sequence of linear layers: </span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExcavationBrush(nn.Module):</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, spectral_channels<span class="op">=</span><span class="dv">1000</span>, artifact_dimensions<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.compressor <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>            nn.Linear(spectral_channels, <span class="dv">512</span>),</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">512</span>),</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>),</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>),</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, artifact_dimensions)</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, buried_spectrum):</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.compressor(buried_spectrum)</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>The encoder takes our high-dimensional spectrum and compresses it into something more manageable. But this isn't a simple compression algorithm: the model learns which features matter most, like an experienced archaeologist who can tell the difference between "priceless artifact" and "rock that looks vaguely interesting."</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a><span class="fu">### Phase 2: The latent space </span></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a><span class="al">![The archeological vs. spectroscopic latent space](latent_space_comp.png)</span></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>The latent space is our archaeological museum's storage room—not the fancy public galleries with mood lighting and gift shops, but the back room where things actually get done. Here, each spectrum becomes a neat little index card with just the essential information.</span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a><span class="co"># In our latent space, each spectrum becomes coordinates on an ancient map</span></span>
<span id="cb9-99"><a href="#cb9-99" aria-hidden="true" tabindex="-1"></a><span class="co"># It's like Google Maps, but for molecules</span></span>
<span id="cb9-100"><a href="#cb9-100" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode_spectrum(encoder, noisy_spectrum):</span>
<span id="cb9-101"><a href="#cb9-101" aria-hidden="true" tabindex="-1"></a>    latent_artifacts <span class="op">=</span> encoder(noisy_spectrum)  <span class="co"># Returns a point in hyperspace</span></span>
<span id="cb9-102"><a href="#cb9-102" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> latent_artifacts</span>
<span id="cb9-103"><a href="#cb9-103" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-104"><a href="#cb9-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-105"><a href="#cb9-105" aria-hidden="true" tabindex="-1"></a>But this is no ordinary storage room. It's a magical space where similar artifacts naturally cluster together, like teenagers at a high school cafeteria. Polymers hang out in their valley, ceramics claim the mountain peaks, and metal oxides spread across their plains like they own the place.</span>
<span id="cb9-106"><a href="#cb9-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-107"><a href="#cb9-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-108"><a href="#cb9-108" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb9-109"><a href="#cb9-109" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Math Behind Clustering</span></span>
<span id="cb9-110"><a href="#cb9-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-111"><a href="#cb9-111" aria-hidden="true" tabindex="-1"></a>The clustering emerges from what we call the <span class="co">[</span><span class="ot">manifold hypothesis</span><span class="co">](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)</span>—the idea that high-dimensional data actually lives on a lower-dimensional surface. </span>
<span id="cb9-112"><a href="#cb9-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-113"><a href="#cb9-113" aria-hidden="true" tabindex="-1"></a>Mathematically, our encoder learns a mapping:</span>
<span id="cb9-114"><a href="#cb9-114" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-115"><a href="#cb9-115" aria-hidden="true" tabindex="-1"></a>f_\phi: \mathcal{X} \rightarrow \mathcal{Z}</span>
<span id="cb9-116"><a href="#cb9-116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-117"><a href="#cb9-117" aria-hidden="true" tabindex="-1"></a>Where $\mathcal{X} \subset \mathbb{R}^n$ is where our data lives (the messy real world) and $\mathcal{Z} \subset \mathbb{R}^m$ is our nice, clean latent space.</span>
<span id="cb9-118"><a href="#cb9-118" aria-hidden="true" tabindex="-1"></a>This mapping preserves important properties:</span>
<span id="cb9-119"><a href="#cb9-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-120"><a href="#cb9-120" aria-hidden="true" tabindex="-1"></a>**Distance preservation**: Similar inputs map to nearby points</span>
<span id="cb9-121"><a href="#cb9-121" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-122"><a href="#cb9-122" aria-hidden="true" tabindex="-1"></a>d_{\mathcal{Z}}(f_\phi(x_i), f_\phi(x_j)) \approx d_{\mathcal{X}}(x_i, x_j)</span>
<span id="cb9-123"><a href="#cb9-123" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-124"><a href="#cb9-124" aria-hidden="true" tabindex="-1"></a>**Continuity**: Small changes in input create small changes in output</span>
<span id="cb9-125"><a href="#cb9-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-126"><a href="#cb9-126" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-127"><a href="#cb9-127" aria-hidden="true" tabindex="-1"></a>\|f_\phi(x_1) - f_\phi(x_2)\| \leq L\|x_1 - x_2\|</span>
<span id="cb9-128"><a href="#cb9-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-129"><a href="#cb9-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-130"><a href="#cb9-130" aria-hidden="true" tabindex="-1"></a>(The Lipschitz condition.)</span>
<span id="cb9-131"><a href="#cb9-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-132"><a href="#cb9-132" aria-hidden="true" tabindex="-1"></a>So materials with similar spectra end up as neighbors in latent space, forming these natural clusters. It's like chemical social networking!</span>
<span id="cb9-133"><a href="#cb9-133" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-134"><a href="#cb9-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-135"><a href="#cb9-135" aria-hidden="true" tabindex="-1"></a><span class="fu">### Phase 3: Bringing it back together (Reconstruction)</span></span>
<span id="cb9-136"><a href="#cb9-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-137"><a href="#cb9-137" aria-hidden="true" tabindex="-1"></a><span class="al">![Archeological vs. spectroscopic reconstruction.](reconstruction.png)</span></span>
<span id="cb9-138"><a href="#cb9-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-139"><a href="#cb9-139" aria-hidden="true" tabindex="-1"></a>Using only our compressed representation (those index cards), we attempt to reconstruct the original spectrum. It's like trying to rebuild a dinosaur from a few bones and a lot of imagination, except our imagination is constrained by mathematics.</span>
<span id="cb9-140"><a href="#cb9-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-141"><a href="#cb9-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-144"><a href="#cb9-144" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-145"><a href="#cb9-145" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb9-146"><a href="#cb9-146" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Reconstructor(nn.Module):</span>
<span id="cb9-147"><a href="#cb9-147" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, artifact_dimensions<span class="op">=</span><span class="dv">32</span>, spectral_channels<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb9-148"><a href="#cb9-148" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-149"><a href="#cb9-149" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-150"><a href="#cb9-150" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reconstruction_process <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-151"><a href="#cb9-151" aria-hidden="true" tabindex="-1"></a>            nn.Linear(artifact_dimensions, <span class="dv">256</span>),</span>
<span id="cb9-152"><a href="#cb9-152" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-153"><a href="#cb9-153" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>),</span>
<span id="cb9-154"><a href="#cb9-154" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-155"><a href="#cb9-155" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>),</span>
<span id="cb9-156"><a href="#cb9-156" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-157"><a href="#cb9-157" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">512</span>),</span>
<span id="cb9-158"><a href="#cb9-158" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-159"><a href="#cb9-159" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, spectral_channels),</span>
<span id="cb9-160"><a href="#cb9-160" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid() </span>
<span id="cb9-161"><a href="#cb9-161" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-162"><a href="#cb9-162" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-163"><a href="#cb9-163" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, artifact_description):</span>
<span id="cb9-164"><a href="#cb9-164" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.reconstruction_process(artifact_description)</span>
<span id="cb9-165"><a href="#cb9-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-166"><a href="#cb9-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-167"><a href="#cb9-167" aria-hidden="true" tabindex="-1"></a>The decoder takes our compressed representation and attempts to rebuild the original spectrum. If we've done our job right (and haven't accidentally trained our network to just output pictures of cats), the reconstruction should be faithful to the original.</span>
<span id="cb9-168"><a href="#cb9-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-169"><a href="#cb9-169" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Mathematics of Archaeological Documentation</span></span>
<span id="cb9-170"><a href="#cb9-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-171"><a href="#cb9-171" aria-hidden="true" tabindex="-1"></a>Just as physical conservation laws govern the preservation of matter and energy (thanks, Emmy Noether!), information theory dictates how we can compress and reconstruct data without turning it into digital gibberish.</span>
<span id="cb9-172"><a href="#cb9-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-173"><a href="#cb9-173" aria-hidden="true" tabindex="-1"></a>The fundamental equation governing our autoencoder is the reconstruction loss:</span>
<span id="cb9-174"><a href="#cb9-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-175"><a href="#cb9-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-176"><a href="#cb9-176" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{reconstruction}} = \|x - \hat{x}\|^2</span>
<span id="cb9-177"><a href="#cb9-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-178"><a href="#cb9-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-179"><a href="#cb9-179" aria-hidden="true" tabindex="-1"></a>Where $(x)$ is our original spectrum (the truth, the whole truth, and nothing but the truth) and $(\hat{x})$ is our reconstruction.</span>
<span id="cb9-180"><a href="#cb9-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-181"><a href="#cb9-181" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb9-182"><a href="#cb9-182" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why MSE Makes Statistical Sense</span></span>
<span id="cb9-183"><a href="#cb9-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-184"><a href="#cb9-184" aria-hidden="true" tabindex="-1"></a>Let me tell you a tale about why MSE and Gaussian noise are BFFs. </span>
<span id="cb9-185"><a href="#cb9-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-186"><a href="#cb9-186" aria-hidden="true" tabindex="-1"></a>If we assume our noise is Gaussian with mean 0 and variance $\sigma^2$:</span>
<span id="cb9-187"><a href="#cb9-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-188"><a href="#cb9-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-189"><a href="#cb9-189" aria-hidden="true" tabindex="-1"></a>p(x|z) = \mathcal{N}(x; f_\theta(z), \sigma^2I)</span>
<span id="cb9-190"><a href="#cb9-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-191"><a href="#cb9-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-192"><a href="#cb9-192" aria-hidden="true" tabindex="-1"></a>The likelihood for a single data point becomes:</span>
<span id="cb9-193"><a href="#cb9-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-194"><a href="#cb9-194" aria-hidden="true" tabindex="-1"></a>p(x|z) = \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{\|x - f_\theta(z)\|^2}{2\sigma^2}\right)</span>
<span id="cb9-195"><a href="#cb9-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-196"><a href="#cb9-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-197"><a href="#cb9-197" aria-hidden="true" tabindex="-1"></a>Taking the negative log-likelihood:</span>
<span id="cb9-198"><a href="#cb9-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-199"><a href="#cb9-199" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-200"><a href="#cb9-200" aria-hidden="true" tabindex="-1"></a>-\log p(x|z) = \frac{n}{2}\log(2\pi\sigma^2) + \frac{\|x - f_\theta(z)\|^2}{2\sigma^2}</span>
<span id="cb9-201"><a href="#cb9-201" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-202"><a href="#cb9-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-203"><a href="#cb9-203" aria-hidden="true" tabindex="-1"></a>Since the first term is constant w.r.t. θ (our parameters), minimizing negative log-likelihood is equivalent to minimizing:</span>
<span id="cb9-204"><a href="#cb9-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-205"><a href="#cb9-205" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-206"><a href="#cb9-206" aria-hidden="true" tabindex="-1"></a>\frac{\|x - f_\theta(z)\|^2}{2\sigma^2}</span>
<span id="cb9-207"><a href="#cb9-207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-208"><a href="#cb9-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-209"><a href="#cb9-209" aria-hidden="true" tabindex="-1"></a>Which is just MSE in a fancy hat! So when you use MSE loss, you're implicitly assuming Gaussian noise. </span>
<span id="cb9-210"><a href="#cb9-210" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-211"><a href="#cb9-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-212"><a href="#cb9-212" aria-hidden="true" tabindex="-1"></a>But we can be fancier with a composite loss function:</span>
<span id="cb9-213"><a href="#cb9-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-214"><a href="#cb9-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-215"><a href="#cb9-215" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{total}} = \underbrace{\|x - \hat{x}\|^2}_{\text{Be accurate}} + \lambda_1 \underbrace{\|\nabla x - \nabla \hat{x}\|^2}_{\text{Be smooth}} + \lambda_2 \underbrace{\sum_{p \in \text{peaks}} |x_p - \hat{x_p}|}_{\text{Don't mess up the peaks}} + \lambda_3 \underbrace{\mathcal{R}(\phi, \theta)}_{\text{Don't go crazy}}</span>
<span id="cb9-216"><a href="#cb9-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-217"><a href="#cb9-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-218"><a href="#cb9-218" aria-hidden="true" tabindex="-1"></a>Each term has a job:</span>
<span id="cb9-219"><a href="#cb9-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-220"><a href="#cb9-220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fidelity term: "Make it look like the original"</span>
<span id="cb9-221"><a href="#cb9-221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gradient penalty: "Keep it smooth, no sudden jumps"</span>
<span id="cb9-222"><a href="#cb9-222" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Feature preservation: "Those peaks are important, don't lose them!"</span>
<span id="cb9-223"><a href="#cb9-223" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regularization: "Stay humble, don't overfit"</span>
<span id="cb9-224"><a href="#cb9-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-225"><a href="#cb9-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-226"><a href="#cb9-226" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Manifold Hypothesis: Why This Archaeological Dig Makes Sense At All</span></span>
<span id="cb9-227"><a href="#cb9-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-228"><a href="#cb9-228" aria-hidden="true" tabindex="-1"></a>Let's address a fundamental question: why should this even work? Shouldn't compressing our beautiful high-dimensional spectrum lose valuable information? Welcome to the manifold hypothesis, the reason dimensionality reduction isn't just mathematical vandalism.</span>
<span id="cb9-229"><a href="#cb9-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-230"><a href="#cb9-230" aria-hidden="true" tabindex="-1"></a>The manifold hypothesis suggests that high-dimensional data (like our spectroscopic signals) aren't actually using all those dimensions effectively. Instead, the data lies on or near a lower-dimensional surface (a manifold) embedded in that high-dimensional space. It's like discovering that what looks like a complex 3D sculpture is actually just a cleverly folded 2D sheet of paper.</span>
<span id="cb9-231"><a href="#cb9-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-232"><a href="#cb9-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-233"><a href="#cb9-233" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb9-234"><a href="#cb9-234" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why spectroscopic data probably lives on a manifold</span></span>
<span id="cb9-235"><a href="#cb9-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-236"><a href="#cb9-236" aria-hidden="true" tabindex="-1"></a>Spectroscopic data is fundamentally constrained by:</span>
<span id="cb9-237"><a href="#cb9-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-238"><a href="#cb9-238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Physics**: Certain combinations of absorption bands are physically impossible due to quantum mechanical selection rules. You can't just have arbitrary patterns of peaks!</span>
<span id="cb9-239"><a href="#cb9-239" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Chemistry**: Molecular structures create specific patterns of vibrations, rotations, and electronic transitions. A carbonyl group will always give you that telltale peak around 1700 cm⁻¹ in IR spectroscopy. And the space of possible chemicals is constrained (you cannot combine all atoms in all possible ways)</span>
<span id="cb9-240"><a href="#cb9-240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Instrumental limitations**: Your spectrometer has a specific resolution and response function, further constraining the space of possible measurements.</span>
<span id="cb9-241"><a href="#cb9-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-242"><a href="#cb9-242" aria-hidden="true" tabindex="-1"></a>These constraints mean that despite having thousands of wavelength points, your spectrum is likely determined by a much smaller number of underlying variables—chemical compositions, molecular structures, temperature, etc.</span>
<span id="cb9-243"><a href="#cb9-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-244"><a href="#cb9-244" aria-hidden="true" tabindex="-1"></a>Mathematically, if your spectral data points $<span class="sc">\{</span>x_1, x_2, \dots, x_n<span class="sc">\}</span> \in \mathbb{R}^d$ (where d might be thousands of wavelengths), they likely lie on or near a $k$-dimensional manifold $\mathcal{M} \subset \mathbb{R}^d$ where $k\ll d$.</span>
<span id="cb9-245"><a href="#cb9-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-246"><a href="#cb9-246" aria-hidden="true" tabindex="-1"></a>The goal of our autoencoder is to learn this manifold—the archaeological site map, if you will.</span>
<span id="cb9-247"><a href="#cb9-247" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-248"><a href="#cb9-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-249"><a href="#cb9-249" aria-hidden="true" tabindex="-1"></a>To visualize this, imagine our spectra are actually faces of ancient masks (stay with me here). Each mask has thousands of pixels (dimensions), but you could describe any mask with far fewer parameters: eye size, mouth width, nose shape, etc. That's your manifold! Autoencoders discover these "facial features" of spectra automatically. </span>
<span id="cb9-250"><a href="#cb9-250" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">You might be familiar with [eigenfaces](https://en.wikipedia.org/wiki/Eigenface), which are "basis vectors" of human faces one can derive with PCA.</span><span class="co">]</span>{.aside}</span>
<span id="cb9-251"><a href="#cb9-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-254"><a href="#cb9-254" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-255"><a href="#cb9-255" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb9-256"><a href="#cb9-256" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-manifold</span></span>
<span id="cb9-257"><a href="#cb9-257" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Illustration of how data might lie on a lower-dimensional manifold in a higher-dimensional space."</span></span>
<span id="cb9-258"><a href="#cb9-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-259"><a href="#cb9-259" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Swiss roll dataset</span></span>
<span id="cb9-260"><a href="#cb9-260" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb9-261"><a href="#cb9-261" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb9-262"><a href="#cb9-262" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="fl">1.5</span> <span class="op">*</span> np.pi <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.random.rand(n_samples))</span>
<span id="cb9-263"><a href="#cb9-263" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">21</span> <span class="op">*</span> np.random.rand(n_samples)</span>
<span id="cb9-264"><a href="#cb9-264" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> t <span class="op">*</span> np.cos(t)</span>
<span id="cb9-265"><a href="#cb9-265" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> t <span class="op">*</span> np.sin(t)</span>
<span id="cb9-266"><a href="#cb9-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-267"><a href="#cb9-267" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the figure</span></span>
<span id="cb9-268"><a href="#cb9-268" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb9-269"><a href="#cb9-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-270"><a href="#cb9-270" aria-hidden="true" tabindex="-1"></a><span class="co"># 3D plot</span></span>
<span id="cb9-271"><a href="#cb9-271" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">121</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb9-272"><a href="#cb9-272" aria-hidden="true" tabindex="-1"></a>ax1.scatter(x, y, z, c<span class="op">=</span>t, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb9-273"><a href="#cb9-273" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'High-dimensional data</span><span class="ch">\n</span><span class="st">(but structured on a manifold)'</span>)</span>
<span id="cb9-274"><a href="#cb9-274" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb9-275"><a href="#cb9-275" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Y'</span>)</span>
<span id="cb9-276"><a href="#cb9-276" aria-hidden="true" tabindex="-1"></a>ax1.set_zlabel(<span class="st">'Z'</span>)</span>
<span id="cb9-277"><a href="#cb9-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-278"><a href="#cb9-278" aria-hidden="true" tabindex="-1"></a><span class="co"># 2D plot (unrolled manifold)</span></span>
<span id="cb9-279"><a href="#cb9-279" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">122</span>)</span>
<span id="cb9-280"><a href="#cb9-280" aria-hidden="true" tabindex="-1"></a>ax2.scatter(t, y, c<span class="op">=</span>t, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb9-281"><a href="#cb9-281" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Unrolled lower-dimensional representation</span><span class="ch">\n</span><span class="st">(the latent space)'</span>)</span>
<span id="cb9-282"><a href="#cb9-282" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'θ (angle)'</span>)</span>
<span id="cb9-283"><a href="#cb9-283" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Height'</span>)</span>
<span id="cb9-284"><a href="#cb9-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-285"><a href="#cb9-285" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-286"><a href="#cb9-286" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-287"><a href="#cb9-287" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-288"><a href="#cb9-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-289"><a href="#cb9-289" aria-hidden="true" tabindex="-1"></a><span class="fu">## From Classical to Neural: The Connection Between PCA and Linear Autoencoders</span></span>
<span id="cb9-290"><a href="#cb9-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-291"><a href="#cb9-291" aria-hidden="true" tabindex="-1"></a>Long before neural networks were cool, archaeologists (well, statisticians) had their own dimensionality reduction technique: Principal Component Analysis, or PCA. </span>
<span id="cb9-292"><a href="#cb9-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-293"><a href="#cb9-293" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb9-294"><a href="#cb9-294" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Mathematical Connection Between PCA and Linear Autoencoders</span></span>
<span id="cb9-295"><a href="#cb9-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-296"><a href="#cb9-296" aria-hidden="true" tabindex="-1"></a>Let's consider a linear autoencoder with:</span>
<span id="cb9-297"><a href="#cb9-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-298"><a href="#cb9-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input dimension $d$</span>
<span id="cb9-299"><a href="#cb9-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Latent dimension $k$ (where $k &lt; d$)</span>
<span id="cb9-300"><a href="#cb9-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Encoder weight matrix $W_1 \in  \mathbb{R}^{k \times d}$</span>
<span id="cb9-301"><a href="#cb9-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Decoder weight matrix $W_2 \in  \mathbb{R}^{d \times k}$</span>
<span id="cb9-302"><a href="#cb9-302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No biases or activation functions </span>
<span id="cb9-303"><a href="#cb9-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-304"><a href="#cb9-304" aria-hidden="true" tabindex="-1"></a>For an input $x \in \mathbb{R}^d$, the encoding and reconstruction process is:</span>
<span id="cb9-305"><a href="#cb9-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-306"><a href="#cb9-306" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Encode: $z = W_1 x$ (where $z \in \mathbb{R}^k$)</span>
<span id="cb9-307"><a href="#cb9-307" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Decode: $\hat{x} = W_2 z = W_2W_1x$</span>
<span id="cb9-308"><a href="#cb9-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-309"><a href="#cb9-309" aria-hidden="true" tabindex="-1"></a>The reconstruction error we minimize is:</span>
<span id="cb9-310"><a href="#cb9-310" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-311"><a href="#cb9-311" aria-hidden="true" tabindex="-1"></a>\mathcal{L} = \|x - \hat{x}\|^2 = \|x - W_2W_1x\|^2</span>
<span id="cb9-312"><a href="#cb9-312" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-313"><a href="#cb9-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-314"><a href="#cb9-314" aria-hidden="true" tabindex="-1"></a>Under the constraint that $W_1$ and $W_2$ minimize this reconstruction error, the optimal solution has the following properties:</span>
<span id="cb9-315"><a href="#cb9-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-316"><a href="#cb9-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$W_2 = W_1^T$ (the decoder is the transpose of the encoder)</span>
<span id="cb9-317"><a href="#cb9-317" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The rows of $W_1$ are the first k principal components of the data</span>
<span id="cb9-318"><a href="#cb9-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-319"><a href="#cb9-319" aria-hidden="true" tabindex="-1"></a>To see why, let's decompose our data matrix $X$ using SVD:</span>
<span id="cb9-320"><a href="#cb9-320" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-321"><a href="#cb9-321" aria-hidden="true" tabindex="-1"></a>X = U\Sigma V^T</span>
<span id="cb9-322"><a href="#cb9-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-323"><a href="#cb9-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-324"><a href="#cb9-324" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb9-325"><a href="#cb9-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-326"><a href="#cb9-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$U$ contains the left singular vectors</span>
<span id="cb9-327"><a href="#cb9-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Sigma$ contains the singular values on its diagonal</span>
<span id="cb9-328"><a href="#cb9-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$V^T$ contains the right singular vectors</span>
<span id="cb9-329"><a href="#cb9-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-330"><a href="#cb9-330" aria-hidden="true" tabindex="-1"></a>The optimal linear projection to $k$ dimensions is given by:</span>
<span id="cb9-331"><a href="#cb9-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-332"><a href="#cb9-332" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-333"><a href="#cb9-333" aria-hidden="true" tabindex="-1"></a>W_1 = U_k^T</span>
<span id="cb9-334"><a href="#cb9-334" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-335"><a href="#cb9-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-336"><a href="#cb9-336" aria-hidden="true" tabindex="-1"></a>Where $U_k$ contains the first $k$ columns of $U$ (corresponding to the $k$ largest singular values).</span>
<span id="cb9-337"><a href="#cb9-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-338"><a href="#cb9-338" aria-hidden="true" tabindex="-1"></a>And the optimal reconstruction matrix is:</span>
<span id="cb9-339"><a href="#cb9-339" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-340"><a href="#cb9-340" aria-hidden="true" tabindex="-1"></a>W_2 = U_k</span>
<span id="cb9-341"><a href="#cb9-341" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-342"><a href="#cb9-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-343"><a href="#cb9-343" aria-hidden="true" tabindex="-1"></a>Which is exactly $W_1^T$.</span>
<span id="cb9-344"><a href="#cb9-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-345"><a href="#cb9-345" aria-hidden="true" tabindex="-1"></a>Therefore, our reconstructed data is:</span>
<span id="cb9-346"><a href="#cb9-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-347"><a href="#cb9-347" aria-hidden="true" tabindex="-1"></a>\hat{X} = W_2W_1X = U_kU_k^TX</span>
<span id="cb9-348"><a href="#cb9-348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-349"><a href="#cb9-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-350"><a href="#cb9-350" aria-hidden="true" tabindex="-1"></a>Which is precisely the reconstruction you'd get from projecting X onto the first k principal components and back.</span>
<span id="cb9-351"><a href="#cb9-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-352"><a href="#cb9-352" aria-hidden="true" tabindex="-1"></a>This means our linear autoencoder will learn the same subspace as PCA, just with more computational effort and the possibility of getting stuck in local minima. It's like taking a road trip to your neighbor's house—you'll get there, but was the scenic route necessary?</span>
<span id="cb9-353"><a href="#cb9-353" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-354"><a href="#cb9-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-355"><a href="#cb9-355" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Practical Example: Finding the Redundant Dimension</span></span>
<span id="cb9-356"><a href="#cb9-356" aria-hidden="true" tabindex="-1"></a>Let's make this concrete with an example. Imagine we have a spectrum where two neighboring wavelengths always vary together—perhaps due to a broad absorption band or some instrumental correlation.</span>
<span id="cb9-357"><a href="#cb9-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-360"><a href="#cb9-360" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-361"><a href="#cb9-361" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb9-362"><a href="#cb9-362" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dataset with a redundant dimension</span></span>
<span id="cb9-363"><a href="#cb9-363" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_redundant_spectrum(num_samples<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb9-364"><a href="#cb9-364" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Independent features</span></span>
<span id="cb9-365"><a href="#cb9-365" aria-hidden="true" tabindex="-1"></a>    independent_features <span class="op">=</span> np.random.randn(num_samples, <span class="dv">3</span>)</span>
<span id="cb9-366"><a href="#cb9-366" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-367"><a href="#cb9-367" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a 5D spectrum where dimensions 2 and 3 are correlated</span></span>
<span id="cb9-368"><a href="#cb9-368" aria-hidden="true" tabindex="-1"></a>    spectra <span class="op">=</span> np.zeros((num_samples, <span class="dv">5</span>))</span>
<span id="cb9-369"><a href="#cb9-369" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">0</span>] <span class="op">=</span> independent_features[:, <span class="dv">0</span>]  <span class="co"># Independent</span></span>
<span id="cb9-370"><a href="#cb9-370" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">1</span>] <span class="op">=</span> independent_features[:, <span class="dv">1</span>]  <span class="co"># Independent</span></span>
<span id="cb9-371"><a href="#cb9-371" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">2</span>] <span class="op">=</span> independent_features[:, <span class="dv">2</span>]  <span class="co"># Independent</span></span>
<span id="cb9-372"><a href="#cb9-372" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">3</span>] <span class="op">=</span> <span class="fl">0.95</span> <span class="op">*</span> independent_features[:, <span class="dv">2</span>] <span class="op">+</span> <span class="fl">0.05</span> <span class="op">*</span> np.random.randn(num_samples)  <span class="co"># Correlated with dim 2</span></span>
<span id="cb9-373"><a href="#cb9-373" aria-hidden="true" tabindex="-1"></a>    spectra[:, <span class="dv">4</span>] <span class="op">=</span> independent_features[:, <span class="dv">0</span>] <span class="op">-</span> independent_features[:, <span class="dv">1</span>]  <span class="co"># Another linear combination</span></span>
<span id="cb9-374"><a href="#cb9-374" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-375"><a href="#cb9-375" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> spectra</span>
<span id="cb9-376"><a href="#cb9-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-377"><a href="#cb9-377" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a linear autoencoder</span></span>
<span id="cb9-378"><a href="#cb9-378" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearAutoencoder(nn.Module):</span>
<span id="cb9-379"><a href="#cb9-379" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim<span class="op">=</span><span class="dv">5</span>, latent_dim<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb9-380"><a href="#cb9-380" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-381"><a href="#cb9-381" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Linear(input_dim, latent_dim, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># No bias</span></span>
<span id="cb9-382"><a href="#cb9-382" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Linear(latent_dim, input_dim, bias<span class="op">=</span><span class="va">False</span>)  <span class="co"># No bias</span></span>
<span id="cb9-383"><a href="#cb9-383" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-384"><a href="#cb9-384" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-385"><a href="#cb9-385" aria-hidden="true" tabindex="-1"></a>        latent <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb9-386"><a href="#cb9-386" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(latent)</span>
<span id="cb9-387"><a href="#cb9-387" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-388"><a href="#cb9-388" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tie_weights(<span class="va">self</span>):</span>
<span id="cb9-389"><a href="#cb9-389" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This enforces W_2 = W_1^T </span></span>
<span id="cb9-390"><a href="#cb9-390" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder.weight.data <span class="op">=</span> <span class="va">self</span>.encoder.weight.data.t()</span>
<span id="cb9-391"><a href="#cb9-391" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-392"><a href="#cb9-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-393"><a href="#cb9-393" aria-hidden="true" tabindex="-1"></a>When we train this model, it should learn to identify dimension 3 as redundant (since it's nearly identical to dimension 2). Also dimension 4 is only a linear combination of other dimensions. A 3-dimensional latent space will capture all the variance in the 5-dimensional input.</span>
<span id="cb9-394"><a href="#cb9-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-397"><a href="#cb9-397" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-398"><a href="#cb9-398" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: true</span></span>
<span id="cb9-399"><a href="#cb9-399" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-pca</span></span>
<span id="cb9-400"><a href="#cb9-400" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "PCA analysis of the redundant spectrum data, showing the explained variance ratio."</span></span>
<span id="cb9-401"><a href="#cb9-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-402"><a href="#cb9-402" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate redundant spectrum</span></span>
<span id="cb9-403"><a href="#cb9-403" aria-hidden="true" tabindex="-1"></a>spectra <span class="op">=</span> create_redundant_spectrum()</span>
<span id="cb9-404"><a href="#cb9-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-405"><a href="#cb9-405" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply PCA</span></span>
<span id="cb9-406"><a href="#cb9-406" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">5</span>)  <span class="co"># Get all components to see variance</span></span>
<span id="cb9-407"><a href="#cb9-407" aria-hidden="true" tabindex="-1"></a>spectra_reduced <span class="op">=</span> pca.fit_transform(spectra)</span>
<span id="cb9-408"><a href="#cb9-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-409"><a href="#cb9-409" aria-hidden="true" tabindex="-1"></a>pca_three <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb9-410"><a href="#cb9-410" aria-hidden="true" tabindex="-1"></a>spectra_reduced_three <span class="op">=</span> pca_three.fit_transform(spectra)</span>
<span id="cb9-411"><a href="#cb9-411" aria-hidden="true" tabindex="-1"></a>spectra_reconstructed <span class="op">=</span> pca_three.inverse_transform(spectra_reduced_three)</span>
<span id="cb9-412"><a href="#cb9-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-413"><a href="#cb9-413" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate reconstruction error</span></span>
<span id="cb9-414"><a href="#cb9-414" aria-hidden="true" tabindex="-1"></a>reconstruction_error <span class="op">=</span> np.mean((spectra <span class="op">-</span> spectra_reconstructed) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb9-415"><a href="#cb9-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-416"><a href="#cb9-416" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the explained variance</span></span>
<span id="cb9-417"><a href="#cb9-417" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb9-418"><a href="#cb9-418" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>), pca.explained_variance_ratio_)</span>
<span id="cb9-419"><a href="#cb9-419" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Principal Component"</span>)</span>
<span id="cb9-420"><a href="#cb9-420" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Explained Variance Ratio"</span>)</span>
<span id="cb9-421"><a href="#cb9-421" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"PCA Explained Variance (Reconstruction Error: </span><span class="sc">{</span>reconstruction_error<span class="sc">:.6f}</span><span class="ss">)"</span>)</span>
<span id="cb9-422"><a href="#cb9-422" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">6</span>))</span>
<span id="cb9-423"><a href="#cb9-423" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb9-424"><a href="#cb9-424" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-425"><a href="#cb9-425" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-426"><a href="#cb9-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-427"><a href="#cb9-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-428"><a href="#cb9-428" aria-hidden="true" tabindex="-1"></a>In the real world, spectroscopic data often has many such redundancies. Neighboring wavelengths are correlated, certain patterns of peaks occur together, and baseline effects introduce further correlations. These redundancies are exactly what autoencoders exploit—the manifold structure of our data.</span>
<span id="cb9-429"><a href="#cb9-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-430"><a href="#cb9-430" aria-hidden="true" tabindex="-1"></a>The difference is that nonlinear autoencoders can capture more complex manifolds that PCA misses. It's like upgrading from a 2D map to a 3D hologram of your archaeological site.</span>
<span id="cb9-431"><a href="#cb9-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-432"><a href="#cb9-432" aria-hidden="true" tabindex="-1"></a><span class="fu">## Beyond Linear Maps: Where Neural Networks Actually Shine</span></span>
<span id="cb9-433"><a href="#cb9-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-434"><a href="#cb9-434" aria-hidden="true" tabindex="-1"></a>Now we've seen that linear autoencoders are just PCA in disguise, let's talk about why we still bother with neural networks.</span>
<span id="cb9-435"><a href="#cb9-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-436"><a href="#cb9-436" aria-hidden="true" tabindex="-1"></a>The magic happens when we add nonlinearities: those lovely activation functions like ReLU, sigmoid, or tanh. These allow autoencoders to learn complex, curved manifolds that PCA could never dream of capturing.</span>
<span id="cb9-437"><a href="#cb9-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-438"><a href="#cb9-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-441"><a href="#cb9-441" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-442"><a href="#cb9-442" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb9-443"><a href="#cb9-443" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NonlinearArchaeologist(nn.Module):</span>
<span id="cb9-444"><a href="#cb9-444" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim<span class="op">=</span><span class="dv">1000</span>, latent_dim<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb9-445"><a href="#cb9-445" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-446"><a href="#cb9-446" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-447"><a href="#cb9-447" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Now with extra nonlinear goodness!</span></span>
<span id="cb9-448"><a href="#cb9-448" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-449"><a href="#cb9-449" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_dim, <span class="dv">512</span>),</span>
<span id="cb9-450"><a href="#cb9-450" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),  <span class="co"># This is where the magic happens</span></span>
<span id="cb9-451"><a href="#cb9-451" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>),</span>
<span id="cb9-452"><a href="#cb9-452" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),  <span class="co"># More magic!</span></span>
<span id="cb9-453"><a href="#cb9-453" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, latent_dim)</span>
<span id="cb9-454"><a href="#cb9-454" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-455"><a href="#cb9-455" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-456"><a href="#cb9-456" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-457"><a href="#cb9-457" aria-hidden="true" tabindex="-1"></a>            nn.Linear(latent_dim, <span class="dv">256</span>),</span>
<span id="cb9-458"><a href="#cb9-458" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-459"><a href="#cb9-459" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>),</span>
<span id="cb9-460"><a href="#cb9-460" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-461"><a href="#cb9-461" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, input_dim)</span>
<span id="cb9-462"><a href="#cb9-462" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-463"><a href="#cb9-463" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-464"><a href="#cb9-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-465"><a href="#cb9-465" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb9-466"><a href="#cb9-466" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Power of Nonlinearity</span></span>
<span id="cb9-467"><a href="#cb9-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-468"><a href="#cb9-468" aria-hidden="true" tabindex="-1"></a>Consider a simple nonlinear manifold: data points lying on a curved surface, like a <span class="co">[</span><span class="ot">swiss roll</span><span class="co">](https://en.wikipedia.org/wiki/Swiss_roll)</span> or a spiral. Linear methods like PCA can only find a flat subspace that minimizes the average distance to all points.</span>
<span id="cb9-469"><a href="#cb9-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-470"><a href="#cb9-470" aria-hidden="true" tabindex="-1"></a>But with nonlinear transformations, we can "unroll" or "straighten" the manifold.</span>
<span id="cb9-471"><a href="#cb9-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-472"><a href="#cb9-472" aria-hidden="true" tabindex="-1"></a>For autoencoders, this means:</span>
<span id="cb9-473"><a href="#cb9-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-474"><a href="#cb9-474" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The encoder can learn a function $f: \mathbb{R}^d \to \mathbb{R}^m$ that maps the curved manifold to a flat latent space</span>
<span id="cb9-475"><a href="#cb9-475" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The decoder learns the inverse mapping $g: \mathbb{R}^m \to \mathbb{R}^d$ to bring it back</span>
<span id="cb9-476"><a href="#cb9-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-477"><a href="#cb9-477" aria-hidden="true" tabindex="-1"></a>The nonlinear functions effectively learn to "straighten" the manifold in latent space, making it more amenable to analysis and visualization.</span>
<span id="cb9-478"><a href="#cb9-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-479"><a href="#cb9-479" aria-hidden="true" tabindex="-1"></a>It's like being able to translate an ancient text written on a curved vase simply by "unwrapping" it digitally!</span>
<span id="cb9-480"><a href="#cb9-480" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-481"><a href="#cb9-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-482"><a href="#cb9-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-483"><a href="#cb9-483" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Nonlinear Archaeologist's Advantage</span></span>
<span id="cb9-484"><a href="#cb9-484" aria-hidden="true" tabindex="-1"></a>Imagine two archaeological sites with similar artifacts. A traditional archaeologist might classify them identically based on simple metrics. But our advanced neural archaeologist notices subtle nonlinear patterns.</span>
<span id="cb9-485"><a href="#cb9-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-486"><a href="#cb9-486" aria-hidden="true" tabindex="-1"></a>Similarly, nonlinear autoencoders can distinguish between spectral patterns that would be indistinguishable to linear methods. They can capture:</span>
<span id="cb9-487"><a href="#cb9-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-488"><a href="#cb9-488" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Peak shifting** - When peaks move slightly based on local environment</span>
<span id="cb9-489"><a href="#cb9-489" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multiplicative interactions** - When components don't just add linearly</span>
<span id="cb9-490"><a href="#cb9-490" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Complex baselines** - When background signals have complicated, nonlinear forms</span>
<span id="cb9-491"><a href="#cb9-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-492"><a href="#cb9-492" aria-hidden="true" tabindex="-1"></a>This is why, despite the elegance and interpretability of PCA, we still train these complex nonlinear beasts for real spectroscopic data. The archaeology of molecules is rarely a linear affair!</span>
<span id="cb9-493"><a href="#cb9-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-494"><a href="#cb9-494" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Probabilistic Excavation: Variational Autoencoders</span></span>
<span id="cb9-495"><a href="#cb9-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-496"><a href="#cb9-496" aria-hidden="true" tabindex="-1"></a>What if our archaeologist isn't completely certain about what they've found? Enter <span class="co">[</span><span class="ot">the Variational Autoencoder (VAE)</span><span class="co">](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)</span>—the probabilistic archaeologist who deals in uncertainties rather than absolutes.</span>
<span id="cb9-497"><a href="#cb9-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-500"><a href="#cb9-500" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-501"><a href="#cb9-501" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb9-502"><a href="#cb9-502" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ProbabilisticArchaeologist(nn.Module):</span>
<span id="cb9-503"><a href="#cb9-503" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim<span class="op">=</span><span class="dv">1000</span>, latent_dim<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb9-504"><a href="#cb9-504" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-505"><a href="#cb9-505" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-506"><a href="#cb9-506" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encoder produces distribution parameters</span></span>
<span id="cb9-507"><a href="#cb9-507" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_base <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-508"><a href="#cb9-508" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_dim, <span class="dv">512</span>),</span>
<span id="cb9-509"><a href="#cb9-509" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-510"><a href="#cb9-510" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>),</span>
<span id="cb9-511"><a href="#cb9-511" aria-hidden="true" tabindex="-1"></a>            nn.ReLU()</span>
<span id="cb9-512"><a href="#cb9-512" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-513"><a href="#cb9-513" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-514"><a href="#cb9-514" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Two outputs: mean and log-variance</span></span>
<span id="cb9-515"><a href="#cb9-515" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_mu <span class="op">=</span> nn.Linear(<span class="dv">256</span>, latent_dim)</span>
<span id="cb9-516"><a href="#cb9-516" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_logvar <span class="op">=</span> nn.Linear(<span class="dv">256</span>, latent_dim)</span>
<span id="cb9-517"><a href="#cb9-517" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-518"><a href="#cb9-518" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Decoder reconstructs from samples</span></span>
<span id="cb9-519"><a href="#cb9-519" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-520"><a href="#cb9-520" aria-hidden="true" tabindex="-1"></a>            nn.Linear(latent_dim, <span class="dv">256</span>),</span>
<span id="cb9-521"><a href="#cb9-521" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-522"><a href="#cb9-522" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>),</span>
<span id="cb9-523"><a href="#cb9-523" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-524"><a href="#cb9-524" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, input_dim)</span>
<span id="cb9-525"><a href="#cb9-525" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-526"><a href="#cb9-526" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-527"><a href="#cb9-527" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x):</span>
<span id="cb9-528"><a href="#cb9-528" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.encoder_base(x)</span>
<span id="cb9-529"><a href="#cb9-529" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> <span class="va">self</span>.fc_mu(h)         <span class="co"># "I think the artifact is here"</span></span>
<span id="cb9-530"><a href="#cb9-530" aria-hidden="true" tabindex="-1"></a>        logvar <span class="op">=</span> <span class="va">self</span>.fc_logvar(h)  <span class="co"># "But I could be wrong by this much"</span></span>
<span id="cb9-531"><a href="#cb9-531" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu, logvar</span>
<span id="cb9-532"><a href="#cb9-532" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-533"><a href="#cb9-533" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reparameterize(<span class="va">self</span>, mu, logvar):</span>
<span id="cb9-534"><a href="#cb9-534" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The famous reparameterization trick</span></span>
<span id="cb9-535"><a href="#cb9-535" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> logvar)</span>
<span id="cb9-536"><a href="#cb9-536" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> torch.randn_like(std)</span>
<span id="cb9-537"><a href="#cb9-537" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu <span class="op">+</span> eps <span class="op">*</span> std</span>
<span id="cb9-538"><a href="#cb9-538" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-539"><a href="#cb9-539" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-540"><a href="#cb9-540" aria-hidden="true" tabindex="-1"></a>        mu, logvar <span class="op">=</span> <span class="va">self</span>.encode(x)</span>
<span id="cb9-541"><a href="#cb9-541" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.reparameterize(mu, logvar)</span>
<span id="cb9-542"><a href="#cb9-542" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(z), mu, logvar</span>
<span id="cb9-543"><a href="#cb9-543" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-544"><a href="#cb9-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-545"><a href="#cb9-545" aria-hidden="true" tabindex="-1"></a><span class="fu">### Manifold Cartography: The KL Divergence as Map-Making</span></span>
<span id="cb9-546"><a href="#cb9-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-547"><a href="#cb9-547" aria-hidden="true" tabindex="-1"></a>Here's where the VAE truly shines: it doesn't just learn the manifold, it learns a *probabilistic* manifold with a well-behaved coordinate system. The VAE loss function has two terms:</span>
<span id="cb9-548"><a href="#cb9-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-549"><a href="#cb9-549" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-550"><a href="#cb9-550" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{VAE}} = \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction: Make it look right}} - \underbrace{D_{\text{KL}}(q_\phi(z|x) \| p(z))}_{\text{KL divergence: Keep it reasonable}}</span>
<span id="cb9-551"><a href="#cb9-551" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb9-552"><a href="#cb9-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-553"><a href="#cb9-553" aria-hidden="true" tabindex="-1"></a>The first part is our familiar reconstruction loss - "make the reconstruction look like the input."</span>
<span id="cb9-554"><a href="#cb9-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-555"><a href="#cb9-555" aria-hidden="true" tabindex="-1"></a>The second part is the Kullback-Leibler divergence, which measures how much our learned distribution $q_\phi(z|x)$ differs from a prior distribution $p(z)$ (typically a standard normal distribution).</span>
<span id="cb9-556"><a href="#cb9-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-557"><a href="#cb9-557" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb9-558"><a href="#cb9-558" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why the KL Term Matters for the Manifold</span></span>
<span id="cb9-559"><a href="#cb9-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-560"><a href="#cb9-560" aria-hidden="true" tabindex="-1"></a>The KL divergence term in VAEs serves multiple crucial purposes that make it perfect for learning manifolds:</span>
<span id="cb9-561"><a href="#cb9-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-562"><a href="#cb9-562" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**It creates a continuous latent space**: By encouraging overlap between the distributions of similar data points, the KL term ensures that nearby points in input space map to overlapping regions in latent space. This creates a smooth manifold where interpolation makes sense.</span>
<span id="cb9-563"><a href="#cb9-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-564"><a href="#cb9-564" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**It regularizes the coordinate system**: Without the KL term, the autoencoder could learn any arbitrary mapping that preserves information. The KL term acts like a cartographer imposing a standard coordinate system on a newly discovered land.</span>
<span id="cb9-565"><a href="#cb9-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-566"><a href="#cb9-566" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**It enables generative sampling**: By forcing the aggregate posterior to match the prior distribution, we can sample from the prior and generate new data points that lie on the learned manifold - essentially "discovering" new artifacts that could plausibly exist.</span>
<span id="cb9-567"><a href="#cb9-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-568"><a href="#cb9-568" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**It prevents overfitting**: The KL term acts as a complexity penalty that prevents the model from learning an overly complex mapping that might not generalize well.</span>
<span id="cb9-569"><a href="#cb9-569" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb9-570"><a href="#cb9-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-571"><a href="#cb9-571" aria-hidden="true" tabindex="-1"></a>When applied to spectroscopic data, this is particularly powerful because:</span>
<span id="cb9-572"><a href="#cb9-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-573"><a href="#cb9-573" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We can generate new realistic spectra by sampling from the latent space</span>
<span id="cb9-574"><a href="#cb9-574" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We can perform meaningful interpolation between spectra</span>
<span id="cb9-575"><a href="#cb9-575" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We can quantify uncertainty in our representations</span>
<span id="cb9-576"><a href="#cb9-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-579"><a href="#cb9-579" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-580"><a href="#cb9-580" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb9-581"><a href="#cb9-581" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vae_loss(reconstruction, x, mu, logvar, beta<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb9-582"><a href="#cb9-582" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate the VAE loss with reconstruction and KL terms"""</span></span>
<span id="cb9-583"><a href="#cb9-583" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reconstruction loss (how well does the output match the input?)</span></span>
<span id="cb9-584"><a href="#cb9-584" aria-hidden="true" tabindex="-1"></a>    recon_loss <span class="op">=</span> F.mse_loss(reconstruction, x, reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb9-585"><a href="#cb9-585" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-586"><a href="#cb9-586" aria-hidden="true" tabindex="-1"></a>    <span class="co"># KL divergence (how much does our distribution differ from the prior?)</span></span>
<span id="cb9-587"><a href="#cb9-587" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For the standard normal prior, this has a nice closed form</span></span>
<span id="cb9-588"><a href="#cb9-588" aria-hidden="true" tabindex="-1"></a>    kl_loss <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(<span class="dv">1</span> <span class="op">+</span> logvar <span class="op">-</span> mu.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="op">-</span> logvar.exp())</span>
<span id="cb9-589"><a href="#cb9-589" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-590"><a href="#cb9-590" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Total loss with β weighting</span></span>
<span id="cb9-591"><a href="#cb9-591" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> recon_loss <span class="op">+</span> beta <span class="op">*</span> kl_loss</span>
<span id="cb9-592"><a href="#cb9-592" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-593"><a href="#cb9-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-594"><a href="#cb9-594" aria-hidden="true" tabindex="-1"></a>By adjusting the β parameter, we can control the trade-off between reconstruction quality and the "niceness" of our latent space. Higher β values force the latent space to be more like a standard normal distribution, while lower values prioritize reconstruction accuracy.</span>
<span id="cb9-595"><a href="#cb9-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-596"><a href="#cb9-596" aria-hidden="true" tabindex="-1"></a>This gives us a powerful tool for exploring the manifold of spectroscopic data - not just finding it, but mapping it in a way that makes it useful for generation, interpolation, and understanding the underlying physical parameters.</span>
<span id="cb9-597"><a href="#cb9-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-598"><a href="#cb9-598" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion: The Journey Continues</span></span>
<span id="cb9-599"><a href="#cb9-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-600"><a href="#cb9-600" aria-hidden="true" tabindex="-1"></a>Our archaeological expedition through the world of autoencoders has revealed powerful tools for uncovering the hidden structure in spectroscopic data. We've seen how:</span>
<span id="cb9-601"><a href="#cb9-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-602"><a href="#cb9-602" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Linear autoencoders connect to classical methods like PCA</span>
<span id="cb9-603"><a href="#cb9-603" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Nonlinear autoencoders can capture complex manifold structures</span>
<span id="cb9-604"><a href="#cb9-604" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Variational autoencoders add a probabilistic perspective that enables generation and interpolation</span>
<span id="cb9-605"><a href="#cb9-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-606"><a href="#cb9-606" aria-hidden="true" tabindex="-1"></a>Just as archaeologists piece together ancient civilizations from fragments, we can piece together the underlying molecular and material properties from noisy, complex spectral data.</span>
<span id="cb9-607"><a href="#cb9-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-608"><a href="#cb9-608" aria-hidden="true" tabindex="-1"></a>And just like archaeology, the field continues to evolve with new techniques and approaches. From graph neural networks to attention mechanisms to diffusion models, the tools for spectroscopic data analysis keep getting more sophisticated - allowing us to uncover ever more subtle patterns and relationships in our molecular artifacts.</span>
<span id="cb9-609"><a href="#cb9-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-610"><a href="#cb9-610" aria-hidden="true" tabindex="-1"></a>So grab your digital trowel and start digging!</span>
<span id="cb9-611"><a href="#cb9-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-614"><a href="#cb9-614" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb9-615"><a href="#cb9-615" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb9-616"><a href="#cb9-616" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-vae</span></span>
<span id="cb9-617"><a href="#cb9-617" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Visualization of latent space sampling in a VAE, showing how we can generate new spectra."</span></span>
<span id="cb9-618"><a href="#cb9-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-619"><a href="#cb9-619" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate simple toy spectra</span></span>
<span id="cb9-620"><a href="#cb9-620" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_toy_spectra(num_samples<span class="op">=</span><span class="dv">500</span>, x_points<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb9-621"><a href="#cb9-621" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, x_points)</span>
<span id="cb9-622"><a href="#cb9-622" aria-hidden="true" tabindex="-1"></a>    spectra <span class="op">=</span> []</span>
<span id="cb9-623"><a href="#cb9-623" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-624"><a href="#cb9-624" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_samples):</span>
<span id="cb9-625"><a href="#cb9-625" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Random parameters</span></span>
<span id="cb9-626"><a href="#cb9-626" aria-hidden="true" tabindex="-1"></a>        amplitude <span class="op">=</span> np.random.uniform(<span class="fl">0.5</span>, <span class="fl">2.0</span>)</span>
<span id="cb9-627"><a href="#cb9-627" aria-hidden="true" tabindex="-1"></a>        center <span class="op">=</span> np.random.uniform(<span class="dv">3</span>, <span class="dv">7</span>)</span>
<span id="cb9-628"><a href="#cb9-628" aria-hidden="true" tabindex="-1"></a>        width <span class="op">=</span> np.random.uniform(<span class="fl">0.5</span>, <span class="fl">1.5</span>)</span>
<span id="cb9-629"><a href="#cb9-629" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-630"><a href="#cb9-630" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate peak</span></span>
<span id="cb9-631"><a href="#cb9-631" aria-hidden="true" tabindex="-1"></a>        spectrum <span class="op">=</span> amplitude <span class="op">*</span> np.exp(<span class="op">-</span>((x <span class="op">-</span> center) <span class="op">/</span> width)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb9-632"><a href="#cb9-632" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-633"><a href="#cb9-633" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add some noise</span></span>
<span id="cb9-634"><a href="#cb9-634" aria-hidden="true" tabindex="-1"></a>        spectrum <span class="op">+=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.05</span>, x_points)</span>
<span id="cb9-635"><a href="#cb9-635" aria-hidden="true" tabindex="-1"></a>        spectra.append(spectrum)</span>
<span id="cb9-636"><a href="#cb9-636" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-637"><a href="#cb9-637" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(spectra), x</span>
<span id="cb9-638"><a href="#cb9-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-639"><a href="#cb9-639" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb9-640"><a href="#cb9-640" aria-hidden="true" tabindex="-1"></a>spectra, x_axis <span class="op">=</span> generate_toy_spectra()</span>
<span id="cb9-641"><a href="#cb9-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-642"><a href="#cb9-642" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure for VAE visualization</span></span>
<span id="cb9-643"><a href="#cb9-643" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb9-644"><a href="#cb9-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-645"><a href="#cb9-645" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the latent space</span></span>
<span id="cb9-646"><a href="#cb9-646" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> fig.add_subplot(<span class="dv">121</span>)</span>
<span id="cb9-647"><a href="#cb9-647" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate latent space by using PCA for visualization</span></span>
<span id="cb9-648"><a href="#cb9-648" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-649"><a href="#cb9-649" aria-hidden="true" tabindex="-1"></a>latent_points <span class="op">=</span> pca.fit_transform(spectra)</span>
<span id="cb9-650"><a href="#cb9-650" aria-hidden="true" tabindex="-1"></a>ax1.scatter(latent_points[:, <span class="dv">0</span>], latent_points[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb9-651"><a href="#cb9-651" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Latent Space'</span>)</span>
<span id="cb9-652"><a href="#cb9-652" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Latent Dimension 1'</span>)</span>
<span id="cb9-653"><a href="#cb9-653" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Latent Dimension 2'</span>)</span>
<span id="cb9-654"><a href="#cb9-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-655"><a href="#cb9-655" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot some example spectra</span></span>
<span id="cb9-656"><a href="#cb9-656" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> fig.add_subplot(<span class="dv">122</span>)</span>
<span id="cb9-657"><a href="#cb9-657" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb9-658"><a href="#cb9-658" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="bu">len</span>(spectra))</span>
<span id="cb9-659"><a href="#cb9-659" aria-hidden="true" tabindex="-1"></a>    ax2.plot(x_axis, spectra[idx], label<span class="op">=</span><span class="ss">f'Sample </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb9-660"><a href="#cb9-660" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Generated Spectra from Latent Space'</span>)</span>
<span id="cb9-661"><a href="#cb9-661" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Wavelength'</span>)</span>
<span id="cb9-662"><a href="#cb9-662" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Intensity'</span>)</span>
<span id="cb9-663"><a href="#cb9-663" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb9-664"><a href="#cb9-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-665"><a href="#cb9-665" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-666"><a href="#cb9-666" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb9-667"><a href="#cb9-667" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb9-668"><a href="#cb9-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-669"><a href="#cb9-669" aria-hidden="true" tabindex="-1"></a>You can find a short lecture on this on <span class="co">[</span><span class="ot">YouTube</span><span class="co">](https://youtu.be/fibGQX3nlM0?si=cWmN3VQnBLtEu5j2)</span>.</span>
<span id="cb9-670"><a href="#cb9-670" aria-hidden="true" tabindex="-1"></a>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/fibGQX3nlM0?si=sXc7Ne5f7mBSMITn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kevin-maik-jablonka/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kjappelbaum">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://mailhide.io/e/o4LeOUlq">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://scholar.google.com/citations?user=R2ntI8IAAAAJ&amp;hl=en">
      <i class="bi bi-mortarboard-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/kjappelbaum/kjappelbaum.github.io/edit/main/blog/posts/autencoder_spectroscopy/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>