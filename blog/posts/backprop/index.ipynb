{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Developing an intuition for backpropagation'\n",
        "description: <i>Efficient long-distant errorpropagation</i>\n",
        "image: \"\"\n",
        "sidebar: false\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-tools: true\n",
        "---"
      ],
      "id": "4e31b250"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting weights in neural networks \n",
        "\n",
        "When we build neural networks, we tune weights to ensure that the outputs are close to what we want them to be. \n",
        "\n",
        "The power of deep learning is that having many layers of weights allows us to learn very complex functions (i.e. mappings from input to output). \n",
        "\n",
        "Here, we want to understand how to systematically tune the weights to achieve this. \n",
        "\n",
        "\n",
        "```{=html}\n",
        " <style>\n",
        "        .flex-container {\n",
        "            display: flex;\n",
        "            justify-content: center;\n",
        "            align-items: start; /* Adjust this as needed */\n",
        "        }\n",
        "        .slider-container {\n",
        "            flex: 2;\n",
        "            padding: 1px;\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            justify-content: center;\n",
        "        }\n",
        "        .visualization-container {\n",
        "            flex: 2; /* Gives the visualization more room */\n",
        "            padding: 1px;\n",
        "        }\n",
        "        .slider-label {\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "    </style>\n",
        "  <head>\n",
        "    <meta charset=\"UTF-8\" />\n",
        "    <title>Neural Network Visualization</title>\n",
        "    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n",
        "    <style>\n",
        "      .slider-label {\n",
        "        display: block;\n",
        "        margin-top: 10px;\n",
        "      }\n",
        "      #outputLabel {\n",
        "        margin-top: 10px;\n",
        "      }\n",
        "\n",
        "    </style>\n",
        "  </head>\n",
        "  <body>\n",
        "    \n",
        "    <div class=\"flex-container\">\n",
        "        <div class=\"slider-container\">\n",
        "    <div class=\"slider-label\">\n",
        "      Input:\n",
        "      <input\n",
        "        type=\"range\"\n",
        "        min=\"0\"\n",
        "        max=\"1\"\n",
        "        step=\"0.01\"\n",
        "        value=\"0.5\"\n",
        "        id=\"inputSlider\"\n",
        "      />\n",
        "    </div>\n",
        "    <div class=\"slider-label\">\n",
        "      Weight 1-1:\n",
        "      <input\n",
        "        type=\"range\"\n",
        "        min=\"-1\"\n",
        "        max=\"1\"\n",
        "        step=\"0.01\"\n",
        "        value=\"0.5\"\n",
        "        id=\"weight1_1Slider\"\n",
        "      />\n",
        "    </div>\n",
        "    <div class=\"slider-label\">\n",
        "      Weight 1-2:\n",
        "      <input\n",
        "        type=\"range\"\n",
        "        min=\"-1\"\n",
        "        max=\"1\"\n",
        "        step=\"0.01\"\n",
        "        value=\"0.5\"\n",
        "        id=\"weight1_2Slider\"\n",
        "      />\n",
        "    </div>\n",
        "    <div class=\"slider-label\">\n",
        "      Weight 2-1:\n",
        "      <input\n",
        "        type=\"range\"\n",
        "        min=\"-1\"\n",
        "        max=\"1\"\n",
        "        step=\"0.01\"\n",
        "        value=\"0.5\"\n",
        "        id=\"weight2_1Slider\"\n",
        "      />\n",
        "    </div>\n",
        "    <div class=\"slider-label\">\n",
        "      Weight 2-2:\n",
        "      <input\n",
        "        type=\"range\"\n",
        "        min=\"-1\"\n",
        "        max=\"1\"\n",
        "        step=\"0.01\"\n",
        "        value=\"0.5\"\n",
        "        id=\"weight2_2Slider\"\n",
        "      />\n",
        "    </div>\n",
        "    <div class=\"slider-label\">\n",
        "      Target Output:\n",
        "      <input\n",
        "        type=\"range\"\n",
        "        min=\"0\"\n",
        "        max=\"1\"\n",
        "        step=\"0.01\"\n",
        "        value=\"0.5\"\n",
        "        id=\"targetOutputSlider\"\n",
        "      />\n",
        "    </div>\n",
        "    <bf><div id=\"outputLabel\">Loss: 0.0000</div></bf>\n",
        "    </div>\n",
        "    <div class=\"visualization-container\"></div>\n",
        "    <svg id=\"networkVisualization\" width=\"600\" height=\"400\"></svg>\n",
        "    </div>\n",
        "    <script>\n",
        "      document.addEventListener(\"DOMContentLoaded\", function () {\n",
        "        function relu(x) {\n",
        "          return Math.max(0, x);\n",
        "        }\n",
        "\n",
        "        function forwardPass(inputs, weights1, weights2) {\n",
        "          let hiddenLayerInput = [inputs * weights1[0], inputs * weights1[1]];\n",
        "          let hiddenLayerOutput = hiddenLayerInput.map(relu);\n",
        "          let outputLayerInput =\n",
        "            hiddenLayerOutput[0] * weights2[0] +\n",
        "            hiddenLayerOutput[1] * weights2[1];\n",
        "          return outputLayerInput;\n",
        "        }\n",
        "\n",
        "        function computeMSELoss(predicted, target) {\n",
        "          return Math.pow(predicted - target, 2);\n",
        "        }\n",
        "        const colorScale = d3.scaleLinear()\n",
        "            .domain([-1, 0, 1])\n",
        "            .range([\"blue\",\"red\"]);\n",
        "\n",
        "\n",
        "         function drawNetwork(selector, weights1, weights2, inputs, hiddenActivations, outputActivation) {\n",
        "        const svg = d3.select(selector);\n",
        "        svg.selectAll(\"*\").remove(); // Clear previous drawing\n",
        "\n",
        "        const width = +svg.attr(\"width\");\n",
        "        const height = +svg.attr(\"height\");\n",
        "\n",
        "        // Define neuron positions\n",
        "        const positions = {\n",
        "            input: [{x: width * 0.2, y: height / 2, value: inputs}],\n",
        "            hidden: [\n",
        "                {x: width * 0.5, y: height * 0.3, value: hiddenActivations[0]},\n",
        "                {x: width * 0.5, y: height * 0.7, value: hiddenActivations[1]}\n",
        "            ],\n",
        "            output: [{x: width * 0.8, y: height / 2, value: outputActivation[0]}]\n",
        "        };\n",
        "\n",
        "        // Draw connections and labels for weights\n",
        "        positions.input.forEach((inputPos, i) => {\n",
        "            positions.hidden.forEach((hiddenPos, j) => {\n",
        "                svg.append(\"line\")\n",
        "                    .attr(\"x1\", inputPos.x)\n",
        "                    .attr(\"y1\", inputPos.y)\n",
        "                    .attr(\"x2\", hiddenPos.x)\n",
        "                    .attr(\"y2\", hiddenPos.y)\n",
        "                    .attr(\"stroke\", colorScale(weights1[j]))\n",
        "                    .attr(\"stroke-width\", Math.abs(weights1[j]) * 2 + 1);\n",
        "\n",
        "                // Label for weight\n",
        "                svg.append(\"text\")\n",
        "                    .attr(\"x\", (inputPos.x + hiddenPos.x) / 2 -10)\n",
        "                    .attr(\"y\", (inputPos.y + hiddenPos.y) / 2 - (j === 0 ? 20 : -40))\n",
        "                    .attr(\"dy\", \"-5\")\n",
        "                    .attr(\"text-anchor\", \"middle\")\n",
        "                    .text(`weight 1-${j+1}: ${weights1[j].toFixed(2)}`);\n",
        "            });\n",
        "        });\n",
        "\n",
        "        positions.hidden.forEach((hiddenPos, i) => {\n",
        "            svg.append(\"line\")\n",
        "                .attr(\"x1\", hiddenPos.x)\n",
        "                .attr(\"y1\", hiddenPos.y)\n",
        "                .attr(\"x2\", positions.output[0].x)\n",
        "                .attr(\"y2\", positions.output[0].y)\n",
        "                .attr(\"stroke\", colorScale(weights2[i]))\n",
        "                .attr(\"stroke-width\", Math.abs(weights2[i]) * 2 + 1);\n",
        "\n",
        "            // Label for weight\n",
        "            svg.append(\"text\")\n",
        "                .attr(\"x\", (hiddenPos.x + positions.output[0].x) / 2 + 10)\n",
        "                .attr(\"y\", (hiddenPos.y + positions.output[0].y) / 2 - (i === 0 ? 20 : -40))\n",
        "                .attr(\"dy\", \"-5\")\n",
        "                .attr(\"text-anchor\", \"middle\")\n",
        "                .text(`weight 2-${i+1}: ${weights2[i].toFixed(2)}`);\n",
        "        });\n",
        "\n",
        "        // Draw neurons and labels for activations\n",
        "        [...positions.input, ...positions.hidden, ...positions.output].forEach(pos => {\n",
        "            svg.append(\"circle\")\n",
        "                .attr(\"cx\", pos.x)\n",
        "                .attr(\"cy\", pos.y)\n",
        "                .attr(\"r\", 20)\n",
        "                .attr(\"fill\", colorScale(pos.value))\n",
        "                .attr(\"stroke\", \"black\");\n",
        "\n",
        "            // Label for neuron value\n",
        "            svg.append(\"text\")\n",
        "                .attr(\"x\", pos.x)\n",
        "                .attr(\"y\", pos.y)\n",
        "                .attr(\"dy\", \"5\")\n",
        "                .attr(\"text-anchor\", \"middle\")\n",
        "                .attr(\"fill\", \"white\")\n",
        "                .text(pos.value.toFixed(2));\n",
        "        });\n",
        "    }\n",
        "\n",
        "        function updateVisualization() {\n",
        "          let inputs = parseFloat(document.getElementById(\"inputSlider\").value);\n",
        "          let weights1 = [\n",
        "            parseFloat(document.getElementById(\"weight1_1Slider\").value),\n",
        "            parseFloat(document.getElementById(\"weight1_2Slider\").value),\n",
        "          ];\n",
        "          let weights2 = [\n",
        "            parseFloat(document.getElementById(\"weight2_1Slider\").value),\n",
        "            parseFloat(document.getElementById(\"weight2_2Slider\").value),\n",
        "          ];\n",
        "          let targetOutput = parseFloat(\n",
        "            document.getElementById(\"targetOutputSlider\").value\n",
        "          );\n",
        "\n",
        "          let output = forwardPass(inputs, weights1, weights2);\n",
        "          let loss = computeMSELoss(output, targetOutput);\n",
        "\n",
        "          document.getElementById(\n",
        "            \"outputLabel\"\n",
        "          ).innerText = `Loss: ${loss.toFixed(\n",
        "            4\n",
        "          )}`;\n",
        "\n",
        "          drawNetwork(\n",
        "            \"#networkVisualization\",\n",
        "            weights1,\n",
        "            weights2,\n",
        "            inputs,\n",
        "            weights1.map(relu),\n",
        "            [output]\n",
        "          );\n",
        "        }\n",
        "\n",
        "        document.querySelectorAll(\"input[type=range]\").forEach((slider) => {\n",
        "          slider.addEventListener(\"input\", updateVisualization);\n",
        "        });\n",
        "\n",
        "        updateVisualization(); // Initial visualization\n",
        "      });\n",
        "    </script>\n",
        "  </body>\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "When we think of the tiny neural network in the widget below one might think of many different ways for optimizing the weights (line strenghts) of this model.\n",
        "\n",
        "\n",
        "### Option 1: Randomly choose weights \n",
        "One option you might try is to randomly try different weight values to then find one that minimizes the difference between ground truth and prediction (i.e., minimizes the loss). \n",
        "While we might be lucky for this toy example, we can imagine that it might take a long time until we guessed all the weights in a billion-parameter model (e.g. GPT-3) correctly. \n",
        "\n",
        "Using a strategy like a grid search (in which you loop over a range of possible weight values for all weights) will also only work for small models (think of the $100^4$ combinations you would have to just try of 100 trial values for 4 weights). \n",
        "\n",
        "### Option 2: Using numerical gradients \n",
        "\n",
        "When we think of our neural network, the loss forms a landscape, that can be very complex. In our simple example below, it looks as follows:\n"
      ],
      "id": "89621d05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def forward_pass(inputs, weights1, weights2, record_activation=False):\n",
        "    hidden_layer_input = np.dot(inputs, weights1)\n",
        "    hidden_layer_output = relu(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights2)\n",
        "    output = linear(output_layer_input)\n",
        "    if record_activation:\n",
        "        return output, hidden_layer_output\n",
        "    return output\n",
        "\n",
        "def compute_mse_loss(predicted, target):\n",
        "    loss =  np.mean(np.square(predicted - target))\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Simplify the scenario for clear visualization\n",
        "# Set the target output and input\n",
        "target = 1.9\n",
        "input_val = 0.8  # A simple input value to keep the forward pass straightforward\n",
        "\n",
        "# Define a range for weight updates that centers around an expected minimum\n",
        "weight_range = 3.0  # Explore weights within [-2, 2] for both weights\n",
        "num_steps = 100  # Increase the number of steps for finer resolution\n",
        "step_size = weight_range / num_steps\n",
        "\n",
        "weight1_1_range = np.linspace(0, weight_range, 2 * num_steps + 1)  # Start from 0 to weight_range\n",
        "weight2_1_range = np.linspace(-weight_range, weight_range, 2 * num_steps + 1)  # Keep full range for weight2_1\n",
        "weight1_1_vals, weight2_1_vals = np.meshgrid(weight1_1_range, weight2_1_range)\n",
        "\n",
        "fixed_weight1_2 = 1.2\n",
        "fixed_weight2_2 = 0.8\n",
        "losses = np.zeros((len(weight1_1_range), len(weight2_1_range)))\n",
        "# Recalculate the losses with the updated range\n",
        "for i in range(len(weight1_1_range)):\n",
        "    for j in range(len(weight2_1_range)):\n",
        "        current_weights1 = np.array([weight1_1_vals[i, j], fixed_weight1_2])\n",
        "        current_weights2 = np.array([weight2_1_vals[i, j], fixed_weight2_2])\n",
        "        output = forward_pass(np.array([[input_val]]), current_weights1.reshape(1, 2), current_weights2.reshape(2, 1))\n",
        "        losses[i, j] = compute_mse_loss(output, np.array([[target]]))\n",
        "\n",
        "# Create a 3D surface plot to visualize the loss landscape\n",
        "fig = go.Figure(data=[go.Surface(z=losses, x=weight1_1_range, y=weight2_1_range, colorscale='RdBu_r')])\n",
        "\n",
        "fig.update_layout(title='Loss Landscape', autosize=False,\n",
        "                  width=800, height=600,\n",
        "                  margin=dict(l=65, r=50, b=65, t=90),\n",
        "                  scene=dict(\n",
        "                      xaxis_title='Weight 1-1',\n",
        "                      yaxis_title='Weight 2-1',\n",
        "                      zaxis_title='Loss'))\n",
        "\n",
        "# Plotting the contour as a 2D projection\n",
        "fig.add_trace(go.Contour(z=losses, x=weight1_1_range, y=weight2_1_range,\n",
        "                         contours_coloring='lines', line_width=2, colorscale='RdBu_r',\n",
        "                         contours=dict(start=0.001, end=losses.max(), size=20),\n",
        "                         colorbar=dict(title='Loss')))\n",
        "\n",
        "fig.show()"
      ],
      "id": "1072b8ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create this plot, we keep two weights fixed, vary two others and then analyze how the loss looks like.\n",
        "We see that there is a clear structure that might remind us of a hilly landscape. \n",
        "\n",
        "With the random search we have been randomly jumping around on this landscape. But seeing this image, we might also decide that we want to follow the path downhill; ultimately, our goal is to find the valley (the lowest loss).\n",
        "That is, the best value to try next should not be a random one but one downhill from where we are now.\n",
        "\n",
        "This direction (\"downhill\") is the slope of our hilly landscape, i.e. the gradient.\n",
        "\n",
        "$$\n",
        "\\frac{\\mathrm{d}f(x)}{\\mathrm{d}x} = \\lim_{h\\to0} \\frac{f(x+h) - f(x)}{h}\n",
        "$$\n",
        "\n",
        "Based on the formula above, we might decide to compute a gradient numerically using [finite differences](https://en.wikipedia.org/wiki/Finite_difference).\n",
        "\n",
        "The problem is that we need to perform _many evaluations_ of the loss to make it work (one per weight, which can be a lot for current frontier models). In addition, we add up errors because $h$ will be different from $0$ (truncation error) and because be have to work with machine precision and hence add rounding errors.\n",
        "\n",
        "\n",
        "![If we compute numerical gradients, we have two main sources of error. One stems from the fact that $h$ in the euqation above is not exactly 0. This is known as truncation error. On the other hand, the finite difference equation leads to numberical problems (rounding errors) as two almost identical numbers are substracted and then divided by a very small number. Figure taken from [Baydin et al.](http://arxiv.org/abs/1502.05767)](errors.png)\n",
        "\n",
        "\n",
        "### Option 3: Analytical gradients\n",
        "\n",
        "Obviously, we could save many evaluations when we could write down the derviates for a given functions. \n",
        "However, for our neural networks we cannot do this by hand. \n",
        "\n",
        "The question is thus how we _efficiently_ compute the gradient of function such as a neural network.\n",
        "\n",
        "## Evaluating analytical gradients for any function: Backpropagation\n",
        "\n",
        "### Calculus 101: Rules for computing derivatives\n",
        "\n",
        "Let's assume \n",
        "\n",
        "$$\n",
        "f(x,y) = xy\n",
        "$$\n",
        "\n",
        "then the _partial derivates_ are \n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x} = y \\quad \\frac{\\partial f}{\\partial y} = x\n",
        "$$\n",
        "\n",
        "The derivative on one variable tells us how sensitive the expression is on its value. To interactively explore this, you can use the widget below. \n",
        "\n",
        "TODO\n",
        "\n",
        "You can enter different functions and then try different points and see what a change in $x$ and $y$ would have as effect on the values and how this is reflected in the partial derivatives.\n",
        "\n",
        "\n",
        "We can also use the animation to explore a few examples for differentiation such as \n",
        "\n",
        "$$\n",
        "f(x) = x^2 + y^2 \\quad \\rightarrow \\quad \\frac{\\partial f}{\\partial x} = 2x \\quad \\frac{\\partial f}{\\partial y} = 2y\n",
        "$$\n",
        "\n",
        "You would see, for $x=y=0$, for instance that the derivatives are zero (because you have a minimum). You also see that the influence of $x$ and $y$ is symmetric and independent from each other.\n",
        "\n",
        "\n",
        "An important rule for differentiation we will need to apply frequently, as it focusses on function composition, is the chain rule \n",
        "\n",
        "$$\n",
        "(g(f(x)))^{\\prime}=(g \\circ f)^{\\prime}(x)=g^{\\prime}(f(x)) f^{\\prime}(x)\n",
        "$$\n",
        "\n",
        "with $g \\circ f$ being function composition $x \\to f(x) \\to g(f(x))$.\n",
        "\n",
        "In the multivariate case, we would write\n",
        "\n",
        "$$\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d} t} f(x(t), y(t))=\\frac{\\partial f}{\\partial x} \\frac{\\mathrm{d} x}{\\mathrm{~d} t}+\\frac{\\partial f}{\\partial y} \\frac{\\mathrm{d} y}{\\mathrm{~d} t}.\n",
        "$$\n",
        "\n",
        "\n",
        "::: {.callout-tip}\n",
        "#### Intuitive understanding of chain rule \n",
        "    \n",
        "How do you intuitively understand that? Let's borrow from [George F. Simmons](https://ia802808.us.archive.org/7/items/GeorgeSimmonsCalculusWithAnalyticGeometry1996McGrawHillScienceEngineeringMath/George%20Simmons%20-%20Calculus%20With%20Analytic%20Geometry%20%281996%2C%20McGraw-Hill%20Science_Engineering_Math%29.pdf):\n",
        "\n",
        "> If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.\n",
        "\n",
        "With \n",
        "\n",
        "- $x$ the position of the car\n",
        "- $y$ the position of the bicycle \n",
        "- $z$ the position of the walking man \n",
        "\n",
        "The rate of change in relative positions is given by terms like $\\frac{\\mathrm{d}x}{\\mathrm{d}y}$, which gives us the change in relative position of bicycle and car. It we now aim to compute the rate of change of relative position of car to the walking man, $\\frac{\\mathrm{d}x}{\\mathrm{d}z}$, we find \n",
        "\n",
        "$$\n",
        "\\frac{\\mathrm{d}x}{\\mathrm{d}x} = \\frac{\\mathrm{d}x}{\\mathrm{d}y} \\frac{\\mathrm{d}y}{\\mathrm{d}z} = \\underbrace{2}_{\\text{car twice as fast as bicycle}} \\cdot \\underbrace{4}_{\\text{bicycle is four times as fast as walking man}} = 8 \n",
        "$$\n",
        "    \n",
        ":::\n",
        "\n",
        "\n",
        "### Computing derivatives as in calculus 101\n",
        "\n",
        "In neural networks, we nest functions. That is, will end up differentiating compound expression of the form \n",
        "\n",
        "$$\n",
        "{\\displaystyle h(x)=f(g(x))}\n",
        "$$\n",
        "\n",
        "For instance, you might look at a simple regularized logistic regression: \n",
        "\n",
        "$$\n",
        "L = \\frac{1}{2}\\left(\\sigma(wx +b) -t \\right)^2 + \\frac{\\lambda}{2} w^2,\n",
        "$$\n",
        "\n",
        "where $\\sigma$ is some activation function (e.g. the sigmoid).\n",
        "\n",
        "If we now want to know what the influence of the weight $w$ is, we can differentiate the loss with respect to $w$: \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial w} &= \\frac{\\partial}{\\partial w} \\left[\\frac{1}{2}\\left(\\sigma(wx +b) -t \\right)^2 + \\frac{\\lambda}{2} w^2 \\right] \\\\\n",
        "&= \\frac{1}{2}\\frac{\\partial}{\\partial w} \\left(\\sigma(wx +b) -t \\right)^2 + \\frac{\\lambda}{2}\\frac{\\partial}{\\partial w} w^2 \\\\ \n",
        "&= \\left(\\sigma(wx+b) - t\\right)\\frac{\\partial}{\\partial w}\\left(\\sigma(wx+b)-t\\right) + \\lambda w \\\\\n",
        "&= \\left(\\sigma(wx+b) - t\\right)\\sigma'(wx +b)\\frac{\\partial}{\\partial w}(wx+b) + \\lambda w \\\\ \n",
        "&= \\left(\\sigma(wx+b) - t\\right)\\sigma'(wx +b)x + \\lambda w\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "Puh! That was a lot of copying and pasting and quite error prone. And it might be quite costly to just directly evaluate such an expression (we might end up with an exponentially large expression, \"expression swell\").\n",
        "\n",
        "There must be a better way.\n",
        "\n",
        "### Making it efficient with caching \n",
        "\n",
        "One thing that we can observe is that we need to do the same computation several times. For instance, $wx +b$ is evaluated two times. We code trade off space and time complexity by caching this using an intermediate variable. \n",
        "\n",
        "If we do this systematically, we can very efficiently compute gradients -- in a form that is symmetric to the computation of the function itself (and those with basically the same cost). \n",
        "\n",
        "#### General computation with intermediate values\n",
        "\n",
        "As a simple example, let's start with \n",
        "\n",
        "$$\n",
        "f(x,y,z) = (x+y)z\n",
        "$$\n",
        "\n",
        "It can be convienient to introduce the following intermediate variable\n",
        "\n",
        "\n",
        "$$\n",
        "p = (x + y) \n",
        "$$\n",
        "\n",
        "We can then write \n",
        "\n",
        "$$\n",
        "f = pz\n",
        "$$\n",
        "\n",
        "and also compute some partial derivatives \n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial q} = z \\quad \\frac{\\partial f}{\\partial z} = q\n",
        "$$\n",
        "\n",
        "and we also know how to differentiate $p$ for $x$ and $y$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial p}{\\partial x} = 1 \\quad \\frac{\\partial p}{\\partial y} =1. \n",
        "$$\n",
        "\n",
        "Using the _chain rule_ we can combine those findings, as the chain rule states that we need to multiply the gradients to chain them: \n",
        "\n",
        "$$\n",
        "\\frac{\\partial f(p,z)}{\\partial x} = \\frac{\\partial f(p, x)}{\\partial p}  \\frac{\\partial p(x,y)}{\\partial x} \n",
        "$$\n",
        "\n",
        "This typically means that two numbers are multiplied. \n",
        "\n",
        "If we try it for the example above we can use the following code. Note how we _cache_ intermediate results (i.e. trade off time- vs. space-complexity). \n"
      ],
      "id": "0404953d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# the inputs we will use \n",
        "x = -2\n",
        "y = 5\n",
        "z = -4\n",
        "\n",
        "# let's compute our intermediate terms\n",
        "t1 = x + y \n",
        "f = t1 * z"
      ],
      "id": "a35144ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can look at the derivatives we got above\n"
      ],
      "id": "ca50123a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "dt1dx = 1.\n",
        "dt1dy = 1.\n",
        "\n",
        "dfdt1 = z\n",
        "dfdz = t1"
      ],
      "id": "6dbccecb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can use the chain rule to combine them \n"
      ],
      "id": "53a64f18"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "dfdx = dfdt1 * dt1dx\n",
        "dfdy = dfdt1 * dt1dy"
      ],
      "id": "a2883684",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sensitivity to $x$, $y$, and $z$ is hence\n"
      ],
      "id": "c996d1f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "print(dfdz, dfdy, dfdz)"
      ],
      "id": "8ef6fad4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also interactively look at this in a circuit diagram \n",
        "TODO\n",
        "\n",
        "\n",
        "How do we gain understanding from this widget?\n",
        "\n",
        "If we assume the setting from above ($x =-2, y =5, z=-4$), then the local gradient for $z$ is 3. Meaning that to increase the output of this circuit, we should increase the value of $z$. \n",
        "\n",
        "Before we move ahead, realize what we did: \n",
        "    \n",
        "We computed gradients by recursively applying the chain rule, starting at the end: \n",
        "\n",
        "- our computation graph is x -> p -> f\n",
        "- we first compute df/dp, then dp/dx. Chaining them gives us df/dx = df/dp dp/dx\n",
        "\n",
        "We can write this in a more general form as follows.\n",
        "\n",
        "If we assume we have $N$ intermediate variables $t_N$, with $t_N$ being our output $f$, by definition we have\n",
        "\n",
        "$$\n",
        "\\frac{\\mathrm{d}{f}}{\\mathrm{d}t_N} = 1\n",
        "$$\n",
        "\n",
        "For the other intermediate variables we have: \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\mathrm{d}f}{\\mathrm{d} t_{n-1}} &= \\frac{\\mathrm{d}f}{\\mathrm{d}f_{n}} \\frac{\\mathrm{d}f_{n}}{\\mathrm{d}t_{n-1}} \\\\\n",
        "\\frac{\\mathrm{d}f}{\\mathrm{d} t_{n-2}} &= \\frac{\\mathrm{d}f}{\\mathrm{d}f_{n}} \\frac{\\mathrm{d}f_{n}}{\\mathrm{d}t_{n-1}} \\frac{\\mathrm{d}f_{n-1}}{\\mathrm{d}t_{n-2}} \\\\\n",
        "\\frac{\\mathrm{d}f}{\\mathrm{d} t_{n-3}} &= \\frac{\\mathrm{d}f}{\\mathrm{d}f_{n}} \\frac{\\mathrm{d}f_{n}}{\\mathrm{d}t_{n-1}} \\frac{\\mathrm{d}f_{n-1}}{\\mathrm{d}t_{n-2}} \\frac{\\mathrm{d}f_{n-2}}{\\mathrm{d}t_{n-3}} \\\\\n",
        "\\frac{\\mathrm{d}f}{\\mathrm{d} t_i} &= \\frac{\\mathrm{d}f}{\\mathrm{d}f_{n}} \\frac{\\mathrm{d}f_{n}}{\\mathrm{d}t_{n-1}} \\frac{\\mathrm{d}f_{n-1}}{\\mathrm{d}t_{n-2}} \\ldots \\frac{\\mathrm{d}f_{i+1}}{\\mathrm{d}t_{i}}\n",
        "\\end{align} \n",
        "$$\n",
        "\n",
        "Note that many of the terms we computed can be reused.\n",
        "\n",
        "\n",
        "### Application to neural networks\n",
        "\n",
        "Neural networks are more complicated circuits -- nested functions.\n",
        "\n",
        "Let's assume a very simply case\n",
        "\n",
        "$$\n",
        "y=\\frac{1}{1+\\exp (-(wx+b))}.\n",
        "$$\n",
        "\n",
        "We can write it using the chaining of the following primitive operations (forming our computation graph). \n",
        "\n",
        "$$\n",
        "t_1 = wx\n",
        "$$\n",
        "$$\n",
        "t_2 = t_1 + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "t_3 = −t_2\n",
        "$$\n",
        "\n",
        "$$\n",
        "t_4 = \\exp(t_3)\n",
        "$$\n",
        "\n",
        "$$\n",
        "t_5 = 1 + t_4\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "t_6 = 1/t_5\n",
        "$$\n",
        "\n",
        "(this list of evaluations is sometimes called evaluation trace or Wengert list).\n",
        "\n",
        "As we would like again get the derivative w.r.t to the output like the loss\n",
        "\n",
        "$$\n",
        "L = (t_6-y)^2, \n",
        "$$\n",
        "\n",
        "which we can write down with some more evaluations \n",
        "\n",
        "$$\n",
        "t_7 = t_6-t\n",
        "$$\n",
        "\n",
        "$$\n",
        "t_8 = t_7^2.\n",
        "$$\n",
        "\n",
        "We call this evaluation the _forward pass_.\n",
        "\n",
        "The beauty of backprop is that the computation for the derivative follows the same structure as the computation of the function itself (and, for example, is not drastically more complex as one might expect). To see this, we can try out: \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial t_8}{\\partial t_8} &= 1 \\\\\n",
        "\\frac{\\partial t_8}{\\partial t_7} &= 2 t_7 \\\\\n",
        "\\frac{\\partial t_7}{\\partial t_6} & = 1 \\\\\n",
        "\\frac{\\partial t_6}{\\partial t_5} &=  -1/t_5^2 \\\\\n",
        "\\frac{\\partial t_5}{\\partial t_4} &= 1\\\\\n",
        "\\frac{\\partial t_4}{\\partial t_3} &= \\exp(t_3) t_3 \\\\\n",
        "\\frac{\\partial t_3}{\\partial t_2} &= - 1\\\\\n",
        "\\frac{\\partial t_2}{\\partial t_1} &= 1 \\\\\n",
        "\\frac{\\partial t_1}{\\partial w} &= x\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Armed with those partial derivatives, we can now multiply them to get the final goal -- the derivative of the loss w.r.t. the weight ($\\frac{\\partial L}{\\partial w}$).\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial t_8}{\\partial t_6} &= \\frac{\\partial t_8}{\\partial t_7} \\frac{\\partial t_7}{\\partial t_6} = 2 t_7 \\cdot 1 = 2(t_6 -y) \\\\\n",
        "\\frac{\\partial t_8}{\\partial t_5} &= \\frac{\\partial t_8}{\\partial t_6} \\frac{\\partial t_6}{\\partial t_5} = 2(t_6 -y) \\cdot  \\left(-\\frac{1}{t_5^2} \\right) =  -2/t_5^2 (t_6 -y) \\\\\n",
        "\\frac{\\partial t_8}{\\partial t_4} &= \\frac{\\partial t_8}{\\partial t_5} \\frac{\\partial t_5}{\\partial t_4} = -2/t_5^2 (t_6 -y) \\cdot 1 = -2/t_5^2 (t_6 -y) \\\\\n",
        "\\frac{\\partial t_8}{\\partial t_3} &= \\frac{\\partial t_8}{\\partial t_4} \\frac{\\partial t_4}{\\partial t_3} = -2/t_5^2 (t_6 -y) \\cdot \\exp(t_3) t_3 = -2/t_5^2 (t_6 -y) \\cdot \\exp(t_3) t_3 \\\\\n",
        "\\frac{\\partial t_8}{\\partial t_2} &= \\frac{\\partial t_8}{\\partial t_3} \\frac{\\partial t_3}{\\partial t_2} = -2/t_5^2 (t_6 -y) \\cdot \\exp(t_3) t_3 \\cdot -1 = 2/t_5^2 (t_6 -y) \\cdot \\exp(t_3) t_3 \\\\\n",
        "\\frac{\\partial t_8}{\\partial t_1} &= \\frac{\\partial t_8}{\\partial t_2} \\frac{\\partial t_2}{\\partial t_1} =  2/t_5^2 (t_6 -y) \\cdot \\exp(t_3) t_3 \\\\\n",
        "\\frac{\\partial t_8}{\\partial w} &= \\frac{\\partial t_8}{\\partial t_1} \\frac{\\partial t_1}{\\partial w} = 2/t_5^2 (t_6 -y) \\cdot \\exp(t_3) t_3 \\cdot x\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "In practice, we would use autodifferentiation using a datastructure as follows to keep track of the computation graph.\n"
      ],
      "id": "bec40c5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# code taken from https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb\n",
        "from graphviz import Digraph\n",
        "\n",
        "def trace(root):\n",
        "    nodes, edges = set(), set()\n",
        "    def build(v):\n",
        "        if v not in nodes:\n",
        "            nodes.add(v)\n",
        "            for child in v._prev:\n",
        "                edges.add((child, v))\n",
        "                build(child)\n",
        "    build(root)\n",
        "    return nodes, edges\n",
        "\n",
        "def draw_dot(root, format='svg', rankdir='LR'):\n",
        "    \"\"\"\n",
        "    format: png | svg | ...\n",
        "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
        "    \"\"\"\n",
        "    assert rankdir in ['LR', 'TB']\n",
        "    nodes, edges = trace(root)\n",
        "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
        "    \n",
        "    for n in nodes:\n",
        "        dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n",
        "        if n._op:\n",
        "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
        "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
        "    \n",
        "    for n1, n2 in edges:\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "    \n",
        "    return dot"
      ],
      "id": "3657a3fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "\n",
        "# taken from micrograd\n",
        "import numpy as np\n",
        "class Value:\n",
        "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        # propagate the gradient on out to parents\n",
        "        # i.e. self and other \n",
        "        # since out = self + other, then d(out)/dself = 1 and d(out)/dother = 1\n",
        "        # so we can just add the gradient to both parents\n",
        "        def _backward():\n",
        "            self.grad = out.grad\n",
        "            other.grad = out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad = other.data * out.grad\n",
        "            other.grad = self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad = (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def exp(self):\n",
        "        out = Value(np.exp(self.data), (self,), 'exp')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad = np.exp(self.data) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ],
      "id": "9d233b24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now write down our expression from before using the `Value` class\n"
      ],
      "id": "538e9853"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "\n",
        "# initialize some values\n",
        "w = Value(2.0)\n",
        "b = Value(0.0)\n",
        "\n",
        "# define the input\n",
        "x = Value(1.0)\n",
        "target = Value(10.0)\n",
        "\n",
        "# define the computation\n",
        "t1 = w * x\n",
        "t2 = t1 + b\n",
        "t3 = -1 * t2\n",
        "t4 = t3.exp()\n",
        "t5 = t4 + 1\n",
        "t6 = t5**(-1)\n",
        "\n",
        "t7 = t6 - target\n",
        "t8 = t7**2\n",
        "\n",
        "draw_dot(t8)"
      ],
      "id": "c44e20f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to seed the gradient of the loss\n"
      ],
      "id": "918d496e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "\n",
        "t8.grad = 1.0"
      ],
      "id": "7cdb9b70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can perform the backward pass by calling the `_backward` function of the loss node, which will in turn call the `_backward` functions of all its parents, and so on, until the entire graph has been visited.\n"
      ],
      "id": "c5d13506"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# #| \n",
        "t8._backward()\n",
        "print(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)"
      ],
      "id": "ac69500b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# #| \n",
        "t7._backward()\n",
        "print(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)"
      ],
      "id": "ba910aed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# #| \n",
        "t6._backward()\n",
        "print(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)"
      ],
      "id": "45ff9e41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# #| \n",
        "t5._backward()  \n",
        "print(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)"
      ],
      "id": "5487dc71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# #| \n",
        "t4._backward()\n",
        "print(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)"
      ],
      "id": "2d0df91f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# #| \n",
        "t3._backward()\n",
        "print(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)"
      ],
      "id": "6c558aa3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# #| \n",
        "t2._backward()\n",
        "w._backward()\n",
        "print(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)"
      ],
      "id": "b9ea6599",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To avoid calling the backward function multiple times, we can implement a `backprop` function that traverses the graph in reverse topological order and calls the `_backward` function of each node only once. \n",
        "\n",
        "Topological sorting can be implemented using the following code\n"
      ],
      "id": "3a4521e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "\n",
        "topo = []\n",
        "visited = set()\n",
        "def build_topo(v):\n",
        "    if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "            build_topo(child)\n",
        "        topo.append(v)"
      ],
      "id": "851c67e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip}\n",
        "### Why does this sorting algorithm work?\n",
        "\n",
        "- The algorithm is a depth-first search (DFS)\n",
        "- The deepest nodes are added to the `topo` list first\n",
        "- Recursiveness ensures that nodes another node depends on are added first (`topo.append` only happens after the recursive call)\n",
        "\n",
        "Not that this algorithm does not work for cyclic graphs.\n",
        ":::"
      ],
      "id": "822fbcc2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}