<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-05-02">
<meta name="description" content="Developing an understanding for how LLMs work.">

<title>Building a GPT that can generate molecules from scratch – Kevin’s Homepage</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-f4ed29be9f94c8431cc02458ea51e422.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-S9W9LVHXJK"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-S9W9LVHXJK', { 'anonymize_ip': true});
</script>
<style>
.cell-output-stdout {
  overflow-y: scroll;
  max-height: 400px;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Building a GPT that can generate molecules from scratch – Kevin’s Homepage">
<meta property="og:description" content="Developing an understanding for how LLMs work.">
<meta property="og:image" content="https://kjablonka.com/blog/posts/building_an_llm/index_files/figure-html/cell-6-output-1.png">
<meta property="og:site_name" content="Kevin's Homepage">
<meta property="og:image:height" content="150">
<meta property="og:image:width" content="450">
<meta name="twitter:title" content="Building a GPT that can generate molecules from scratch – Kevin’s Homepage">
<meta name="twitter:description" content="Developing an understanding for how LLMs work.">
<meta name="twitter:image" content="https://kjablonka.com/blog/posts/building_an_llm/index_files/figure-html/cell-6-output-1.png">
<meta name="twitter:creator" content="@kmjablonka">
<meta name="twitter:image-height" content="150">
<meta name="twitter:image-width" content="450">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> <i class="bi bi-home" role="img">
</i> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../opensource/opensource.html"> 
<span class="menu-text">Open Source</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#dealing-with-smiles" id="toc-dealing-with-smiles" class="nav-link active" data-scroll-target="#dealing-with-smiles">Dealing with SMILES</a>
  <ul class="collapse">
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization">Tokenization</a></li>
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">Embeddings</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding">Positional encoding</a></li>
  <li><a href="#language-modeling-dataset" id="toc-language-modeling-dataset" class="nav-link" data-scroll-target="#language-modeling-dataset">Language modeling dataset</a></li>
  </ul></li>
  <li><a href="#a-simple-bigram-model" id="toc-a-simple-bigram-model" class="nav-link" data-scroll-target="#a-simple-bigram-model">A simple bigram model</a>
  <ul class="collapse">
  <li><a href="#training-the-model" id="toc-training-the-model" class="nav-link" data-scroll-target="#training-the-model">Training the model</a></li>
  </ul></li>
  <li><a href="#making-tokens-talk-using-attention" id="toc-making-tokens-talk-using-attention" class="nav-link" data-scroll-target="#making-tokens-talk-using-attention">Making tokens talk using attention</a>
  <ul class="collapse">
  <li><a href="#refactoring-into-a-module" id="toc-refactoring-into-a-module" class="nav-link" data-scroll-target="#refactoring-into-a-module">Refactoring into a module</a></li>
  <li><a href="#revamped-bigram-model" id="toc-revamped-bigram-model" class="nav-link" data-scroll-target="#revamped-bigram-model">Revamped Bigram Model</a></li>
  </ul></li>
  <li><a href="#interlude-additional-perspectives-on-attention" id="toc-interlude-additional-perspectives-on-attention" class="nav-link" data-scroll-target="#interlude-additional-perspectives-on-attention">Interlude: Additional perspectives on attention</a>
  <ul class="collapse">
  <li><a href="#attention-as-gnn" id="toc-attention-as-gnn" class="nav-link" data-scroll-target="#attention-as-gnn">Attention as GNN</a></li>
  <li><a href="#attention-as-kernel-smoothing" id="toc-attention-as-kernel-smoothing" class="nav-link" data-scroll-target="#attention-as-kernel-smoothing">Attention as Kernel smoothing</a></li>
  </ul></li>
  <li><a href="#adding-more-expressive-power-with-more-heads-and-fully-connected-layers" id="toc-adding-more-expressive-power-with-more-heads-and-fully-connected-layers" class="nav-link" data-scroll-target="#adding-more-expressive-power-with-more-heads-and-fully-connected-layers">Adding more expressive power with more heads and fully connected layers</a></li>
  <li><a href="#abstracting-transformers-into-blocks" id="toc-abstracting-transformers-into-blocks" class="nav-link" data-scroll-target="#abstracting-transformers-into-blocks">Abstracting transformers into blocks</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.dev/kjappelbaum/kjappelbaum.github.io/blob/master/blog/posts/building_an_llm/index.ipynb" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Building a GPT that can generate molecules from scratch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">machine-learning</div>
    <div class="quarto-category">llm</div>
    <div class="quarto-category">teaching</div>
  </div>
  </div>

<div>
  <div class="description">
    <i>Developing an understanding for how LLMs work.</i>
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 2, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Molecules can be represented in multitude of ways. One of the most widely used representations is to use text, for example in the so-called SMILES notation. In SMILES notation, a molecule is represented as a string of characters, where each character represents an atom or a bond. For example, the SMILES notation for ethanol is <code>CCO</code>. The one for benzene is <code>c1ccccc1</code>. You see that hydrogen atoms are typically omitted in SMILES notation, and that lower case letters are used for aromatic atoms. There is a <a href="http://opensmiles.org/opensmiles.html">full grammar for SMILES notation</a> and <a href="https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf">various alternative representations</a>, but we will stick to this simple version for this notebook.</p>
<p>Important problems that our final solution will need to be able to solve are:</p>
<ul>
<li>dealing with inputs of different lengths (e.g, different number of atoms in different molecules)</li>
<li>incorporating information about the semantic meaning of the atoms in the molecule (to obtain meaningful molecules, the model, e.g., should probably “know” what kind of bonds carbon can form)</li>
<li>dealing with the interaction between atoms in the molecule (not all arrangements of atoms are equally likely)</li>
</ul>
<div id="cell-3" class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rdkit <span class="im">import</span> Chem</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> exp</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-4" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_num_parameters(model):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return the number of trainable parameters in the model."""</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_num_parameters_per_layer(model):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return the number of trainable parameters in the model per layer."""</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> {}</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, p <span class="kw">in</span> model.named_parameters():</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> p.requires_grad:</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            layers[name] <span class="op">=</span> p.numel()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> layers</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_device():</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.backends.mps.is_built():</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> <span class="st">'mps'</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="st">'cuda'</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="st">'cpu'</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> device</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> set_device()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="dealing-with-smiles" class="level2">
<h2 class="anchored" data-anchor-id="dealing-with-smiles">Dealing with SMILES</h2>
<p>Before we can do anything, we need to obtain data. For doing so, we will need a dataset of SMILES strings. We will use the <a href="https://zinc.docking.org/">ZINC dataset</a> which is a public database of commercially-available compounds. We will use the <code>250k</code> subset of the dataset which contains 250,000 compounds.</p>
<div id="cell-7" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget <span class="st">'https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/zinc15_250K_2D.tar.gz'</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>tar <span class="op">-</span>xzf zinc15_250K_2D.tar.gz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>/Users/kevinmaikjablonka/.zshenv:.:1: no such file or directory: /Users/kevinmaikjablonka/.cargo/env
--2024-05-02 12:20:55--  https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/zinc15_250K_2D.tar.gz
Resolving deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)... 52.219.120.49, 52.219.120.145, 52.219.193.50, ...
Connecting to deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)|52.219.120.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 6941580 (6.6M) [application/x-gzip]
Saving to: ‘zinc15_250K_2D.tar.gz’

zinc15_250K_2D.tar. 100%[===================&gt;]   6.62M  1.25MB/s    in 14s     

2024-05-02 12:21:11 (497 KB/s) - ‘zinc15_250K_2D.tar.gz’ saved [6941580/6941580]

/Users/kevinmaikjablonka/.zshenv:.:1: no such file or directory: /Users/kevinmaikjablonka/.cargo/env</code></pre>
</div>
</div>
<p>After downloading and extracting the dataset, we can load it into memory and take a look at some molecules.</p>
<div id="cell-9" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'zinc15_250K_2D.csv'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-10" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Chem.MolFromSmiles(df[<span class="st">'smiles'</span>][<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Before we continue any further, we will also create train/valid and test sets.</p>
<div id="cell-12" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>train, valid, test <span class="op">=</span> torch.utils.data.random_split(df[<span class="st">'smiles'</span>], [<span class="dv">200000</span>, <span class="dv">25000</span>, <span class="dv">25000</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<p>For training a language model, we will need to split the SMILES into tokens. Tokens are the smallest units of text that the model will work with. The model will learn to predict a molecule token by token. There is not one correct way to do this, but one very common way is to split the SMILES into “chemical tokens”. For this, <a href="https://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc02339e">Philippe Schwaller wrote down a regular expression</a>.</p>
<p>Commonly used other tokenization methods are:</p>
<ul>
<li><a href="https://github.com/google/sentencepiece">SentencePiece</a></li>
<li><a href="https://github.com/openai/tiktoken">Byte-Pair Encoding (BPE)</a></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some try to move completely away from tokenization and <a href="https://byte-gpt.github.io/">directly</a> <a href="https://www.youtube.com/watch?v=kcd0BTKJuXk">model</a> <a href="https://arxiv.org/abs/2105.13626">bytes</a>.</p>
</div>
</div>
<div id="cell-15" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(smiles: <span class="bu">str</span>) <span class="op">-&gt;</span> List[<span class="bu">str</span>]:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Tokenize a SMILES</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">        smiles (str): SMILES string</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">        List[str]: List of tokens</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    SMI_REGEX_PATTERN <span class="op">=</span> <span class="vs">r"""(\[[^\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;&gt;?|\*|\$|\%[0-9]</span><span class="sc">{2}</span><span class="vs">|[0-9])"""</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> re.findall(SMI_REGEX_PATTERN, smiles)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The molecule, CCO (ethanol), is tokenized as [‘C’, ‘C’, ‘O’].</p>
<div id="cell-17" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>tokenize(<span class="st">'CCO'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>['C', 'C', 'O']</code></pre>
</div>
</div>
<section id="converting-tokens-into-ids" class="level4">
<h4 class="anchored" data-anchor-id="converting-tokens-into-ids">Converting tokens into IDs</h4>
<p>For inputing tokens into a model, we will need to convert them into numbers.</p>
<p>To do so, we will set up a “vocabulary” which is a dictionary that maps tokens to integers. The vocabulary also defines the tokens that are known to the model.</p>
</section>
<section id="special-tokens" class="level4">
<h4 class="anchored" data-anchor-id="special-tokens">Special tokens</h4>
<p>Our model will be fed sequences of fixed length. Our SMILES, however, are of variable length. We will have to pad them to a fixed length. We will use a padding token for this purpose. That is, we will add a specific “[PAD]” token to the vocabulary which only serves the purpose of padding.</p>
<p>Often, we also add other tokens such as <code>[EOS]</code> (end of sequence) or <code>[BOS]</code> (beginning of sequence).</p>
<p>They are typically used as follows:</p>
<ul>
<li><code>[BOS]</code> is added at the beginning of each sequence</li>
<li><code>[EOS]</code> is added at the end of each sequence</li>
<li><code>[PAD]</code> is added to the end of each sequence to pad it to a fixed length</li>
<li><code>[UNK]</code> is used to replace tokens that are not in the vocabulary</li>
</ul>
<p>We can put all of this together in a <code>Tokenizer</code> class.</p>
<div id="cell-22" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Tokenizer:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokens: List[<span class="bu">str</span>], eos: <span class="bu">str</span> <span class="op">=</span> <span class="st">'[EOS]'</span>, bos: <span class="bu">str</span> <span class="op">=</span> <span class="st">'[BOS]'</span>, pad: <span class="bu">str</span> <span class="op">=</span> <span class="st">'[PAD]'</span>, unk: <span class="bu">str</span> <span class="op">=</span> <span class="st">'[UNK]'</span>):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokens <span class="op">=</span> [pad, bos, eos, unk] <span class="op">+</span> tokens</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._token_to_index <span class="op">=</span> {token: index <span class="cf">for</span> index, token <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.tokens)}</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.index_to_token <span class="op">=</span> {index: token <span class="cf">for</span> index, token <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.tokens)}</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> token_to_index(<span class="va">self</span>, token: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>._token_to_index[token]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>._token_to_index[<span class="st">'[UNK]'</span>]</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.tokens)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, item):</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.token_to_index[item]</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__contains__</span>(<span class="va">self</span>, item):</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item <span class="kw">in</span> <span class="va">self</span>.tokens</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, smiles: <span class="bu">str</span>, add_sos: <span class="bu">bool</span><span class="op">=</span><span class="va">False</span>, add_eos: <span class="bu">bool</span><span class="op">=</span><span class="va">False</span>) <span class="op">-&gt;</span> List[<span class="bu">int</span>]:</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Encode a SMILES into a list of indices</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="co">            smiles (str): SMILES string</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co">            add_sos (bool): Add start of sentence token</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co">            add_eos (bool): Add end of sentence token</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co">            List[int]: List of indices</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> []</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> add_sos:</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>            tokens.append(<span class="va">self</span>.token_to_index(<span class="st">'[BOS]'</span>))</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">+=</span> [<span class="va">self</span>.token_to_index(token) <span class="cf">for</span> token <span class="kw">in</span> tokenize(smiles)]</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> add_eos:</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>            tokens.append(<span class="va">self</span>.token_to_index(<span class="st">'[EOS]'</span>))</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tokens</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, indices: List[<span class="bu">int</span>], strip_special_tokens: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>) <span class="op">-&gt;</span> <span class="bu">str</span>: </span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Decode a list of indices into a SMILES</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a><span class="co">            indices (List[int]): List of indices</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a><span class="co">            str: SMILES string</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>        decoded <span class="op">=</span> <span class="st">''</span>.join([<span class="va">self</span>.index_to_token[index] <span class="cf">for</span> index <span class="kw">in</span> indices])</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> strip_special_tokens:</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> decoded.replace(<span class="st">'[PAD]'</span>, <span class="st">''</span>).replace(<span class="st">'[BOS]'</span>, <span class="st">''</span>).replace(<span class="st">'[EOS]'</span>, <span class="st">''</span>)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> decoded</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To instantiate the tokenizer, we need to pass the list of tokens that we want to use. (This is sometimes called “training” the tokenizer, but in this case, we are just defining the tokens that we want to use.) We will use the following tokens:</p>
<div id="cell-24" class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>lengths <span class="op">=</span> []</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> smiles <span class="kw">in</span> train.dataset.values:</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    tokens_ <span class="op">=</span> tokenize(smiles)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    tokens.update(tokens_)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    lengths.append(<span class="bu">len</span>(tokens_))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-25" class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plt.hist(lengths, bins<span class="op">=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>(array([3.0000e+00, 4.0000e+00, 7.0000e+00, 0.0000e+00, 2.3000e+01,
        5.6000e+01, 0.0000e+00, 7.8000e+01, 2.0100e+02, 0.0000e+00,
        3.8900e+02, 8.0200e+02, 1.4320e+03, 0.0000e+00, 2.5760e+03,
        3.9450e+03, 0.0000e+00, 5.8570e+03, 8.0820e+03, 0.0000e+00,
        1.0313e+04, 1.2675e+04, 1.4914e+04, 0.0000e+00, 1.7137e+04,
        1.8718e+04, 0.0000e+00, 2.0510e+04, 2.0796e+04, 0.0000e+00,
        2.1073e+04, 2.0330e+04, 1.8396e+04, 0.0000e+00, 1.6193e+04,
        1.2172e+04, 0.0000e+00, 9.8210e+03, 5.8470e+03, 0.0000e+00,
        3.9460e+03, 2.1220e+03, 9.6800e+02, 0.0000e+00, 4.1200e+02,
        1.4500e+02, 0.0000e+00, 4.6000e+01, 1.0000e+01, 1.0000e+00]),
 array([17. , 17.7, 18.4, 19.1, 19.8, 20.5, 21.2, 21.9, 22.6, 23.3, 24. ,
        24.7, 25.4, 26.1, 26.8, 27.5, 28.2, 28.9, 29.6, 30.3, 31. , 31.7,
        32.4, 33.1, 33.8, 34.5, 35.2, 35.9, 36.6, 37.3, 38. , 38.7, 39.4,
        40.1, 40.8, 41.5, 42.2, 42.9, 43.6, 44.3, 45. , 45.7, 46.4, 47.1,
        47.8, 48.5, 49.2, 49.9, 50.6, 51.3, 52. ]),
 &lt;BarContainer object of 50 artists&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-26" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(<span class="bu">list</span>(tokens))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-27" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>tokenizer.encode(<span class="st">'CCO'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>[45, 45, 38]</code></pre>
</div>
</div>
</section>
</section>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">Embeddings</h3>
<p>Currently, we only encode the SMILES strings into a list of indices. There is no inherent meaning to the indices themselves, and we can improve modeling by representing each index as a vector. We call those vectors embeddings, but they are nothing more than a vector representation–like a feature vector–for each index.</p>
<p>Ideally, those vectors ensure that similar indices are close to each other in the embedding space. There are many ways to create those embeddings. But for now it is only important to know this concept.</p>
</section>
<section id="positional-encoding" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding">Positional encoding</h3>
<p>The embeddings we just created contain only information about their identity. However, they contain no information about their position in the sequence.</p>
<p>To add positional information, we can add a positional encoding to the embeddings. Again, there are many ways to do this.</p>
<p>A very simple way is called <em>absolute positional encoding</em>. For this we simply add the position index to the embedding vector.</p>
<p>For example</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>B, T, C <span class="op">=</span> <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span> <span class="co"># batch size, sequence length, embedding size</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(B, T, C)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> torch.arange(T).unsqueeze(<span class="dv">0</span>).repeat(B, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="language-modeling-dataset" class="level3">
<h3 class="anchored" data-anchor-id="language-modeling-dataset">Language modeling dataset</h3>
<p>A dataset class is a class that inherits from <code>torch.utils.data.Dataset</code>. It is used to load data into a model.</p>
<p>The most important methods of a dataset class are:</p>
<ul>
<li><code>__len__</code>: This method returns the length of the dataset. It is used by the <code>DataLoader</code> to determine how many batches to load.</li>
<li><code>__getitem__</code>: This method returns a single sample from the dataset. It is used by the <code>DataLoader</code> to load a batch of samples.</li>
</ul>
<div id="cell-33" class="cell" data-execution_count="266">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalLanguageModelingDataset(Dataset):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, texts, tokenizer, max_length):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.texts <span class="op">=</span> texts</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_length <span class="op">=</span> max_length</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> []</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> []</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> text <span class="kw">in</span> texts:</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>            input_ids <span class="op">=</span> np.array(tokenizer.encode(text))</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(input_ids) <span class="op">&gt;</span> <span class="va">self</span>.max_length:</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>            input_ids <span class="op">=</span> <span class="va">self</span>._pad_right(input_ids, <span class="va">self</span>.max_length)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># make next token the target create datasets with sliding windows</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(input_ids)):</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.inputs.append(<span class="va">self</span>._pad_left(input_ids[:i], <span class="va">self</span>.max_length))</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.targets.append([input_ids[i]])</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.inputs)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        input_ids <span class="op">=</span> <span class="va">self</span>.inputs[idx]</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        target_ids <span class="op">=</span> <span class="va">self</span>.targets[idx]</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>  torch.tensor(input_ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>), torch.tensor(target_ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pad_left(<span class="va">self</span>, sequence, max_len):</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>        pad_value <span class="op">=</span> <span class="va">self</span>.tokenizer.token_to_index(<span class="st">'[PAD]'</span>)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        padded_sequence <span class="op">=</span> np.full(max_len, pad_value)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        padded_sequence[<span class="op">-</span><span class="bu">len</span>(sequence):] <span class="op">=</span> sequence</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> padded_sequence</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pad_right(<span class="va">self</span>, sequence, max_len):</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        pad_value <span class="op">=</span> <span class="va">self</span>.tokenizer.token_to_index(<span class="st">'[PAD]'</span>)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>        padded_sequence <span class="op">=</span> np.full(max_len, pad_value)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>        padded_sequence[:<span class="bu">len</span>(sequence)] <span class="op">=</span> sequence</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> padded_sequence</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You hopefully note something very interesting in this dataset: Based on one SMILES, we can create multiple training examples, because we can slide a window over the SMILES and predict the next token. (Note that our implementation is relatively naiive and is optimized to make this point clear. In practice, you should use dedicated methods, e.g., from the <code>transformers</code> library, to create language model datasets.)</p>
</section>
</section>
<section id="a-simple-bigram-model" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-bigram-model">A simple bigram model</h2>
<p>The simplest language model is a bigram model. In a bigram model, we predict the next token based on the previous token. A bigram model is the simplest form of <code>n-gram</code> model. In an <code>n-gram</code> model, we predict the next token based on the previous <code>n</code> tokens.</p>
<p><span class="math inline">\(N\)</span>-gram models are a simple but effective way to model language. The idea is to predict the next word in a sentence given the previous <span class="math inline">\(n-1\)</span> words. For example, in a 2-gram (bigram) model, we would predict the next word given only the previous word. In a 3-gram model, we would predict the next word given the previous two words. In general, we would predict the next word given the previous <span class="math inline">\(n-1\)</span> words.</p>
<p>Formally, we can write down the bigram model as follows:</p>
<p><span class="math display">\[
p(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
\]</span></p>
<p>where <span class="math inline">\(w_i\)</span> is the <span class="math inline">\(i\)</span>-th word in the sentence, <span class="math inline">\(C(w_{i-1}, w_i)\)</span> is the number of times the bigram <span class="math inline">\(w_{i-1}, w_i\)</span> occurs in the training set, and <span class="math inline">\(C(w_{i-1})\)</span> is the number of times the word <span class="math inline">\(w_{i-1}\)</span> occurs in the training set.</p>
<p>Since the bigram model only considers the previous word/token, we only need a lookup table.</p>
<p>Such lookup tables are implemented in PyTorch as <code>nn.Embedding</code>. Keep in mind that an embedding layer is nothing fancy. It works like inputting a one-hot encoded vector in a linear layer:</p>
<div class="callout callout-style-default callout-note callout-titled" title="What are embedding layers?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What are embedding layers?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sebastian Raschka made a great figure about that.</p>
<blockquote class="twitter-tweet blockquote" data-media-max-width="560">
<p lang="en" dir="ltr">
Embedding layers are often perceived as a fancy operation that we apply to encode the inputs (each word tokens) for large language models.<br>But embedding layers = fully-connected layers on one-hot encoded inputs. They just replace expensive matrix multiplications w index look-ups. <a href="https://t.co/0I3AFk4por">pic.twitter.com/0I3AFk4por</a>
</p>
— Sebastian Raschka (<span class="citation" data-cites="rasbt">@rasbt</span>) <a href="https://twitter.com/rasbt/status/1611401567030083587?ref_src=twsrc%5Etfw">January 6, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>You can try it yourself using the following code (taken from Sebastian’s tweet):</p>
<p>You can first use an embedding layer to encode the indices and then use a linear layer to do the same. You will see that the results are the same.</p>
<p>Here for example, we encode the indices <code>[2, 3, 1]</code> into a 5-dimensional vector using an embedding layer and a linear layer.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)<span class="op">;</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> torch.tensor([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>]) <span class="co"># 3 training examples</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>num_idx <span class="op">=</span> <span class="bu">max</span>(idx)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>out_dim <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> torch.nn.Embedding(num_idx, out_dim)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>embedding(idx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The code for the linear layer is:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">123</span>)<span class="op">;</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> torch.tensor([<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>]) <span class="co"># 3 training examples</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>one_hot <span class="op">=</span> torch.nn.functional.one_hot(idx, num_classes<span class="op">=</span>num_idx)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> torch.nn.Linear(num_idx, out_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>linear.weight <span class="op">=</span> torch.nn.Parameter(embedding.weight.T.detach()) <span class="co"># nn.Linear does xW^T, so we need to transpose the weight matrix</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>linear(one_hot.<span class="bu">float</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Using the <code>Embedding</code> layer, we can create a simple Bigram model.</p>
<div id="cell-39" class="cell" data-execution_count="267">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BigramModel(nn.Module):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">40</span>):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># "learnable dictionary" that maps one token to another token</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mapping_layer <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the forward pass only consists of a lookup in the mapping layer</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mapping_layer(x)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y): </span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x has shape (batch_size, sequence_length)</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> predictions.shape</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predictions has shape (batch_size, sequence_length, vocab_size)</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> predictions.view(B<span class="op">*</span>T, C)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y has the shape (batch_size, sequence_length)</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B<span class="op">*</span>T)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we use cross entropy loss to train the model</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.cross_entropy(predictions, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-40" class="cell" data-execution_count="268">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>bigram <span class="op">=</span> BigramModel(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Given a token ID, the model predict how likely each token of the vocabulary is to be the next. Right now, the model is not trained, so it will predict the next token randomly.</p>
<div id="cell-42" class="cell" data-execution_count="269">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>F.softmax(bigram(torch.tensor([<span class="dv">1</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/ipykernel_51170/730608109.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  F.softmax(bigram(torch.tensor([1])))</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="269">
<pre><code>tensor([[0.0465, 0.0137, 0.0966, 0.0857, 0.3933, 0.0212, 0.0415, 0.0283, 0.0550,
         0.2181]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
</div>
</div>
<p>For generating a sequence, we can implement a <code>generate</code> method that iteratively predicts the next token and appends it to the sequence. We can then use this method to generate a sequence of a given length.</p>
<div id="cell-44" class="cell" data-execution_count="288">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BigramModel(nn.Module):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># read of the logits of the next token from table</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mapping_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mapping_table(x) <span class="co"># returns tensor of shape (batch_size, time_steps, vocab_size)</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y):</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note that that the implementation below is because of how we - for educational purposes - have defined the dataset</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A better way is to have inputs and outputs of the same length (and to not manually code the sliding window</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but to instead use a causal mask)</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in our case, y only contains the next token</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so we only care about the last token in Bigram</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(B, C)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B)</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.cross_entropy(logits, y)</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, x, max_new_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we generate max_new_tokens new tokens</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>        new_tokens <span class="op">=</span> []</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _t <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># we only care about the last token in Bigram, hence we bow have shape (B, C)</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># we generate probabilities for the next token</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>            <span class="co"># torch.multinomial(probs, num_samples=1) returns a tensor of shape (B, 1) </span></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># where each element is the index of the sampled token</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>            new_tokens.append(next_token)</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, next_token], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To evaluate the model performance, we will use the helper function below.</p>
<p>As performance metric we will use perplexity. Perplexity is a metric that measures how well a probability model predicts a sample. It is defined as <span class="math inline">\(2^H\)</span>, where <span class="math inline">\(H\)</span> is the cross entropy loss. The lower the perplexity, the better the model.</p>
<p>To better understand it, let’s recall a few things:</p>
<p>LLMs are trained to predict the probability of a word given the previous words. For instance, in the sentence “She went to the…”, the model predicts the probability of what the next word could be (e.g., store, park, etc.).</p>
<p><em>Cross entropy</em> is a measure of the difference between two probability distributions - in this case, the distribution predicted by the model and the actual distribution of words in the language. A lower cross-entropy means the model’s predictions are closer to the actual distribution. We can calculate it as follows:</p>
<p><span class="math display">\[H(p,q) = - \sum_{x} p(x) \log q(x)\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the actual distribution and <span class="math inline">\(q\)</span> is the predicted distribution.</p>
<p><em>Perplexity</em> can be thought of as the “effective number of choices” the model feels it has when making a prediction. A lower perplexity indicates that the model is more confident (or less “perplexed”) about its predictions.</p>
<p>For example, if a model has a perplexity of 10 on a dataset, it means that, on average, each time it tries to predict the next word, it’s as uncertain as if it were choosing uniformly and randomly among 10 options. If the perplexity is 100, it’s as uncertain as if it were choosing among 100 options, and so on.</p>
<p>You can find further information about such metrics <a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">here</a>.</p>
<div id="cell-46" class="cell" data-execution_count="289">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_perplexity(model, data_loader):</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the model to evaluation mode, i.e., </span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    total_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> data_loader:</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(device)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.to(device)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> model.loss(x, y)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        total_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(total_loss <span class="op">/</span> total_count)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="training-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model">Training the model</h3>
<p>To train the model, we will use a simple training loop and the Adam optimizer.</p>
<p>The role of the <code>Adam</code> optimizer is to update the parameters of the model using a technique called <a href="http://d2l.ai/chapter_optimization/minibatch-sgd.html">mini-batch stochastic gradient descent</a>. The idea is that we update the weights in the direction of the gradient of the loss function, which we estimate on a small batch of data. The learning rate controls how big the steps are that we take in the direction of the gradient.</p>
<p>Setting learning rate is not trivial, you can find more background <a href="https://www.jeremyjordan.me/nn-learning-rate/">here</a>.</p>
<p>It is import to remember to use the <code>zero_grad</code> function to clear the gradients before computing the gradients for the current batch. Also, remember to call <code>loss.backward()</code> to compute the gradients for the current batch.</p>
<p>For now, we will use a very simple approach (to reuse our old dataloader) and just predict the second token given the first one.</p>
<div id="cell-48" class="cell" data-execution_count="290">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BigramModel(<span class="bu">len</span>(tokenizer))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-49" class="cell" data-execution_count="291">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(train, tokenizer, max_length<span class="op">=</span><span class="dv">40</span>), batch_size<span class="op">=</span><span class="dv">2048</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>valid_loader <span class="op">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(valid, tokenizer, max_length<span class="op">=</span><span class="dv">40</span>), batch_size<span class="op">=</span><span class="dv">2048</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(test, tokenizer, max_length<span class="op">=</span><span class="dv">40</span>), batch_size<span class="op">=</span><span class="dv">2048</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-50" class="cell" data-execution_count="292">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, train_loader, val_loader, epochs, lr, eval_every<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set up the optimizer</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># start training</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the model to train mode </span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># iterate over the training data</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (x,y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># move the data to the device</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.to(device)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(device)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> model.loss(x,y)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># clear the gradients</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># compute the gradients</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update the parameters</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print the loss every eval_every iterations</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">%</span> eval_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, iter </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, train loss </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.3f}</span><span class="ss">, val perplexity </span><span class="sc">{</span>estimate_perplexity(model, val_loader)<span class="sc">:.5f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-51" class="cell" data-execution_count="293">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, valid_loader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">1e-3</span>, eval_every<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, iter 0, train loss 4.247, val perplexity 68.89613
Epoch 0, iter 100, train loss 4.084, val perplexity 58.78415
Epoch 0, iter 200, train loss 3.887, val perplexity 50.44378
Epoch 0, iter 300, train loss 3.770, val perplexity 43.52746
Epoch 0, iter 400, train loss 3.647, val perplexity 37.78621
Epoch 0, iter 500, train loss 3.483, val perplexity 32.99921
Epoch 0, iter 600, train loss 3.302, val perplexity 28.98877
Epoch 0, iter 700, train loss 3.246, val perplexity 25.62722
Epoch 0, iter 800, train loss 3.124, val perplexity 22.79053
Epoch 0, iter 900, train loss 3.014, val perplexity 20.38747
Epoch 0, iter 1000, train loss 2.925, val perplexity 18.34650
Epoch 0, iter 1100, train loss 2.821, val perplexity 16.60504
Epoch 0, iter 1200, train loss 2.695, val perplexity 15.11995
Epoch 0, iter 1300, train loss 2.618, val perplexity 13.84288
Epoch 0, iter 1400, train loss 2.565, val perplexity 12.74738
Epoch 0, iter 1500, train loss 2.524, val perplexity 11.80145
Epoch 0, iter 1600, train loss 2.432, val perplexity 10.98570
Epoch 0, iter 1700, train loss 2.295, val perplexity 10.27731
Epoch 0, iter 1800, train loss 2.271, val perplexity 9.66229
Epoch 0, iter 1900, train loss 2.235, val perplexity 9.12911
Epoch 0, iter 2000, train loss 2.189, val perplexity 8.66075
Epoch 0, iter 2100, train loss 2.085, val perplexity 8.24934
Epoch 0, iter 2200, train loss 2.058, val perplexity 7.88684
Epoch 0, iter 2300, train loss 2.025, val perplexity 7.56794
Epoch 0, iter 2400, train loss 2.033, val perplexity 7.28616
Epoch 0, iter 2500, train loss 1.934, val perplexity 7.03687
Epoch 0, iter 2600, train loss 1.882, val perplexity 6.81432
Epoch 0, iter 2700, train loss 1.890, val perplexity 6.61714
Epoch 0, iter 2800, train loss 1.857, val perplexity 6.44163
Epoch 0, iter 2900, train loss 1.860, val perplexity 6.28394
Epoch 0, iter 3000, train loss 1.831, val perplexity 6.14318
Epoch 1, iter 0, train loss 1.806, val perplexity 6.11426
Epoch 1, iter 100, train loss 1.799, val perplexity 5.99120
Epoch 1, iter 200, train loss 1.772, val perplexity 5.88139
Epoch 1, iter 300, train loss 1.758, val perplexity 5.78283
Epoch 1, iter 400, train loss 1.718, val perplexity 5.69448
Epoch 1, iter 500, train loss 1.756, val perplexity 5.61472
Epoch 1, iter 600, train loss 1.741, val perplexity 5.54318
Epoch 1, iter 700, train loss 1.676, val perplexity 5.47892
Epoch 1, iter 800, train loss 1.695, val perplexity 5.42134
Epoch 1, iter 900, train loss 1.671, val perplexity 5.37019
Epoch 1, iter 1000, train loss 1.693, val perplexity 5.32408
Epoch 1, iter 1100, train loss 1.683, val perplexity 5.28262
Epoch 1, iter 1200, train loss 1.651, val perplexity 5.24482
Epoch 1, iter 1300, train loss 1.681, val perplexity 5.21134
Epoch 1, iter 1400, train loss 1.600, val perplexity 5.18109
Epoch 1, iter 1500, train loss 1.627, val perplexity 5.15382
Epoch 1, iter 1600, train loss 1.621, val perplexity 5.12923
Epoch 1, iter 1700, train loss 1.597, val perplexity 5.10679
Epoch 1, iter 1800, train loss 1.592, val perplexity 5.08645
Epoch 1, iter 1900, train loss 1.603, val perplexity 5.06797
Epoch 1, iter 2000, train loss 1.622, val perplexity 5.05094
Epoch 1, iter 2100, train loss 1.600, val perplexity 5.03557
Epoch 1, iter 2200, train loss 1.618, val perplexity 5.02141
Epoch 1, iter 2300, train loss 1.611, val perplexity 5.00810
Epoch 1, iter 2400, train loss 1.586, val perplexity 4.99597
Epoch 1, iter 2500, train loss 1.587, val perplexity 4.98479
Epoch 1, iter 2600, train loss 1.626, val perplexity 4.97436
Epoch 1, iter 2700, train loss 1.594, val perplexity 4.96466
Epoch 1, iter 2800, train loss 1.626, val perplexity 4.95561
Epoch 1, iter 2900, train loss 1.627, val perplexity 4.94720
Epoch 1, iter 3000, train loss 1.615, val perplexity 4.93930
Epoch 2, iter 0, train loss 1.595, val perplexity 4.93764
Epoch 2, iter 100, train loss 1.605, val perplexity 4.93042
Epoch 2, iter 200, train loss 1.562, val perplexity 4.92365
Epoch 2, iter 300, train loss 1.597, val perplexity 4.91732
Epoch 2, iter 400, train loss 1.589, val perplexity 4.91128
Epoch 2, iter 500, train loss 1.604, val perplexity 4.90558
Epoch 2, iter 600, train loss 1.591, val perplexity 4.90023
Epoch 2, iter 700, train loss 1.555, val perplexity 4.89515
Epoch 2, iter 800, train loss 1.597, val perplexity 4.89030
Epoch 2, iter 900, train loss 1.568, val perplexity 4.88572
Epoch 2, iter 1000, train loss 1.568, val perplexity 4.88150
Epoch 2, iter 1100, train loss 1.596, val perplexity 4.87742
Epoch 2, iter 1200, train loss 1.540, val perplexity 4.87349
Epoch 2, iter 1300, train loss 1.585, val perplexity 4.86991
Epoch 2, iter 1400, train loss 1.620, val perplexity 4.86635
Epoch 2, iter 1500, train loss 1.595, val perplexity 4.86316
Epoch 2, iter 1600, train loss 1.609, val perplexity 4.86005
Epoch 2, iter 1700, train loss 1.590, val perplexity 4.85700
Epoch 2, iter 1800, train loss 1.584, val perplexity 4.85425
Epoch 2, iter 1900, train loss 1.588, val perplexity 4.85149
Epoch 2, iter 2000, train loss 1.593, val perplexity 4.84899
Epoch 2, iter 2100, train loss 1.611, val perplexity 4.84653
Epoch 2, iter 2200, train loss 1.570, val perplexity 4.84416
Epoch 2, iter 2300, train loss 1.630, val perplexity 4.84193
Epoch 2, iter 2400, train loss 1.548, val perplexity 4.83973
Epoch 2, iter 2500, train loss 1.544, val perplexity 4.83775
Epoch 2, iter 2600, train loss 1.594, val perplexity 4.83583
Epoch 2, iter 2700, train loss 1.606, val perplexity 4.83394
Epoch 2, iter 2800, train loss 1.567, val perplexity 4.83223
Epoch 2, iter 2900, train loss 1.606, val perplexity 4.83054
Epoch 2, iter 3000, train loss 1.544, val perplexity 4.82894
Epoch 3, iter 0, train loss 1.610, val perplexity 4.82853
Epoch 3, iter 100, train loss 1.624, val perplexity 4.82708
Epoch 3, iter 200, train loss 1.573, val perplexity 4.82555
Epoch 3, iter 300, train loss 1.583, val perplexity 4.82419
Epoch 3, iter 400, train loss 1.561, val perplexity 4.82284
Epoch 3, iter 500, train loss 1.545, val perplexity 4.82160
Epoch 3, iter 600, train loss 1.577, val perplexity 4.82032
Epoch 3, iter 700, train loss 1.536, val perplexity 4.81921
Epoch 3, iter 800, train loss 1.574, val perplexity 4.81807
Epoch 3, iter 900, train loss 1.568, val perplexity 4.81694
Epoch 3, iter 1000, train loss 1.594, val perplexity 4.81590
Epoch 3, iter 1100, train loss 1.532, val perplexity 4.81492
Epoch 3, iter 1200, train loss 1.520, val perplexity 4.81411
Epoch 3, iter 1300, train loss 1.597, val perplexity 4.81317
Epoch 3, iter 1400, train loss 1.563, val perplexity 4.81233
Epoch 3, iter 1500, train loss 1.625, val perplexity 4.81148
Epoch 3, iter 1600, train loss 1.571, val perplexity 4.81063
Epoch 3, iter 1700, train loss 1.590, val perplexity 4.80991
Epoch 3, iter 1800, train loss 1.570, val perplexity 4.80916
Epoch 3, iter 1900, train loss 1.585, val perplexity 4.80845
Epoch 3, iter 2000, train loss 1.617, val perplexity 4.80771
Epoch 3, iter 2100, train loss 1.578, val perplexity 4.80707
Epoch 3, iter 2200, train loss 1.589, val perplexity 4.80649
Epoch 3, iter 2300, train loss 1.561, val perplexity 4.80590
Epoch 3, iter 2400, train loss 1.553, val perplexity 4.80530
Epoch 3, iter 2500, train loss 1.560, val perplexity 4.80477
Epoch 3, iter 2600, train loss 1.571, val perplexity 4.80423
Epoch 3, iter 2700, train loss 1.622, val perplexity 4.80373
Epoch 3, iter 2800, train loss 1.595, val perplexity 4.80326
Epoch 3, iter 2900, train loss 1.562, val perplexity 4.80281
Epoch 3, iter 3000, train loss 1.553, val perplexity 4.80236
Epoch 4, iter 0, train loss 1.520, val perplexity 4.80227
Epoch 4, iter 100, train loss 1.570, val perplexity 4.80192
Epoch 4, iter 200, train loss 1.569, val perplexity 4.80140
Epoch 4, iter 300, train loss 1.576, val perplexity 4.80111
Epoch 4, iter 400, train loss 1.543, val perplexity 4.80074
Epoch 4, iter 500, train loss 1.610, val perplexity 4.80041
Epoch 4, iter 600, train loss 1.576, val perplexity 4.80006
Epoch 4, iter 700, train loss 1.572, val perplexity 4.79980
Epoch 4, iter 800, train loss 1.531, val perplexity 4.79941
Epoch 4, iter 900, train loss 1.592, val perplexity 4.79916
Epoch 4, iter 1000, train loss 1.616, val perplexity 4.79881
Epoch 4, iter 1100, train loss 1.552, val perplexity 4.79852
Epoch 4, iter 1200, train loss 1.555, val perplexity 4.79816
Epoch 4, iter 1300, train loss 1.559, val perplexity 4.79789
Epoch 4, iter 1400, train loss 1.575, val perplexity 4.79779
Epoch 4, iter 1500, train loss 1.551, val perplexity 4.79757
Epoch 4, iter 1600, train loss 1.560, val perplexity 4.79729
Epoch 4, iter 1700, train loss 1.561, val perplexity 4.79705
Epoch 4, iter 1800, train loss 1.601, val perplexity 4.79683
Epoch 4, iter 1900, train loss 1.623, val perplexity 4.79668
Epoch 4, iter 2000, train loss 1.599, val perplexity 4.79643
Epoch 4, iter 2100, train loss 1.590, val perplexity 4.79631
Epoch 4, iter 2200, train loss 1.558, val perplexity 4.79615
Epoch 4, iter 2300, train loss 1.525, val perplexity 4.79596
Epoch 4, iter 2400, train loss 1.539, val perplexity 4.79583
Epoch 4, iter 2500, train loss 1.563, val perplexity 4.79558
Epoch 4, iter 2600, train loss 1.591, val perplexity 4.79545
Epoch 4, iter 2700, train loss 1.521, val perplexity 4.79524
Epoch 4, iter 2800, train loss 1.575, val perplexity 4.79513
Epoch 4, iter 2900, train loss 1.576, val perplexity 4.79506
Epoch 4, iter 3000, train loss 1.547, val perplexity 4.79490
Epoch 5, iter 0, train loss 1.539, val perplexity 4.79486
Epoch 5, iter 100, train loss 1.568, val perplexity 4.79480
Epoch 5, iter 200, train loss 1.569, val perplexity 4.79471
Epoch 5, iter 300, train loss 1.601, val perplexity 4.79453
Epoch 5, iter 400, train loss 1.583, val perplexity 4.79449
Epoch 5, iter 500, train loss 1.590, val perplexity 4.79446
Epoch 5, iter 600, train loss 1.556, val perplexity 4.79428
Epoch 5, iter 700, train loss 1.540, val perplexity 4.79427
Epoch 5, iter 800, train loss 1.568, val perplexity 4.79408
Epoch 5, iter 900, train loss 1.548, val perplexity 4.79403
Epoch 5, iter 1000, train loss 1.571, val perplexity 4.79394
Epoch 5, iter 1100, train loss 1.543, val perplexity 4.79381
Epoch 5, iter 1200, train loss 1.574, val perplexity 4.79374
Epoch 5, iter 1300, train loss 1.602, val perplexity 4.79369
Epoch 5, iter 1400, train loss 1.572, val perplexity 4.79358
Epoch 5, iter 1500, train loss 1.583, val perplexity 4.79348
Epoch 5, iter 1600, train loss 1.588, val perplexity 4.79347
Epoch 5, iter 1700, train loss 1.565, val perplexity 4.79347
Epoch 5, iter 1800, train loss 1.569, val perplexity 4.79332
Epoch 5, iter 1900, train loss 1.548, val perplexity 4.79323
Epoch 5, iter 2000, train loss 1.559, val perplexity 4.79323
Epoch 5, iter 2100, train loss 1.599, val perplexity 4.79318
Epoch 5, iter 2200, train loss 1.581, val perplexity 4.79318
Epoch 5, iter 2300, train loss 1.530, val perplexity 4.79305
Epoch 5, iter 2400, train loss 1.576, val perplexity 4.79312
Epoch 5, iter 2500, train loss 1.561, val perplexity 4.79304
Epoch 5, iter 2600, train loss 1.553, val perplexity 4.79298
Epoch 5, iter 2700, train loss 1.535, val perplexity 4.79300
Epoch 5, iter 2800, train loss 1.569, val perplexity 4.79297
Epoch 5, iter 2900, train loss 1.543, val perplexity 4.79285
Epoch 5, iter 3000, train loss 1.589, val perplexity 4.79279
Epoch 6, iter 0, train loss 1.545, val perplexity 4.79279
Epoch 6, iter 100, train loss 1.613, val perplexity 4.79277
Epoch 6, iter 200, train loss 1.548, val perplexity 4.79275
Epoch 6, iter 300, train loss 1.573, val perplexity 4.79275
Epoch 6, iter 400, train loss 1.556, val perplexity 4.79269
Epoch 6, iter 500, train loss 1.555, val perplexity 4.79263
Epoch 6, iter 600, train loss 1.528, val perplexity 4.79261
Epoch 6, iter 700, train loss 1.527, val perplexity 4.79269
Epoch 6, iter 800, train loss 1.540, val perplexity 4.79256
Epoch 6, iter 900, train loss 1.585, val perplexity 4.79248
Epoch 6, iter 1000, train loss 1.564, val perplexity 4.79251
Epoch 6, iter 1100, train loss 1.542, val perplexity 4.79248
Epoch 6, iter 1200, train loss 1.613, val perplexity 4.79246
Epoch 6, iter 1300, train loss 1.575, val perplexity 4.79240
Epoch 6, iter 1400, train loss 1.543, val perplexity 4.79233
Epoch 6, iter 1500, train loss 1.572, val perplexity 4.79232
Epoch 6, iter 1600, train loss 1.608, val perplexity 4.79226
Epoch 6, iter 1700, train loss 1.562, val perplexity 4.79224
Epoch 6, iter 1800, train loss 1.584, val perplexity 4.79229
Epoch 6, iter 1900, train loss 1.536, val perplexity 4.79232
Epoch 6, iter 2000, train loss 1.524, val perplexity 4.79231
Epoch 6, iter 2100, train loss 1.536, val perplexity 4.79227
Epoch 6, iter 2200, train loss 1.563, val perplexity 4.79223
Epoch 6, iter 2300, train loss 1.573, val perplexity 4.79226
Epoch 6, iter 2400, train loss 1.538, val perplexity 4.79230
Epoch 6, iter 2500, train loss 1.573, val perplexity 4.79224
Epoch 6, iter 2600, train loss 1.606, val perplexity 4.79219
Epoch 6, iter 2700, train loss 1.539, val perplexity 4.79223
Epoch 6, iter 2800, train loss 1.574, val perplexity 4.79216
Epoch 6, iter 2900, train loss 1.582, val perplexity 4.79214
Epoch 6, iter 3000, train loss 1.581, val perplexity 4.79211
Epoch 7, iter 0, train loss 1.586, val perplexity 4.79209
Epoch 7, iter 100, train loss 1.586, val perplexity 4.79212
Epoch 7, iter 200, train loss 1.585, val perplexity 4.79217
Epoch 7, iter 300, train loss 1.583, val perplexity 4.79227
Epoch 7, iter 400, train loss 1.573, val perplexity 4.79208
Epoch 7, iter 500, train loss 1.599, val perplexity 4.79205
Epoch 7, iter 600, train loss 1.531, val perplexity 4.79208
Epoch 7, iter 700, train loss 1.606, val perplexity 4.79202
Epoch 7, iter 800, train loss 1.589, val perplexity 4.79202
Epoch 7, iter 900, train loss 1.543, val perplexity 4.79212
Epoch 7, iter 1000, train loss 1.576, val perplexity 4.79213
Epoch 7, iter 1100, train loss 1.563, val perplexity 4.79207
Epoch 7, iter 1200, train loss 1.581, val perplexity 4.79206
Epoch 7, iter 1300, train loss 1.591, val perplexity 4.79199
Epoch 7, iter 1400, train loss 1.562, val perplexity 4.79195
Epoch 7, iter 1500, train loss 1.533, val perplexity 4.79199
Epoch 7, iter 1600, train loss 1.536, val perplexity 4.79202
Epoch 7, iter 1700, train loss 1.554, val perplexity 4.79206
Epoch 7, iter 1800, train loss 1.565, val perplexity 4.79201
Epoch 7, iter 1900, train loss 1.541, val perplexity 4.79199
Epoch 7, iter 2000, train loss 1.533, val perplexity 4.79195
Epoch 7, iter 2100, train loss 1.555, val perplexity 4.79194
Epoch 7, iter 2200, train loss 1.558, val perplexity 4.79192
Epoch 7, iter 2300, train loss 1.527, val perplexity 4.79196
Epoch 7, iter 2400, train loss 1.599, val perplexity 4.79195
Epoch 7, iter 2500, train loss 1.630, val perplexity 4.79196
Epoch 7, iter 2600, train loss 1.619, val perplexity 4.79197
Epoch 7, iter 2700, train loss 1.537, val perplexity 4.79194
Epoch 7, iter 2800, train loss 1.553, val perplexity 4.79197
Epoch 7, iter 2900, train loss 1.560, val perplexity 4.79203
Epoch 7, iter 3000, train loss 1.589, val perplexity 4.79190
Epoch 8, iter 0, train loss 1.558, val perplexity 4.79190
Epoch 8, iter 100, train loss 1.543, val perplexity 4.79199
Epoch 8, iter 200, train loss 1.567, val perplexity 4.79198
Epoch 8, iter 300, train loss 1.599, val perplexity 4.79195
Epoch 8, iter 400, train loss 1.560, val perplexity 4.79198
Epoch 8, iter 500, train loss 1.569, val perplexity 4.79191
Epoch 8, iter 600, train loss 1.549, val perplexity 4.79200
Epoch 8, iter 700, train loss 1.585, val perplexity 4.79195
Epoch 8, iter 800, train loss 1.590, val perplexity 4.79198
Epoch 8, iter 900, train loss 1.585, val perplexity 4.79204
Epoch 8, iter 1000, train loss 1.582, val perplexity 4.79201
Epoch 8, iter 1100, train loss 1.558, val perplexity 4.79200
Epoch 8, iter 1200, train loss 1.520, val perplexity 4.79202
Epoch 8, iter 1300, train loss 1.588, val perplexity 4.79201
Epoch 8, iter 1400, train loss 1.556, val perplexity 4.79201
Epoch 8, iter 1500, train loss 1.529, val perplexity 4.79189
Epoch 8, iter 1600, train loss 1.569, val perplexity 4.79186
Epoch 8, iter 1700, train loss 1.539, val perplexity 4.79182
Epoch 8, iter 1800, train loss 1.636, val perplexity 4.79178
Epoch 8, iter 1900, train loss 1.536, val perplexity 4.79179
Epoch 8, iter 2000, train loss 1.547, val perplexity 4.79180
Epoch 8, iter 2100, train loss 1.598, val perplexity 4.79194
Epoch 8, iter 2200, train loss 1.527, val perplexity 4.79190
Epoch 8, iter 2300, train loss 1.567, val perplexity 4.79184
Epoch 8, iter 2400, train loss 1.564, val perplexity 4.79183
Epoch 8, iter 2500, train loss 1.553, val perplexity 4.79192
Epoch 8, iter 2600, train loss 1.542, val perplexity 4.79191
Epoch 8, iter 2700, train loss 1.563, val perplexity 4.79192
Epoch 8, iter 2800, train loss 1.567, val perplexity 4.79188
Epoch 8, iter 2900, train loss 1.580, val perplexity 4.79177
Epoch 8, iter 3000, train loss 1.551, val perplexity 4.79179
Epoch 9, iter 0, train loss 1.567, val perplexity 4.79179
Epoch 9, iter 100, train loss 1.571, val perplexity 4.79185
Epoch 9, iter 200, train loss 1.576, val perplexity 4.79187
Epoch 9, iter 300, train loss 1.578, val perplexity 4.79184
Epoch 9, iter 400, train loss 1.593, val perplexity 4.79181
Epoch 9, iter 500, train loss 1.595, val perplexity 4.79183
Epoch 9, iter 600, train loss 1.592, val perplexity 4.79173
Epoch 9, iter 700, train loss 1.554, val perplexity 4.79169
Epoch 9, iter 800, train loss 1.579, val perplexity 4.79179
Epoch 9, iter 900, train loss 1.583, val perplexity 4.79188
Epoch 9, iter 1000, train loss 1.532, val perplexity 4.79181
Epoch 9, iter 1100, train loss 1.561, val perplexity 4.79181
Epoch 9, iter 1200, train loss 1.540, val perplexity 4.79177
Epoch 9, iter 1300, train loss 1.555, val perplexity 4.79179
Epoch 9, iter 1400, train loss 1.550, val perplexity 4.79187
Epoch 9, iter 1500, train loss 1.554, val perplexity 4.79184
Epoch 9, iter 1600, train loss 1.602, val perplexity 4.79190
Epoch 9, iter 1700, train loss 1.556, val perplexity 4.79185
Epoch 9, iter 1800, train loss 1.571, val perplexity 4.79184
Epoch 9, iter 1900, train loss 1.525, val perplexity 4.79194
Epoch 9, iter 2000, train loss 1.588, val perplexity 4.79188
Epoch 9, iter 2100, train loss 1.530, val perplexity 4.79193
Epoch 9, iter 2200, train loss 1.588, val perplexity 4.79187
Epoch 9, iter 2300, train loss 1.578, val perplexity 4.79190
Epoch 9, iter 2400, train loss 1.572, val perplexity 4.79186
Epoch 9, iter 2500, train loss 1.553, val perplexity 4.79189
Epoch 9, iter 2600, train loss 1.567, val perplexity 4.79179
Epoch 9, iter 2700, train loss 1.572, val perplexity 4.79172
Epoch 9, iter 2800, train loss 1.558, val perplexity 4.79176
Epoch 9, iter 2900, train loss 1.534, val perplexity 4.79190
Epoch 9, iter 3000, train loss 1.570, val perplexity 4.79183</code></pre>
</div>
</div>
<p>We can now test the model by generating new SMILES strings. We will start with a random token and generate 100 new tokens.</p>
<div id="cell-53" class="cell" data-execution_count="368">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([[<span class="dv">4</span>]])</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> a.to(device)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>generation <span class="op">=</span> model.generate(a, max_new_tokens<span class="op">=</span><span class="dv">30</span>).cpu().numpy()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>smiles <span class="op">=</span> tokenizer.decode(generation[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-54" class="cell" data-execution_count="369">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>smiles</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="369">
<pre><code>'[C@@](O)C'</code></pre>
</div>
</div>
<div id="cell-55" class="cell" data-execution_count="370">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>Chem.MolFromSmiles(smiles)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="370">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This does not look too bad, but we can do better (if you would run the code multiple times, you would see that the results are not always a valid SMILES).</p>
</section>
</section>
<section id="making-tokens-talk-using-attention" class="level2">
<h2 class="anchored" data-anchor-id="making-tokens-talk-using-attention">Making tokens talk using attention</h2>
<p>In our bigram models we made predictions based on the previous word. This is clearly not enough to make good predictions. We can improve our model by taking into more past tokens into account.</p>
<p>One naïve way to incorporate more context into our model might be to simply “pool” (features of) the preceding tokens. This kind of pooling is similar to what we do in GNNs, e.g., to combine node embeddings.</p>
<p>A very simple pooling operation is the average of the embeddings of the preceding tokens. Later, when we will implement self-attention, we will not use a simple average, but a special weighted average. The code for that will use similar ideas (in particular, the causal mask).</p>
<div id="cell-59" class="cell" data-execution_count="371">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>B, T, C <span class="op">=</span> <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span> <span class="co"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create random data of shape (B, T, C)</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>x_bag_of_words <span class="op">=</span> torch.zeros((B,T,C))</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>        x_prev <span class="op">=</span> x[b,:t<span class="op">+</span><span class="dv">1</span>] <span class="co"># shape (t, C)</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        x_bag_of_words[b, t] <span class="op">=</span> torch.mean(x_prev, dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># shape (C,)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This nested for loop is slow. However, we can implement this in an efficient way if we observe a few things:</p>
<ul>
<li><p>If we want to predict next tokens, we do not want to let the future tokens influence the prediction. Therefore, we can use a so-called causal mask to mask out the future tokens.</p></li>
<li><p>A matrix multiplication can be thought of as a weighted sum of the rows of the matrix, where the weights are given by the columns of the matrix. This is easy to see if we think of the following extremes:</p>
<ul>
<li>We can compute the sum of the rows of a matrix by multiplying the matrix with a lower-triangular matrix filled with ones.</li>
<li>We can compute the mean of the rows of a matrix by multiplying the matrix with a lower-triangular matrix filled with ones and dividing by the number of ones in the lower-triangular matrix.</li>
</ul></li>
</ul>
<p>In <code>torch</code> we can use <code>tril</code> to create a lower-triangular matrix.</p>
<div id="cell-62" class="cell" data-execution_count="372">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>lower_triangular_mask <span class="op">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> torch.ones((T,T))</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> torch.masked_fill(weight, lower_triangular_mask<span class="op">==</span><span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> torch.softmax(weight, dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-63" class="cell" data-execution_count="373">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>weight  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="373">
<pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],
        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])</code></pre>
</div>
</div>
<p>We used the softmax function to normalize the weights in the rows.</p>
<div id="cell-65" class="cell" data-execution_count="374">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>weight <span class="op">@</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="374">
<pre><code>tensor([[[ 2.7713,  0.4576,  2.1195],
         [ 1.8329,  0.5148,  0.9036],
         [ 0.9509,  0.0041,  0.9987],
         [ 0.3513, -0.1176,  0.5793],
         [ 0.1679, -0.1204,  0.5011]],

        [[-0.3739, -0.3857, -0.7389],
         [-0.5810, -0.5098, -1.7110],
         [-0.3690, -0.4240, -1.1107],
         [-0.0953, -0.3274, -0.5838],
         [ 0.1815, -0.3243, -0.4050]]])</code></pre>
</div>
</div>
<p>In the simple average we used above, all past tokens were treated equally. However, it might be useful to <em>pay more attention</em> to certain tokens than to others. That is, we want to gather information from the past – but do this in a data-dependent way. The attention mechanism allows us to do this.</p>
<p>The attention mechanism does this by having a query vector <span class="math inline">\(q\)</span> and a key vector <span class="math inline">\(k\)</span> for each token. We then define “similarity” or “relevance” between two tokens <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> as the dot product between their query and key vectors, which we derive from the embeddings of the tokens by multiplying them with the learnable weight matrices <span class="math inline">\(W_q\)</span> and <span class="math inline">\(W_k\)</span>.</p>
<p><span class="math display">\[
\text{sim}(i, j) = a(i, h) = q_ik_j^T = \text{emb}_i W_q W_k^T \text{emb}_j^T
\]</span></p>
<p>Note that this gives us now a way to refine the <code>weight_matrix</code> we used above. Instead of weighting all tokens equally, we can now learn a weight matrix that tells us how much attention to pay to each token.</p>
<p>To start the implementation, we will first derive query and key vectors from the embeddings. We will then compute the similarity matrix and apply the softmax function to normalize the weights.</p>
<div id="cell-68" class="cell" data-execution_count="375">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>B, T, C <span class="op">=</span> <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span> <span class="co"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>head_size <span class="op">=</span> <span class="dv">16</span> <span class="co"># hyperparameter</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co"># with bias = False, it only perform matrix multiplication</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>key_layer <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)  </span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>query_layer <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The attention matrix defined above is now a simple matrix multiplication between the query and key vectors. The attention matrix is then normalized using a softmax function.</p>
<div id="cell-70" class="cell" data-execution_count="376">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> query_layer(x) <span class="co"># shape (B, T, head_size)</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> key_layer(x) <span class="co"># shape (B, T, head_size)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-71" class="cell" data-execution_count="377">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> query <span class="op">@</span> key.transpose(<span class="dv">1</span>,<span class="dv">2</span>) <span class="co"># shape (B, T, T)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the shape of the attention matrix is (B, T, T). The attention matrix is a matrix where each row corresponds to a query and each column corresponds to a key. The value at position (i, j) in the attention matrix is the attention score between the i-th query and the j-th key.</p>
<div id="cell-73" class="cell" data-execution_count="378">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="378">
<pre><code>tensor([[[-0.1377,  0.3945, -0.1910, -0.3166,  0.5705],
         [ 0.2263, -1.1153,  0.0163,  1.0653, -0.9115],
         [-0.3157,  0.5693, -0.7330, -0.4713,  1.6627],
         [-0.1497,  0.9112, -0.0370, -1.1351,  1.1552],
         [ 0.6523, -1.6878,  1.3558,  1.8505, -4.0957]],

        [[-0.4646,  0.6153, -0.3081,  1.0515,  0.5917],
         [ 0.3343, -1.2245, -0.7600, -1.6172, -1.2108],
         [-0.6809, -0.0852, -1.6940,  0.4584, -0.1262],
         [ 0.7665, -1.8694, -0.5606, -2.6797, -1.8310],
         [ 0.2937, -1.1780, -0.7986, -1.5304, -1.1683]]],
       grad_fn=&lt;UnsafeViewBackward0&gt;)</code></pre>
</div>
</div>
<p>But to avoid the future tokens to influence the prediction, we will use a causal mask. We do this the same way as we did above, by using <code>torch.tril</code>.</p>
<div id="cell-75" class="cell" data-execution_count="379">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>lower_triangular_mask <span class="op">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> torch.masked_fill(attention, lower_triangular_mask<span class="op">==</span><span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))   </span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> torch.softmax(attention, dim<span class="op">=</span><span class="dv">2</span>) <span class="co"># shape (B, T, T), softmax along the last dimension</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> attention <span class="op">@</span> x <span class="co"># shape (B, T, T) @ (B, T, C) = (B, T, C)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the attention mechanism popularized in the <a href="https://arxiv.org/abs/1706.03762">“attention is all you need” paper</a> we add even more expressive power by transforming <code>x</code> before we multiply it with the attention matrix. We call this transformed <code>x</code> the value vector (or matrix). The full implementation of the attention mechanism is then:</p>
<div id="cell-77" class="cell" data-execution_count="380">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>B, T, C <span class="op">=</span> <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span> <span class="co"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>head_size <span class="op">=</span> <span class="dv">16</span> <span class="co"># hyperparameter</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co"># what do I contain</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co"># with bias = False, it only perform matrix multiplication</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="co"># what am I looking for</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="co"># what I will tell you</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>) <span class="co"># Output: (B, T, head_size)</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="co"># self-attention because k, q, v come all from the same input</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> key(x) <span class="co"># shape (B, T, head_size)</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> query(x) <span class="co"># shape (B, T, head_size)</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> value(x) <span class="co"># shape (B, T, head_size)</span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a><span class="co"># now, we want to compute the attention</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to compute the dot product between k and q</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>weight_matrix <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a><span class="co"># now we add the masking</span></span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a><span class="co"># we want to mask out the future</span></span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a><span class="co"># this is what is known as "decoder" block </span></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>lower_triangular <span class="op">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>weight_matrix <span class="op">=</span> weight_matrix.masked_fill(lower_triangular<span class="op">==</span><span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a><span class="co"># use softmax to normalize</span></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>weight_matrix <span class="op">=</span> torch.softmax(weight_matrix, dim<span class="op">=-</span><span class="dv">1</span>)<span class="op">/</span>np.sqrt(head_size) <span class="co"># shape (B, T, T)</span></span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> weight_matrix <span class="op">@</span> v <span class="co"># shape (B, T, T) @ (B, T, head_size) = (B, T, head_size)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="interlude-why-do-we-divide-by-sqrthead_size-in-the-self-attention-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="interlude-why-do-we-divide-by-sqrthead_size-in-the-self-attention-mechanism">Interlude: Why do we divide by sqrt(head_size) in the self-attention mechanism?</h4>
<p>We used one more trick to make the training more stable. We scaled the weight_matrix by the square root of the head_size. <a href="https://ai.stackexchange.com/questions/21237/why-does-this-multiplication-of-q-and-k-have-a-variance-of-d-k-in-scaled">This is because the variance of the dot product is proportional to the dimensionality of the vectors.</a>. Not scaling the weight matrix can lead to numerical instability.</p>
<p>To see this, let’s run a quick experiment</p>
<div id="cell-81" class="cell" data-execution_count="381">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>variances <span class="op">=</span> []</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>dimensions <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">100000</span>]</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> dimensions:</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> torch.randn(B, T, d)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> torch.randn(B, T, d)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute the batched matrix product between k and q</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    weight_matrix <span class="op">=</span> torch.bmm(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))   <span class="co"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    variances.append(weight_matrix.var())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-82" class="cell" data-execution_count="382">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>plt.plot(dimensions, variances)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimensionality'</span>)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Variance'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="382">
<pre><code>Text(0, 0.5, 'Variance')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-39-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This has an important impact when we apply <code>softmax</code>. Positive and negative “outliers” will be “sequeezed” to 1 and 0. You can test this by creating a 1D tensor (<code>a</code>) and applying softmax on it. Then multiply the values in the tensor (<code>a</code>) and again apply softmax.</p>
<div id="cell-84" class="cell" data-execution_count="383">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(F.softmax(torch.tensor([<span class="fl">1.</span>,<span class="fl">2.</span>,<span class="fl">3.</span>])),F.softmax(torch.tensor([<span class="fl">1.</span>,<span class="fl">2.</span>,<span class="fl">3.</span>])<span class="op">*</span><span class="dv">100</span>) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.0900, 0.2447, 0.6652]) tensor([0.0000e+00, 3.7835e-44, 1.0000e+00])</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/ipykernel_51170/1895642280.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  print(F.softmax(torch.tensor([1.,2.,3.])),F.softmax(torch.tensor([1.,2.,3.])*100) )</code></pre>
</div>
</div>
</section>
<section id="the-attention-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="the-attention-mechanism">The attention mechanism</h4>
<p>Written as a formula, the attention mechanism is:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(Q\)</span> is the query matrix, <span class="math inline">\(K\)</span> is the key matrix, and <span class="math inline">\(V\)</span> is the value matrix.</p>
</section>
<section id="refactoring-into-a-module" class="level3">
<h3 class="anchored" data-anchor-id="refactoring-into-a-module">Refactoring into a module</h3>
<div id="cell-88" class="cell" data-execution_count="384">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embed, block_size, head_size):</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embed, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embed, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embed, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'lower_triangular'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): </span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>        B, T, C  <span class="op">=</span> x.shape</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.key(x)</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># B, T, head</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.value(x)   <span class="co"># B, T, head</span></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>        weight_matrix <span class="op">=</span> query <span class="op">@</span> key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C <span class="op">**</span> (<span class="op">-</span><span class="fl">0.5</span>) <span class="co"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>        weight_matrix <span class="op">=</span> weight_matrix.masked_fill(<span class="va">self</span>.lower_triangular[:T, :T].logical_not(), <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>        weight_matrix <span class="op">=</span> F.softmax(weight_matrix, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> weight_matrix <span class="op">@</span> value <span class="co"># shape (B, T, T) @ (B, T, head_size) = (B, T, head_size)</span></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="revamped-bigram-model" class="level3">
<h3 class="anchored" data-anchor-id="revamped-bigram-model">Revamped Bigram Model</h3>
<p>Now, we can use it to “refine” our bigram model. We will additionally also perform two more changes:</p>
<ul>
<li>we will add positional embeddings: We will add the positional embeddings to the input embeddings. This will allow the model to take into account the position of the tokens in the sequence.</li>
<li>we will add one more indirection: One simple way of improving the expressiveness is to add one linear layer. While in the bigram model we only had one embedding layer (that mapped inputs of size <code>vocab_size</code> to <code>vocab_size</code>), we can now change the embedding layer to map inputs of size <code>vocab_size</code> to <code>embedding_size</code>. We can then add a linear layer that maps inputs of size <code>embedding_size</code> to <code>vocab_size</code>. This way, we can learn a more complex mapping from the embeddings to the next token.</li>
</ul>
<div id="cell-91" class="cell" data-execution_count="385">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttentionModel(nn.Module):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, sequence_length<span class="op">=</span><span class="dv">100</span>, head_size<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># map the input ids to embeddings</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embedding <span class="op">=</span>  nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add positional embeddings (each position has its own learnable embedding vector)</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> nn.Embedding(sequence_length, embedding_dim)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the self-attention layer</span></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> Head(embedding_dim, sequence_length, head_size)</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the linear layer that maps the output of the self-attention layer to the vocabulary size</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(head_size, vocab_size)</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># store the sequence length</span></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a>        B, T <span class="op">=</span> x.shape</span>
<span id="cb58-22"><a href="#cb58-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.token_embedding(x) <span class="co"># B, T, C </span></span>
<span id="cb58-23"><a href="#cb58-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">+=</span> <span class="va">self</span>.positional_embedding(torch.arange(T, device<span class="op">=</span>device)) <span class="co"># B, T, C</span></span>
<span id="cb58-24"><a href="#cb58-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.attention(x) <span class="co"># B, T, head_size</span></span>
<span id="cb58-25"><a href="#cb58-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lm_head(x) <span class="co"># B, T, vocab_size</span></span>
<span id="cb58-26"><a href="#cb58-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The prediction is for each token a probability distribution over the vocabulary</span></span>
<span id="cb58-27"><a href="#cb58-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this indicates how likely each token is the next token</span></span>
<span id="cb58-28"><a href="#cb58-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb58-29"><a href="#cb58-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-30"><a href="#cb58-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-31"><a href="#cb58-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y):</span>
<span id="cb58-32"><a href="#cb58-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb58-33"><a href="#cb58-33" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb58-34"><a href="#cb58-34" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb58-35"><a href="#cb58-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-36"><a href="#cb58-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note that that the implementation below is because of how we - for educational purposes - have defined the dataset</span></span>
<span id="cb58-37"><a href="#cb58-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A better way is to have inputs and outputs of the same length (and to not manually code the sliding window</span></span>
<span id="cb58-38"><a href="#cb58-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but to instead use a causal mask)</span></span>
<span id="cb58-39"><a href="#cb58-39" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># we only care about the last token </span></span>
<span id="cb58-40"><a href="#cb58-40" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(B, C)</span>
<span id="cb58-41"><a href="#cb58-41" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B)</span>
<span id="cb58-42"><a href="#cb58-42" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb58-43"><a href="#cb58-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb58-44"><a href="#cb58-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb58-45"><a href="#cb58-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, x, max_new_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb58-46"><a href="#cb58-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb58-47"><a href="#cb58-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we generate max_new_tokens new tokens</span></span>
<span id="cb58-48"><a href="#cb58-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _t <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb58-49"><a href="#cb58-49" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb58-50"><a href="#cb58-50" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb58-51"><a href="#cb58-51" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb58-52"><a href="#cb58-52" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb58-53"><a href="#cb58-53" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, next_token], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb58-54"><a href="#cb58-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb58-55"><a href="#cb58-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb58-56"><a href="#cb58-56" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-92" class="cell" data-execution_count="386">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SelfAttentionModel(<span class="bu">len</span>(tokenizer.tokens), embedding_dim<span class="op">=</span><span class="dv">128</span>, sequence_length<span class="op">=</span><span class="dv">40</span>, head_size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, valid_loader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, iter 0, train loss 3.889, val perplexity 46.51448
Epoch 0, iter 100, train loss 1.883, val perplexity 6.66113
Epoch 0, iter 200, train loss 1.638, val perplexity 5.01923
Epoch 0, iter 300, train loss 1.536, val perplexity 4.70023
Epoch 0, iter 400, train loss 1.548, val perplexity 4.58597
Epoch 0, iter 500, train loss 1.504, val perplexity 4.38599
Epoch 0, iter 600, train loss 1.441, val perplexity 4.25886
Epoch 0, iter 700, train loss 1.469, val perplexity 4.18827
Epoch 0, iter 800, train loss 1.392, val perplexity 4.15055
Epoch 0, iter 900, train loss 1.401, val perplexity 4.07537
Epoch 0, iter 1000, train loss 1.405, val perplexity 3.99194
Epoch 0, iter 1100, train loss 1.363, val perplexity 3.90569
Epoch 0, iter 1200, train loss 1.358, val perplexity 3.86271
Epoch 0, iter 1300, train loss 1.274, val perplexity 3.82789
Epoch 0, iter 1400, train loss 1.339, val perplexity 3.80141
Epoch 0, iter 1500, train loss 1.336, val perplexity 3.77024
Epoch 0, iter 1600, train loss 1.320, val perplexity 3.74822
Epoch 0, iter 1700, train loss 1.306, val perplexity 3.71429
Epoch 0, iter 1800, train loss 1.319, val perplexity 3.67578
Epoch 0, iter 1900, train loss 1.317, val perplexity 3.65535
Epoch 0, iter 2000, train loss 1.317, val perplexity 3.58378
Epoch 0, iter 2100, train loss 1.286, val perplexity 3.55721
Epoch 0, iter 2200, train loss 1.259, val perplexity 3.53200
Epoch 0, iter 2300, train loss 1.223, val perplexity 3.53396
Epoch 0, iter 2400, train loss 1.276, val perplexity 3.51743
Epoch 0, iter 2500, train loss 1.250, val perplexity 3.48564
Epoch 0, iter 2600, train loss 1.247, val perplexity 3.47809
Epoch 0, iter 2700, train loss 1.269, val perplexity 3.46225
Epoch 0, iter 2800, train loss 1.275, val perplexity 3.46858
Epoch 0, iter 2900, train loss 1.243, val perplexity 3.45377
Epoch 0, iter 3000, train loss 1.246, val perplexity 3.45255
Epoch 1, iter 0, train loss 1.241, val perplexity 3.43818
Epoch 1, iter 100, train loss 1.225, val perplexity 3.43851
Epoch 1, iter 200, train loss 1.247, val perplexity 3.41987
Epoch 1, iter 300, train loss 1.211, val perplexity 3.43688
Epoch 1, iter 400, train loss 1.240, val perplexity 3.40300
Epoch 1, iter 500, train loss 1.222, val perplexity 3.37348
Epoch 1, iter 600, train loss 1.164, val perplexity 3.33770
Epoch 1, iter 700, train loss 1.235, val perplexity 3.32229
Epoch 1, iter 800, train loss 1.180, val perplexity 3.31498
Epoch 1, iter 900, train loss 1.176, val perplexity 3.32122
Epoch 1, iter 1000, train loss 1.178, val perplexity 3.29877
Epoch 1, iter 1100, train loss 1.198, val perplexity 3.28752
Epoch 1, iter 1200, train loss 1.145, val perplexity 3.28561
Epoch 1, iter 1300, train loss 1.212, val perplexity 3.26526
Epoch 1, iter 1400, train loss 1.222, val perplexity 3.27166
Epoch 1, iter 1500, train loss 1.179, val perplexity 3.26950
Epoch 1, iter 1600, train loss 1.183, val perplexity 3.25246
Epoch 1, iter 1700, train loss 1.204, val perplexity 3.25885
Epoch 1, iter 1800, train loss 1.181, val perplexity 3.25160
Epoch 1, iter 1900, train loss 1.163, val perplexity 3.24419
Epoch 1, iter 2000, train loss 1.137, val perplexity 3.23455
Epoch 1, iter 2100, train loss 1.203, val perplexity 3.23678
Epoch 1, iter 2200, train loss 1.216, val perplexity 3.23619
Epoch 1, iter 2300, train loss 1.185, val perplexity 3.23046
Epoch 1, iter 2400, train loss 1.203, val perplexity 3.22142
Epoch 1, iter 2500, train loss 1.188, val perplexity 3.22653
Epoch 1, iter 2600, train loss 1.157, val perplexity 3.21694
Epoch 1, iter 2700, train loss 1.187, val perplexity 3.21164
Epoch 1, iter 2800, train loss 1.130, val perplexity 3.20060
Epoch 1, iter 2900, train loss 1.143, val perplexity 3.19579
Epoch 1, iter 3000, train loss 1.195, val perplexity 3.19294
Epoch 2, iter 0, train loss 1.126, val perplexity 3.20773
Epoch 2, iter 100, train loss 1.202, val perplexity 3.19967
Epoch 2, iter 200, train loss 1.174, val perplexity 3.18501
Epoch 2, iter 300, train loss 1.188, val perplexity 3.18238
Epoch 2, iter 400, train loss 1.138, val perplexity 3.18118
Epoch 2, iter 500, train loss 1.136, val perplexity 3.18097
Epoch 2, iter 600, train loss 1.168, val perplexity 3.17053
Epoch 2, iter 700, train loss 1.120, val perplexity 3.15899
Epoch 2, iter 800, train loss 1.159, val perplexity 3.15819
Epoch 2, iter 900, train loss 1.118, val perplexity 3.17680
Epoch 2, iter 1000, train loss 1.097, val perplexity 3.15708
Epoch 2, iter 1100, train loss 1.157, val perplexity 3.15672
Epoch 2, iter 1200, train loss 1.170, val perplexity 3.16435
Epoch 2, iter 1300, train loss 1.156, val perplexity 3.16167
Epoch 2, iter 1400, train loss 1.141, val perplexity 3.15502
Epoch 2, iter 1500, train loss 1.138, val perplexity 3.13853
Epoch 2, iter 1600, train loss 1.179, val perplexity 3.14547
Epoch 2, iter 1700, train loss 1.116, val perplexity 3.14258
Epoch 2, iter 1800, train loss 1.125, val perplexity 3.14083
Epoch 2, iter 1900, train loss 1.158, val perplexity 3.14367
Epoch 2, iter 2000, train loss 1.153, val perplexity 3.15006
Epoch 2, iter 2100, train loss 1.071, val perplexity 3.14123
Epoch 2, iter 2200, train loss 1.087, val perplexity 3.13333
Epoch 2, iter 2300, train loss 1.100, val perplexity 3.13311
Epoch 2, iter 2400, train loss 1.177, val perplexity 3.12805
Epoch 2, iter 2500, train loss 1.139, val perplexity 3.12344
Epoch 2, iter 2600, train loss 1.172, val perplexity 3.13074
Epoch 2, iter 2700, train loss 1.152, val perplexity 3.12924
Epoch 2, iter 2800, train loss 1.169, val perplexity 3.12610
Epoch 2, iter 2900, train loss 1.146, val perplexity 3.12171
Epoch 2, iter 3000, train loss 1.104, val perplexity 3.12374
Epoch 3, iter 0, train loss 1.138, val perplexity 3.11965
Epoch 3, iter 100, train loss 1.130, val perplexity 3.11538
Epoch 3, iter 200, train loss 1.149, val perplexity 3.12729
Epoch 3, iter 300, train loss 1.142, val perplexity 3.12698
Epoch 3, iter 400, train loss 1.184, val perplexity 3.11725
Epoch 3, iter 500, train loss 1.139, val perplexity 3.12115
Epoch 3, iter 600, train loss 1.109, val perplexity 3.12539
Epoch 3, iter 700, train loss 1.147, val perplexity 3.11643
Epoch 3, iter 800, train loss 1.129, val perplexity 3.12205
Epoch 3, iter 900, train loss 1.150, val perplexity 3.12080
Epoch 3, iter 1000, train loss 1.148, val perplexity 3.10857
Epoch 3, iter 1100, train loss 1.158, val perplexity 3.10570
Epoch 3, iter 1200, train loss 1.160, val perplexity 3.11082
Epoch 3, iter 1300, train loss 1.096, val perplexity 3.10202
Epoch 3, iter 1400, train loss 1.136, val perplexity 3.11115
Epoch 3, iter 1500, train loss 1.160, val perplexity 3.12037
Epoch 3, iter 1600, train loss 1.115, val perplexity 3.10564
Epoch 3, iter 1700, train loss 1.141, val perplexity 3.10538
Epoch 3, iter 1800, train loss 1.103, val perplexity 3.10921
Epoch 3, iter 1900, train loss 1.126, val perplexity 3.11212
Epoch 3, iter 2000, train loss 1.118, val perplexity 3.10539
Epoch 3, iter 2100, train loss 1.119, val perplexity 3.09715
Epoch 3, iter 2200, train loss 1.113, val perplexity 3.10317
Epoch 3, iter 2300, train loss 1.120, val perplexity 3.09733
Epoch 3, iter 2400, train loss 1.144, val perplexity 3.09822
Epoch 3, iter 2500, train loss 1.134, val perplexity 3.10760
Epoch 3, iter 2600, train loss 1.179, val perplexity 3.09432
Epoch 3, iter 2700, train loss 1.162, val perplexity 3.11052
Epoch 3, iter 2800, train loss 1.158, val perplexity 3.11656
Epoch 3, iter 2900, train loss 1.139, val perplexity 3.09534
Epoch 3, iter 3000, train loss 1.179, val perplexity 3.10282
Epoch 4, iter 0, train loss 1.124, val perplexity 3.10232
Epoch 4, iter 100, train loss 1.141, val perplexity 3.09656
Epoch 4, iter 200, train loss 1.145, val perplexity 3.09358
Epoch 4, iter 300, train loss 1.115, val perplexity 3.09710
Epoch 4, iter 400, train loss 1.169, val perplexity 3.09681
Epoch 4, iter 500, train loss 1.161, val perplexity 3.10573
Epoch 4, iter 600, train loss 1.101, val perplexity 3.10116
Epoch 4, iter 700, train loss 1.121, val perplexity 3.08844
Epoch 4, iter 800, train loss 1.062, val perplexity 3.09668
Epoch 4, iter 900, train loss 1.069, val perplexity 3.09515
Epoch 4, iter 1000, train loss 1.113, val perplexity 3.08247
Epoch 4, iter 1100, train loss 1.160, val perplexity 3.08931
Epoch 4, iter 1200, train loss 1.130, val perplexity 3.08274
Epoch 4, iter 1300, train loss 1.183, val perplexity 3.09541
Epoch 4, iter 1400, train loss 1.150, val perplexity 3.09614
Epoch 4, iter 1500, train loss 1.149, val perplexity 3.08139
Epoch 4, iter 1600, train loss 1.131, val perplexity 3.08812
Epoch 4, iter 1700, train loss 1.143, val perplexity 3.09312
Epoch 4, iter 1800, train loss 1.184, val perplexity 3.08449
Epoch 4, iter 1900, train loss 1.115, val perplexity 3.07812
Epoch 4, iter 2000, train loss 1.145, val perplexity 3.08757
Epoch 4, iter 2100, train loss 1.097, val perplexity 3.08763
Epoch 4, iter 2200, train loss 1.086, val perplexity 3.08908
Epoch 4, iter 2300, train loss 1.118, val perplexity 3.08329
Epoch 4, iter 2400, train loss 1.092, val perplexity 3.07425
Epoch 4, iter 2500, train loss 1.077, val perplexity 3.07932
Epoch 4, iter 2600, train loss 1.124, val perplexity 3.08189
Epoch 4, iter 2700, train loss 1.151, val perplexity 3.09261
Epoch 4, iter 2800, train loss 1.119, val perplexity 3.07745
Epoch 4, iter 2900, train loss 1.099, val perplexity 3.07391
Epoch 4, iter 3000, train loss 1.123, val perplexity 3.09299
Epoch 5, iter 0, train loss 1.118, val perplexity 3.08209
Epoch 5, iter 100, train loss 1.072, val perplexity 3.08084
Epoch 5, iter 200, train loss 1.117, val perplexity 3.09895
Epoch 5, iter 300, train loss 1.109, val perplexity 3.08415
Epoch 5, iter 400, train loss 1.151, val perplexity 3.07640
Epoch 5, iter 500, train loss 1.115, val perplexity 3.07644
Epoch 5, iter 600, train loss 1.173, val perplexity 3.06789
Epoch 5, iter 700, train loss 1.118, val perplexity 3.07208
Epoch 5, iter 800, train loss 1.114, val perplexity 3.06964
Epoch 5, iter 900, train loss 1.123, val perplexity 3.06521
Epoch 5, iter 1000, train loss 1.117, val perplexity 3.07689
Epoch 5, iter 1100, train loss 1.105, val perplexity 3.06304
Epoch 5, iter 1200, train loss 1.155, val perplexity 3.07131
Epoch 5, iter 1300, train loss 1.093, val perplexity 3.06734
Epoch 5, iter 1400, train loss 1.058, val perplexity 3.07034
Epoch 5, iter 1500, train loss 1.149, val perplexity 3.06001
Epoch 5, iter 1600, train loss 1.124, val perplexity 3.06218
Epoch 5, iter 1700, train loss 1.131, val perplexity 3.06177
Epoch 5, iter 1800, train loss 1.130, val perplexity 3.05882
Epoch 5, iter 1900, train loss 1.120, val perplexity 3.06167
Epoch 5, iter 2000, train loss 1.075, val perplexity 3.05305
Epoch 5, iter 2100, train loss 1.100, val perplexity 3.06269
Epoch 5, iter 2200, train loss 1.124, val perplexity 3.06574
Epoch 5, iter 2300, train loss 1.126, val perplexity 3.06347
Epoch 5, iter 2400, train loss 1.113, val perplexity 3.05534
Epoch 5, iter 2500, train loss 1.125, val perplexity 3.08321
Epoch 5, iter 2600, train loss 1.099, val perplexity 3.05985
Epoch 5, iter 2700, train loss 1.158, val perplexity 3.06098
Epoch 5, iter 2800, train loss 1.146, val perplexity 3.05263
Epoch 5, iter 2900, train loss 1.171, val perplexity 3.05878
Epoch 5, iter 3000, train loss 1.108, val perplexity 3.05882
Epoch 6, iter 0, train loss 1.063, val perplexity 3.06478
Epoch 6, iter 100, train loss 1.143, val perplexity 3.05597
Epoch 6, iter 200, train loss 1.086, val perplexity 3.06243
Epoch 6, iter 300, train loss 1.102, val perplexity 3.06036
Epoch 6, iter 400, train loss 1.130, val perplexity 3.05022
Epoch 6, iter 500, train loss 1.109, val perplexity 3.05755
Epoch 6, iter 600, train loss 1.142, val perplexity 3.05923
Epoch 6, iter 700, train loss 1.132, val perplexity 3.05757
Epoch 6, iter 800, train loss 1.085, val perplexity 3.05189
Epoch 6, iter 900, train loss 1.148, val perplexity 3.05542
Epoch 6, iter 1000, train loss 1.133, val perplexity 3.06147
Epoch 6, iter 1100, train loss 1.145, val perplexity 3.05915
Epoch 6, iter 1200, train loss 1.124, val perplexity 3.04750
Epoch 6, iter 1300, train loss 1.142, val perplexity 3.05894
Epoch 6, iter 1400, train loss 1.103, val perplexity 3.04810
Epoch 6, iter 1500, train loss 1.111, val perplexity 3.05013
Epoch 6, iter 1600, train loss 1.144, val perplexity 3.04804
Epoch 6, iter 1700, train loss 1.106, val perplexity 3.05326
Epoch 6, iter 1800, train loss 1.145, val perplexity 3.05340
Epoch 6, iter 1900, train loss 1.105, val perplexity 3.04603
Epoch 6, iter 2000, train loss 1.058, val perplexity 3.04617
Epoch 6, iter 2100, train loss 1.127, val perplexity 3.06316
Epoch 6, iter 2200, train loss 1.136, val perplexity 3.05213
Epoch 6, iter 2300, train loss 1.125, val perplexity 3.05162
Epoch 6, iter 2400, train loss 1.102, val perplexity 3.03990
Epoch 6, iter 2500, train loss 1.106, val perplexity 3.04742
Epoch 6, iter 2600, train loss 1.132, val perplexity 3.04673
Epoch 6, iter 2700, train loss 1.089, val perplexity 3.04486
Epoch 6, iter 2800, train loss 1.144, val perplexity 3.04106
Epoch 6, iter 2900, train loss 1.092, val perplexity 3.04550
Epoch 6, iter 3000, train loss 1.132, val perplexity 3.06314
Epoch 7, iter 0, train loss 1.142, val perplexity 3.03925
Epoch 7, iter 100, train loss 1.121, val perplexity 3.04713
Epoch 7, iter 200, train loss 1.086, val perplexity 3.04520
Epoch 7, iter 300, train loss 1.108, val perplexity 3.04185
Epoch 7, iter 400, train loss 1.133, val perplexity 3.04060
Epoch 7, iter 500, train loss 1.085, val perplexity 3.05072
Epoch 7, iter 600, train loss 1.096, val perplexity 3.03975
Epoch 7, iter 700, train loss 1.102, val perplexity 3.04847
Epoch 7, iter 800, train loss 1.151, val perplexity 3.03987
Epoch 7, iter 900, train loss 1.135, val perplexity 3.03406
Epoch 7, iter 1000, train loss 1.111, val perplexity 3.03815
Epoch 7, iter 1100, train loss 1.103, val perplexity 3.03587
Epoch 7, iter 1200, train loss 1.067, val perplexity 3.04825
Epoch 7, iter 1300, train loss 1.103, val perplexity 3.04531
Epoch 7, iter 1400, train loss 1.131, val perplexity 3.04883
Epoch 7, iter 1500, train loss 1.119, val perplexity 3.04364
Epoch 7, iter 1600, train loss 1.103, val perplexity 3.04025
Epoch 7, iter 1700, train loss 1.173, val perplexity 3.03740
Epoch 7, iter 1800, train loss 1.104, val perplexity 3.03997
Epoch 7, iter 1900, train loss 1.123, val perplexity 3.03791
Epoch 7, iter 2000, train loss 1.104, val perplexity 3.03748
Epoch 7, iter 2100, train loss 1.137, val perplexity 3.04537
Epoch 7, iter 2200, train loss 1.123, val perplexity 3.04487
Epoch 7, iter 2300, train loss 1.141, val perplexity 3.04375
Epoch 7, iter 2400, train loss 1.126, val perplexity 3.04109
Epoch 7, iter 2500, train loss 1.081, val perplexity 3.03005
Epoch 7, iter 2600, train loss 1.139, val perplexity 3.03136
Epoch 7, iter 2700, train loss 1.136, val perplexity 3.02734
Epoch 7, iter 2800, train loss 1.115, val perplexity 3.03626
Epoch 7, iter 2900, train loss 1.096, val perplexity 3.03452
Epoch 7, iter 3000, train loss 1.105, val perplexity 3.03231
Epoch 8, iter 0, train loss 1.150, val perplexity 3.05440
Epoch 8, iter 100, train loss 1.097, val perplexity 3.04180
Epoch 8, iter 200, train loss 1.159, val perplexity 3.04235
Epoch 8, iter 300, train loss 1.107, val perplexity 3.03960
Epoch 8, iter 400, train loss 1.144, val perplexity 3.03573
Epoch 8, iter 500, train loss 1.104, val perplexity 3.03618
Epoch 8, iter 600, train loss 1.080, val perplexity 3.03417
Epoch 8, iter 700, train loss 1.096, val perplexity 3.03178
Epoch 8, iter 800, train loss 1.085, val perplexity 3.03982
Epoch 8, iter 900, train loss 1.102, val perplexity 3.03049
Epoch 8, iter 1000, train loss 1.103, val perplexity 3.03476
Epoch 8, iter 1100, train loss 1.084, val perplexity 3.05317
Epoch 8, iter 1200, train loss 1.077, val perplexity 3.03353
Epoch 8, iter 1300, train loss 1.107, val perplexity 3.04710
Epoch 8, iter 1400, train loss 1.095, val perplexity 3.03429
Epoch 8, iter 1500, train loss 1.104, val perplexity 3.04726
Epoch 8, iter 1600, train loss 1.165, val perplexity 3.04192
Epoch 8, iter 1700, train loss 1.083, val perplexity 3.03373
Epoch 8, iter 1800, train loss 1.133, val perplexity 3.03319
Epoch 8, iter 1900, train loss 1.124, val perplexity 3.03643
Epoch 8, iter 2000, train loss 1.099, val perplexity 3.03579
Epoch 8, iter 2100, train loss 1.103, val perplexity 3.03267
Epoch 8, iter 2200, train loss 1.150, val perplexity 3.03010
Epoch 8, iter 2300, train loss 1.113, val perplexity 3.03193
Epoch 8, iter 2400, train loss 1.146, val perplexity 3.03401
Epoch 8, iter 2500, train loss 1.109, val perplexity 3.02791
Epoch 8, iter 2600, train loss 1.089, val perplexity 3.03479
Epoch 8, iter 2700, train loss 1.057, val perplexity 3.02521
Epoch 8, iter 2800, train loss 1.090, val perplexity 3.02627
Epoch 8, iter 2900, train loss 1.126, val perplexity 3.02693
Epoch 8, iter 3000, train loss 1.116, val perplexity 3.02471
Epoch 9, iter 0, train loss 1.064, val perplexity 3.05216
Epoch 9, iter 100, train loss 1.084, val perplexity 3.02992
Epoch 9, iter 200, train loss 1.097, val perplexity 3.02944
Epoch 9, iter 300, train loss 1.087, val perplexity 3.02935
Epoch 9, iter 400, train loss 1.119, val perplexity 3.03229
Epoch 9, iter 500, train loss 1.139, val perplexity 3.02652
Epoch 9, iter 600, train loss 1.086, val perplexity 3.02736
Epoch 9, iter 700, train loss 1.081, val perplexity 3.03402
Epoch 9, iter 800, train loss 1.113, val perplexity 3.02297
Epoch 9, iter 900, train loss 1.114, val perplexity 3.04144
Epoch 9, iter 1000, train loss 1.136, val perplexity 3.03763
Epoch 9, iter 1100, train loss 1.106, val perplexity 3.02645
Epoch 9, iter 1200, train loss 1.097, val perplexity 3.02900
Epoch 9, iter 1300, train loss 1.119, val perplexity 3.03568
Epoch 9, iter 1400, train loss 1.116, val perplexity 3.03208
Epoch 9, iter 1500, train loss 1.088, val perplexity 3.02868
Epoch 9, iter 1600, train loss 1.158, val perplexity 3.02877
Epoch 9, iter 1700, train loss 1.136, val perplexity 3.02820
Epoch 9, iter 1800, train loss 1.131, val perplexity 3.03247
Epoch 9, iter 1900, train loss 1.099, val perplexity 3.02304
Epoch 9, iter 2000, train loss 1.083, val perplexity 3.02450
Epoch 9, iter 2100, train loss 1.125, val perplexity 3.02888
Epoch 9, iter 2200, train loss 1.133, val perplexity 3.03586
Epoch 9, iter 2300, train loss 1.103, val perplexity 3.03139
Epoch 9, iter 2400, train loss 1.093, val perplexity 3.02467
Epoch 9, iter 2500, train loss 1.117, val perplexity 3.02839
Epoch 9, iter 2600, train loss 1.145, val perplexity 3.02642
Epoch 9, iter 2700, train loss 1.093, val perplexity 3.02810
Epoch 9, iter 2800, train loss 1.164, val perplexity 3.03437
Epoch 9, iter 2900, train loss 1.081, val perplexity 3.02138
Epoch 9, iter 3000, train loss 1.099, val perplexity 3.03002</code></pre>
</div>
</div>
<div id="cell-93" class="cell" data-execution_count="405">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([[tokenizer.token_to_index(<span class="st">'C'</span>)]])</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> a.to(device)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>generation <span class="op">=</span> model.generate(a, max_new_tokens<span class="op">=</span><span class="dv">30</span>).cpu().numpy()</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>tokenizer.decode(generation[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="405">
<pre><code>'C33O4N4S4=ON[C@@H](S)NN'</code></pre>
</div>
</div>
<p>This is not a good model for generating molecules yet … (even though our validation loss is lower.</p>
</section>
</section>
<section id="interlude-additional-perspectives-on-attention" class="level2">
<h2 class="anchored" data-anchor-id="interlude-additional-perspectives-on-attention">Interlude: Additional perspectives on attention</h2>
<section id="attention-as-gnn" class="level3">
<h3 class="anchored" data-anchor-id="attention-as-gnn">Attention as GNN</h3>
<ul>
<li><p>In the attention mechanism we learn how different tokens “communicate” with each other. If we think of tokens as nodes, attention corresponds to learning the edge weights of a fully connected graph.</p></li>
<li><p>The tokens per default have no notion of their position in the sequence. It is basically the communication between sets of vectors.</p></li>
</ul>
<p>In attentional GNNs, we write for the embeddings:</p>
<p><span class="math display">\[
\mathbf{h}_i=\phi\left(\mathbf{x}_i, \bigoplus_{j \in \mathcal{V}} a\left(\mathbf{x}_i, \mathbf{x}_j\right) \psi\left(\mathbf{x}_j\right)\right)
\]</span></p>
<p>where <span class="math inline">\(\bigoplus\)</span> is a permutation invariant function, e.g., sum or mean over the neighborhood <span class="math inline">\(\mathcal{V}\)</span>. <a href="https://petar-v.com/talks/GNN-EEML.pdf">Does this equation look familiar?</a></p>
<p>You can find more information <a href="https://thegradient.pub/transformers-are-graph-neural-networks/">here</a> and <a href="https://arxiv.org/pdf/2301.08210.pdf">here</a>.</p>
<p>The main difference is that in the transformer we model a fully connected graph, whereas in GNNs we model a sparse graph (which is an inductive bias).</p>
</section>
<section id="attention-as-kernel-smoothing" class="level3">
<h3 class="anchored" data-anchor-id="attention-as-kernel-smoothing">Attention as Kernel smoothing</h3>
<ul>
<li>Given that we have been introducing the attention mechanism as a way to compute a weighted average of values, the analogy to a kernel is quite natural.</li>
</ul>
<p>To understand this a bit better, let us introduce <a href="https://en.wikipedia.org/wiki/Kernel_regression">kernel smoothing</a>. Again, it is nothing else then a weighted average. In this weighted average, the weights are determined by a kernel function.</p>
<p><span class="math display">\[
\sum_{i=1}^n y_i \frac{K\left(x_i, x_o\right)}{\sum_{j=1}^n K\left(x_j, x_o\right)},
\]</span></p>
<p>where <span class="math inline">\((x_1, y_1), \dots, (x_n, y_n)\)</span> are the training points and <span class="math inline">\(x_o\)</span> is the point at which we want to make a prediction.</p>
<p>A common kernel function is the Gaussian kernel:</p>
<p><span class="math display">\[
K(x, x_o) = \exp\left(xx_o\right)
\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is a hyperparameter.</p>
<p>We are also free to add weights</p>
<p><span class="math display">\[
K(x, x_o) = \exp\left(\mathbf{w}_1 x  \mathbf{w}_2 x_o\right)
\]</span></p>
<p>where <span class="math inline">\(w\)</span> are square weight matrices. For stability, we might divide by the dimensionality of <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[
K(x, x_o) = \exp\left(\frac{\mathbf{w}_1 x  \mathbf{w}_2 x_o}{\sqrt{d}}\right)
\]</span></p>
<p>where <span class="math inline">\(d\)</span> is the dimensionality of <span class="math inline">\(x\)</span>.</p>
<p>Compare this to the attention equation:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(d_k\)</span> is the dimension of <span class="math inline">\(K\)</span> and <span class="math inline">\(Q\)</span>.</p>
<p>You can find more information on this perspective <a href="http://bactra.org/notebooks/nn-attention-and-transformers.html">here</a>.</p>
</section>
</section>
<section id="adding-more-expressive-power-with-more-heads-and-fully-connected-layers" class="level2">
<h2 class="anchored" data-anchor-id="adding-more-expressive-power-with-more-heads-and-fully-connected-layers">Adding more expressive power with more heads and fully connected layers</h2>
<p>A very simple way to improve the attention mechanism is to use multiple attention heads. That is we apply the attention mechanism multiple times and then concatenate the results.</p>
<p>The intuition behind this is that different attention heads can learn different attention patterns.</p>
<div id="cell-100" class="cell" data-execution_count="406">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, n_embed, block_size, head_size):</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(n_embed, block_size, head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T, C)</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we want to compute the attention for each head</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and then concatenate the results</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we will have a tensor of shape (B, T, num_heads * head_size)</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in practice, we might not concatenate but add another dimension</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to the tensors</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([head(x) <span class="cf">for</span> head <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we let the tokens talk to each other we currently only used one linear layer to map to the outputs. We can expect better performance if we use multiple layers.</p>
<p>One typically uses wide linear layers that can more readily be parallelized than deep linear layers.</p>
<div id="cell-102" class="cell" data-execution_count="407">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForwardLayer(nn.Module):</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embed, hidden):</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(n_embed, hidden),</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),<span class="co"># </span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden, n_embed)</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we put it together, it looks like this:</p>
<div id="cell-104" class="cell" data-execution_count="408">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttentionModel(nn.Module):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, sequence_length<span class="op">=</span><span class="dv">100</span>, head_size<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># read of the logits of the next token from table</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embedding <span class="op">=</span> nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> nn.Embedding(sequence_length, embedding_dim)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(head_size, vocab_size)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(num_heads, embedding_dim, sequence_length, head_size)</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForwardLayer(embedding_dim, <span class="dv">4</span><span class="op">*</span>embedding_dim)</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>        B, T <span class="op">=</span> x.shape</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.token_embedding(x)</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">+=</span> <span class="va">self</span>.positional_embedding(torch.arange(T, device<span class="op">=</span>device))</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.attention(x)</span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y):</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note that that the implementation below is because of how we - for educational purposes - have defined the dataset</span></span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A better way is to have inputs and outputs of the same length (and to not manually code the sliding window   </span></span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(B, C)</span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B)</span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, x, max_new_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we generate max_new_tokens new tokens</span></span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>        new_tokens <span class="op">=</span> []</span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _t <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a>            x_ <span class="op">=</span> x[:, <span class="op">-</span><span class="va">self</span>.sequence_length:]</span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(x_) <span class="co"># (B, T, C)</span></span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># we only care about the last token in Bigram, hence we bow have shape (B, C)</span></span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># we generate probabilities for the next token</span></span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>            <span class="co"># torch.multinomial(probs, num_samples=1) returns a tensor of shape (B, 1) </span></span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a>            <span class="co"># where each element is the index of the sampled token</span></span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a>            new_tokens.append(next_token)</span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, next_token], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb65-47"><a href="#cb65-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb65-48"><a href="#cb65-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-49"><a href="#cb65-49" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="abstracting-transformers-into-blocks" class="level2">
<h2 class="anchored" data-anchor-id="abstracting-transformers-into-blocks">Abstracting transformers into blocks</h2>
<p>It turns out that we can improve the performance by performing the self-attention and feedforward multiple times. For this, it is useful to extract the reusable parts into a block.</p>
<p>However, just making the model deeper can lead to problems with training. To avoid this, we will leverage two tricks: - we will use residual connections: they allow us to “skip” over layers. During optimization, there will be a “shortcut” to between the input and the output of the block. - we will use layer normalization: it allows us to normalize the activations of a layer - we will add dropout: it allows us to randomly drop activations during training. This can be seen as a form of regularization.</p>
<p>We will apply layer norm twice: - once directly on the input - then before we pass the multihead attention output to the feedforward layer</p>
<p>Note that <a href="https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure">there is some debate</a> on where layer norm is optimally placed.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? <br><br>It places the layer normalization between the residual blocks, which doesn't match the code: <a href="https://t.co/z1oMLFpmiZ">https://t.co/z1oMLFpmiZ</a><br><br>PS: This is known as Post-LN Transformer<br><br>1/3 <a href="https://t.co/OOvp4FA8Nz">pic.twitter.com/OOvp4FA8Nz</a>
</p>
— Sebastian Raschka (<span class="citation" data-cites="rasbt">@rasbt</span>) <a href="https://twitter.com/rasbt/status/1655575611979489282?ref_src=twsrc%5Etfw">May 8, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<div id="cell-108" class="cell" data-execution_count="409">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Transformer block: communication followed by computation """</span></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, block_size, n_head):</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># n_embd: embedding dimension, n_head: the number of heads we'd like</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(num_heads<span class="op">=</span>n_head, n_embed<span class="op">=</span>n_embd, block_size<span class="op">=</span>block_size, head_size<span class="op">=</span>head_size)</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedForwardLayer(n_embd, n_embd<span class="op">*</span><span class="dv">4</span>)</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x)) <span class="co"># residual connection</span></span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>An important thing to realize is that a bulk of the parameters is in the <code>FeedForwardLayer</code>.</p>
<div id="cell-110" class="cell" data-execution_count="410">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>block <span class="op">=</span> Block(<span class="dv">128</span>, <span class="dv">100</span>, <span class="dv">4</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>get_num_parameters_per_layer(block)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="410">
<pre><code>{'sa.heads.0.key.weight': 4096,
 'sa.heads.0.query.weight': 4096,
 'sa.heads.0.value.weight': 4096,
 'sa.heads.1.key.weight': 4096,
 'sa.heads.1.query.weight': 4096,
 'sa.heads.1.value.weight': 4096,
 'sa.heads.2.key.weight': 4096,
 'sa.heads.2.query.weight': 4096,
 'sa.heads.2.value.weight': 4096,
 'sa.heads.3.key.weight': 4096,
 'sa.heads.3.query.weight': 4096,
 'sa.heads.3.value.weight': 4096,
 'ffwd.net.0.weight': 65536,
 'ffwd.net.0.bias': 512,
 'ffwd.net.2.weight': 65536,
 'ffwd.net.2.bias': 128,
 'ln1.weight': 128,
 'ln1.bias': 128,
 'ln2.weight': 128,
 'ln2.bias': 128}</code></pre>
</div>
</div>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
I fixed the Transformer diagram :D <a href="https://t.co/qWnOUjZKut">pic.twitter.com/qWnOUjZKut</a>
</p>
— Andrej Karpathy (<span class="citation" data-cites="karpathy">@karpathy</span>) <a href="https://twitter.com/karpathy/status/1658161721251602432?ref_src=twsrc%5Etfw">May 15, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><img src="https://pbs.twimg.com/media/FwL5ROUagAIrsJT?format=jpg&amp;name=medium.png" class="img-fluid"></p>
<p>With all these “tricks” and enhancements of expressivity, we can now build a full GPT.</p>
<div id="cell-113" class="cell" data-execution_count="411">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, n_embd, block_size, n_head, n_blocks):</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tok_emb <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_emb <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, block_size, n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_blocks)])</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(n_embd, vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_size <span class="op">=</span> block_size</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>        B, T <span class="op">=</span> x.shape</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tok_emb(x) <span class="op">+</span> <span class="va">self</span>.pos_emb(torch.arange(T, device<span class="op">=</span>x.device))  <span class="co"># b,tc, batch, time - seqeuence length, embedding dimension</span></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layers(x)</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y):</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note that that the implementation below is because of how we - for educational purposes - have defined the dataset</span></span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># A better way is to have inputs and outputs of the same length (and to not manually code the sliding window</span></span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but to instead use a causal mask)</span></span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(B, C)</span>
<span id="cb69-27"><a href="#cb69-27" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B)</span>
<span id="cb69-28"><a href="#cb69-28" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb69-29"><a href="#cb69-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb69-30"><a href="#cb69-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-31"><a href="#cb69-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-32"><a href="#cb69-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, x, max_new_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb69-33"><a href="#cb69-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb69-34"><a href="#cb69-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we generate max_new_tokens new tokens</span></span>
<span id="cb69-35"><a href="#cb69-35" aria-hidden="true" tabindex="-1"></a>        new_tokens <span class="op">=</span> []</span>
<span id="cb69-36"><a href="#cb69-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _t <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb69-37"><a href="#cb69-37" aria-hidden="true" tabindex="-1"></a>            x_ <span class="op">=</span> x[:, <span class="op">-</span><span class="va">self</span>.block_size:]</span>
<span id="cb69-38"><a href="#cb69-38" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(x_)</span>
<span id="cb69-39"><a href="#cb69-39" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb69-40"><a href="#cb69-40" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb69-41"><a href="#cb69-41" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb69-42"><a href="#cb69-42" aria-hidden="true" tabindex="-1"></a>            new_tokens.append(next_token)</span>
<span id="cb69-43"><a href="#cb69-43" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, next_token], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb69-44"><a href="#cb69-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-114" class="cell" data-execution_count="412">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>gpt <span class="op">=</span> GPT(<span class="bu">len</span>(tokenizer.tokens), n_embd<span class="op">=</span><span class="dv">64</span>, block_size<span class="op">=</span><span class="dv">40</span>, n_head<span class="op">=</span><span class="dv">4</span>, n_blocks<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-115" class="cell" data-execution_count="413">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>get_num_parameters(gpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="413">
<pre><code>191360</code></pre>
</div>
</div>
<p>That is not nothing (but still a very small model by today’s standards). To increase performance, we can use a larger model, more data, and more training time. For this, we need to use a GPU.</p>
<div id="cell-117" class="cell" data-execution_count="414">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>train_model(gpt, train_loader, valid_loader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, iter 0, train loss 4.452, val perplexity 44.73851
Epoch 0, iter 100, train loss 1.376, val perplexity 3.68381
Epoch 0, iter 200, train loss 1.127, val perplexity 3.00821
Epoch 0, iter 300, train loss 0.981, val perplexity 2.76642
Epoch 0, iter 400, train loss 0.933, val perplexity 2.63615
Epoch 0, iter 500, train loss 0.931, val perplexity 2.55779
Epoch 0, iter 600, train loss 0.935, val perplexity 2.51345
Epoch 0, iter 700, train loss 0.852, val perplexity 2.44716
Epoch 0, iter 800, train loss 0.879, val perplexity 2.39918
Epoch 0, iter 900, train loss 0.904, val perplexity 2.37360
Epoch 0, iter 1000, train loss 0.878, val perplexity 2.34873
Epoch 0, iter 1100, train loss 0.828, val perplexity 2.32402
Epoch 0, iter 1200, train loss 0.835, val perplexity 2.31761
Epoch 0, iter 1300, train loss 0.817, val perplexity 2.29036
Epoch 0, iter 1400, train loss 0.808, val perplexity 2.27935
Epoch 0, iter 1500, train loss 0.795, val perplexity 2.26669
Epoch 0, iter 1600, train loss 0.776, val perplexity 2.24548
Epoch 0, iter 1700, train loss 0.785, val perplexity 2.23922
Epoch 0, iter 1800, train loss 0.814, val perplexity 2.22313
Epoch 0, iter 1900, train loss 0.773, val perplexity 2.22298
Epoch 0, iter 2000, train loss 0.780, val perplexity 2.20131
Epoch 0, iter 2100, train loss 0.810, val perplexity 2.19425
Epoch 0, iter 2200, train loss 0.804, val perplexity 2.18350
Epoch 0, iter 2300, train loss 0.745, val perplexity 2.17148
Epoch 0, iter 2400, train loss 0.745, val perplexity 2.16271
Epoch 0, iter 2500, train loss 0.769, val perplexity 2.17975
Epoch 0, iter 2600, train loss 0.789, val perplexity 2.15805
Epoch 0, iter 2700, train loss 0.726, val perplexity 2.15641
Epoch 0, iter 2800, train loss 0.755, val perplexity 2.14469
Epoch 0, iter 2900, train loss 0.738, val perplexity 2.14893
Epoch 0, iter 3000, train loss 0.742, val perplexity 2.14645
Epoch 1, iter 0, train loss 0.732, val perplexity 2.15031
Epoch 1, iter 100, train loss 0.765, val perplexity 2.13708
Epoch 1, iter 200, train loss 0.788, val perplexity 2.13605
Epoch 1, iter 300, train loss 0.717, val perplexity 2.12883
Epoch 1, iter 400, train loss 0.737, val perplexity 2.12692
Epoch 1, iter 500, train loss 0.722, val perplexity 2.11019
Epoch 1, iter 600, train loss 0.748, val perplexity 2.11692
Epoch 1, iter 700, train loss 0.759, val perplexity 2.14582
Epoch 1, iter 800, train loss 0.759, val perplexity 2.11219
Epoch 1, iter 900, train loss 0.755, val perplexity 2.10373
Epoch 1, iter 1000, train loss 0.776, val perplexity 2.09729
Epoch 1, iter 1100, train loss 0.765, val perplexity 2.09119
Epoch 1, iter 1200, train loss 0.734, val perplexity 2.09802
Epoch 1, iter 1300, train loss 0.753, val perplexity 2.08814
Epoch 1, iter 1400, train loss 0.754, val perplexity 2.09319
Epoch 1, iter 1500, train loss 0.737, val perplexity 2.07947
Epoch 1, iter 1600, train loss 0.738, val perplexity 2.08260
Epoch 1, iter 1700, train loss 0.755, val perplexity 2.07799
Epoch 1, iter 1800, train loss 0.744, val perplexity 2.08093
Epoch 1, iter 1900, train loss 0.747, val perplexity 2.07000
Epoch 1, iter 2000, train loss 0.687, val perplexity 2.07157
Epoch 1, iter 2100, train loss 0.707, val perplexity 2.07065
Epoch 1, iter 2200, train loss 0.717, val perplexity 2.05910
Epoch 1, iter 2300, train loss 0.738, val perplexity 2.05107
Epoch 1, iter 2400, train loss 0.711, val perplexity 2.05451
Epoch 1, iter 2500, train loss 0.675, val perplexity 2.04613
Epoch 1, iter 2600, train loss 0.734, val perplexity 2.05447
Epoch 1, iter 2700, train loss 0.734, val perplexity 2.05046
Epoch 1, iter 2800, train loss 0.730, val perplexity 2.04766
Epoch 1, iter 2900, train loss 0.742, val perplexity 2.04536
Epoch 1, iter 3000, train loss 0.701, val perplexity 2.03406
Epoch 2, iter 0, train loss 0.732, val perplexity 2.03579
Epoch 2, iter 100, train loss 0.695, val perplexity 2.03675
Epoch 2, iter 200, train loss 0.707, val perplexity 2.03463
Epoch 2, iter 300, train loss 0.709, val perplexity 2.03361
Epoch 2, iter 400, train loss 0.733, val perplexity 2.03377
Epoch 2, iter 500, train loss 0.704, val perplexity 2.02371
Epoch 2, iter 600, train loss 0.722, val perplexity 2.02579
Epoch 2, iter 700, train loss 0.715, val perplexity 2.02425
Epoch 2, iter 800, train loss 0.657, val perplexity 2.02351
Epoch 2, iter 900, train loss 0.713, val perplexity 2.02179
Epoch 2, iter 1000, train loss 0.672, val perplexity 2.02233
Epoch 2, iter 1100, train loss 0.687, val perplexity 2.01882
Epoch 2, iter 1200, train loss 0.687, val perplexity 2.02302
Epoch 2, iter 1300, train loss 0.714, val perplexity 2.02380
Epoch 2, iter 1400, train loss 0.694, val perplexity 2.01386
Epoch 2, iter 1500, train loss 0.665, val perplexity 2.02308
Epoch 2, iter 1600, train loss 0.674, val perplexity 2.01054
Epoch 2, iter 1700, train loss 0.678, val perplexity 2.01637
Epoch 2, iter 1800, train loss 0.681, val perplexity 2.00936
Epoch 2, iter 1900, train loss 0.695, val perplexity 2.01404
Epoch 2, iter 2000, train loss 0.717, val perplexity 2.00616
Epoch 2, iter 2100, train loss 0.706, val perplexity 2.00699
Epoch 2, iter 2200, train loss 0.731, val perplexity 2.00446
Epoch 2, iter 2300, train loss 0.700, val perplexity 1.99986
Epoch 2, iter 2400, train loss 0.723, val perplexity 2.00541
Epoch 2, iter 2500, train loss 0.702, val perplexity 2.00087
Epoch 2, iter 2600, train loss 0.688, val perplexity 1.99137
Epoch 2, iter 2700, train loss 0.702, val perplexity 1.99125
Epoch 2, iter 2800, train loss 0.672, val perplexity 2.00703
Epoch 2, iter 2900, train loss 0.720, val perplexity 1.99457
Epoch 2, iter 3000, train loss 0.649, val perplexity 1.99512
Epoch 3, iter 0, train loss 0.669, val perplexity 2.00459
Epoch 3, iter 100, train loss 0.691, val perplexity 1.98455
Epoch 3, iter 200, train loss 0.655, val perplexity 1.98914
Epoch 3, iter 300, train loss 0.680, val perplexity 1.98433
Epoch 3, iter 400, train loss 0.678, val perplexity 1.98273
Epoch 3, iter 500, train loss 0.716, val perplexity 1.98985
Epoch 3, iter 600, train loss 0.701, val perplexity 1.99006
Epoch 3, iter 700, train loss 0.679, val perplexity 1.98366
Epoch 3, iter 800, train loss 0.692, val perplexity 1.98338
Epoch 3, iter 900, train loss 0.693, val perplexity 1.98175
Epoch 3, iter 1000, train loss 0.675, val perplexity 1.97931
Epoch 3, iter 1100, train loss 0.704, val perplexity 1.98130
Epoch 3, iter 1200, train loss 0.668, val perplexity 1.97833
Epoch 3, iter 1300, train loss 0.667, val perplexity 1.97971
Epoch 3, iter 1400, train loss 0.651, val perplexity 1.97418
Epoch 3, iter 1500, train loss 0.720, val perplexity 1.97505
Epoch 3, iter 1600, train loss 0.719, val perplexity 1.97627
Epoch 3, iter 1700, train loss 0.660, val perplexity 1.97169
Epoch 3, iter 1800, train loss 0.699, val perplexity 1.96890
Epoch 3, iter 1900, train loss 0.668, val perplexity 1.96781
Epoch 3, iter 2000, train loss 0.644, val perplexity 1.96777
Epoch 3, iter 2100, train loss 0.717, val perplexity 1.97202
Epoch 3, iter 2200, train loss 0.715, val perplexity 1.96867
Epoch 3, iter 2300, train loss 0.692, val perplexity 1.96386
Epoch 3, iter 2400, train loss 0.680, val perplexity 1.96536
Epoch 3, iter 2500, train loss 0.674, val perplexity 1.96576
Epoch 3, iter 2600, train loss 0.682, val perplexity 1.97131
Epoch 3, iter 2700, train loss 0.674, val perplexity 1.96801
Epoch 3, iter 2800, train loss 0.654, val perplexity 1.96514
Epoch 3, iter 2900, train loss 0.670, val perplexity 1.95693
Epoch 3, iter 3000, train loss 0.684, val perplexity 1.95757
Epoch 4, iter 0, train loss 0.680, val perplexity 1.96651
Epoch 4, iter 100, train loss 0.639, val perplexity 1.96087
Epoch 4, iter 200, train loss 0.670, val perplexity 1.95494
Epoch 4, iter 300, train loss 0.691, val perplexity 1.96136
Epoch 4, iter 400, train loss 0.647, val perplexity 1.95324
Epoch 4, iter 500, train loss 0.680, val perplexity 1.95518
Epoch 4, iter 600, train loss 0.680, val perplexity 1.95562
Epoch 4, iter 700, train loss 0.636, val perplexity 1.95423
Epoch 4, iter 800, train loss 0.641, val perplexity 1.95569
Epoch 4, iter 900, train loss 0.611, val perplexity 1.95869
Epoch 4, iter 1000, train loss 0.680, val perplexity 1.94975
Epoch 4, iter 1100, train loss 0.646, val perplexity 1.95034
Epoch 4, iter 1200, train loss 0.651, val perplexity 1.94704
Epoch 4, iter 1300, train loss 0.650, val perplexity 1.95232
Epoch 4, iter 1400, train loss 0.636, val perplexity 1.95301
Epoch 4, iter 1500, train loss 0.661, val perplexity 1.95471
Epoch 4, iter 1600, train loss 0.657, val perplexity 1.95031
Epoch 4, iter 1700, train loss 0.660, val perplexity 1.94747
Epoch 4, iter 1800, train loss 0.659, val perplexity 1.95406
Epoch 4, iter 1900, train loss 0.654, val perplexity 1.94890
Epoch 4, iter 2000, train loss 0.684, val perplexity 1.95166
Epoch 4, iter 2100, train loss 0.630, val perplexity 1.94946
Epoch 4, iter 2200, train loss 0.675, val perplexity 1.95190
Epoch 4, iter 2300, train loss 0.673, val perplexity 1.94920
Epoch 4, iter 2400, train loss 0.653, val perplexity 1.94797
Epoch 4, iter 2500, train loss 0.636, val perplexity 1.94594
Epoch 4, iter 2600, train loss 0.674, val perplexity 1.94101
Epoch 4, iter 2700, train loss 0.666, val perplexity 1.95357
Epoch 4, iter 2800, train loss 0.688, val perplexity 1.94628
Epoch 4, iter 2900, train loss 0.670, val perplexity 1.94341
Epoch 4, iter 3000, train loss 0.672, val perplexity 1.94509
Epoch 5, iter 0, train loss 0.645, val perplexity 1.94178
Epoch 5, iter 100, train loss 0.621, val perplexity 1.93829
Epoch 5, iter 200, train loss 0.667, val perplexity 1.94284
Epoch 5, iter 300, train loss 0.642, val perplexity 1.94148
Epoch 5, iter 400, train loss 0.633, val perplexity 1.94527
Epoch 5, iter 500, train loss 0.651, val perplexity 1.94243
Epoch 5, iter 600, train loss 0.715, val perplexity 1.93943
Epoch 5, iter 700, train loss 0.632, val perplexity 1.93961
Epoch 5, iter 800, train loss 0.673, val perplexity 1.93994
Epoch 5, iter 900, train loss 0.677, val perplexity 1.93811
Epoch 5, iter 1000, train loss 0.655, val perplexity 1.94041
Epoch 5, iter 1100, train loss 0.673, val perplexity 1.93601
Epoch 5, iter 1200, train loss 0.631, val perplexity 1.93800
Epoch 5, iter 1300, train loss 0.662, val perplexity 1.93886
Epoch 5, iter 1400, train loss 0.647, val perplexity 1.93711
Epoch 5, iter 1500, train loss 0.678, val perplexity 1.94009
Epoch 5, iter 1600, train loss 0.643, val perplexity 1.93476
Epoch 5, iter 1700, train loss 0.622, val perplexity 1.94068
Epoch 5, iter 1800, train loss 0.670, val perplexity 1.93373
Epoch 5, iter 1900, train loss 0.695, val perplexity 1.93784
Epoch 5, iter 2000, train loss 0.641, val perplexity 1.93494
Epoch 5, iter 2100, train loss 0.633, val perplexity 1.93339
Epoch 5, iter 2200, train loss 0.680, val perplexity 1.92561
Epoch 5, iter 2300, train loss 0.680, val perplexity 1.93195
Epoch 5, iter 2400, train loss 0.643, val perplexity 1.93222
Epoch 5, iter 2500, train loss 0.652, val perplexity 1.92935
Epoch 5, iter 2600, train loss 0.705, val perplexity 1.93156
Epoch 5, iter 2700, train loss 0.635, val perplexity 1.92765
Epoch 5, iter 2800, train loss 0.708, val perplexity 1.93618
Epoch 5, iter 2900, train loss 0.671, val perplexity 1.92979
Epoch 5, iter 3000, train loss 0.665, val perplexity 1.93058
Epoch 6, iter 0, train loss 0.650, val perplexity 1.93342
Epoch 6, iter 100, train loss 0.642, val perplexity 1.92895
Epoch 6, iter 200, train loss 0.658, val perplexity 1.92651
Epoch 6, iter 300, train loss 0.642, val perplexity 1.92927
Epoch 6, iter 400, train loss 0.611, val perplexity 1.93185
Epoch 6, iter 500, train loss 0.623, val perplexity 1.92889
Epoch 6, iter 600, train loss 0.634, val perplexity 1.92949
Epoch 6, iter 700, train loss 0.660, val perplexity 1.92113
Epoch 6, iter 800, train loss 0.683, val perplexity 1.92645
Epoch 6, iter 900, train loss 0.647, val perplexity 1.92464
Epoch 6, iter 1000, train loss 0.653, val perplexity 1.92540
Epoch 6, iter 1100, train loss 0.645, val perplexity 1.92497
Epoch 6, iter 1200, train loss 0.635, val perplexity 1.92110
Epoch 6, iter 1300, train loss 0.653, val perplexity 1.92494
Epoch 6, iter 1400, train loss 0.646, val perplexity 1.92566
Epoch 6, iter 1500, train loss 0.641, val perplexity 1.93135
Epoch 6, iter 1600, train loss 0.641, val perplexity 1.92010
Epoch 6, iter 1700, train loss 0.632, val perplexity 1.92073
Epoch 6, iter 1800, train loss 0.674, val perplexity 1.93068
Epoch 6, iter 1900, train loss 0.660, val perplexity 1.92327
Epoch 6, iter 2000, train loss 0.665, val perplexity 1.91660
Epoch 6, iter 2100, train loss 0.652, val perplexity 1.92014
Epoch 6, iter 2200, train loss 0.675, val perplexity 1.92575
Epoch 6, iter 2300, train loss 0.636, val perplexity 1.91780
Epoch 6, iter 2400, train loss 0.643, val perplexity 1.91831
Epoch 6, iter 2500, train loss 0.667, val perplexity 1.92267
Epoch 6, iter 2600, train loss 0.691, val perplexity 1.92060
Epoch 6, iter 2700, train loss 0.651, val perplexity 1.91821
Epoch 6, iter 2800, train loss 0.670, val perplexity 1.91989
Epoch 6, iter 2900, train loss 0.658, val perplexity 1.91619
Epoch 6, iter 3000, train loss 0.636, val perplexity 1.91682
Epoch 7, iter 0, train loss 0.653, val perplexity 1.91773
Epoch 7, iter 100, train loss 0.641, val perplexity 1.91795
Epoch 7, iter 200, train loss 0.633, val perplexity 1.92178
Epoch 7, iter 300, train loss 0.645, val perplexity 1.91800
Epoch 7, iter 400, train loss 0.630, val perplexity 1.91701
Epoch 7, iter 500, train loss 0.634, val perplexity 1.91737
Epoch 7, iter 600, train loss 0.665, val perplexity 1.91566
Epoch 7, iter 700, train loss 0.653, val perplexity 1.91685
Epoch 7, iter 800, train loss 0.610, val perplexity 1.91755
Epoch 7, iter 900, train loss 0.631, val perplexity 1.91505
Epoch 7, iter 1000, train loss 0.617, val perplexity 1.91620
Epoch 7, iter 1100, train loss 0.646, val perplexity 1.91237
Epoch 7, iter 1200, train loss 0.692, val perplexity 1.91239
Epoch 7, iter 1300, train loss 0.647, val perplexity 1.91355
Epoch 7, iter 1400, train loss 0.599, val perplexity 1.91479
Epoch 7, iter 1500, train loss 0.615, val perplexity 1.91264
Epoch 7, iter 1600, train loss 0.646, val perplexity 1.90910
Epoch 7, iter 1700, train loss 0.608, val perplexity 1.91005
Epoch 7, iter 1800, train loss 0.621, val perplexity 1.91320
Epoch 7, iter 1900, train loss 0.649, val perplexity 1.91414
Epoch 7, iter 2000, train loss 0.598, val perplexity 1.91187
Epoch 7, iter 2100, train loss 0.663, val perplexity 1.91032
Epoch 7, iter 2200, train loss 0.653, val perplexity 1.91016
Epoch 7, iter 2300, train loss 0.636, val perplexity 1.91301
Epoch 7, iter 2400, train loss 0.647, val perplexity 1.91055
Epoch 7, iter 2500, train loss 0.636, val perplexity 1.91004
Epoch 7, iter 2600, train loss 0.621, val perplexity 1.91090
Epoch 7, iter 2700, train loss 0.661, val perplexity 1.91047
Epoch 7, iter 2800, train loss 0.610, val perplexity 1.90953
Epoch 7, iter 2900, train loss 0.670, val perplexity 1.91044
Epoch 7, iter 3000, train loss 0.649, val perplexity 1.90597
Epoch 8, iter 0, train loss 0.636, val perplexity 1.90433
Epoch 8, iter 100, train loss 0.623, val perplexity 1.90670
Epoch 8, iter 200, train loss 0.642, val perplexity 1.90896
Epoch 8, iter 300, train loss 0.665, val perplexity 1.90487
Epoch 8, iter 400, train loss 0.613, val perplexity 1.90797
Epoch 8, iter 500, train loss 0.621, val perplexity 1.90619
Epoch 8, iter 600, train loss 0.630, val perplexity 1.91150
Epoch 8, iter 700, train loss 0.669, val perplexity 1.90974
Epoch 8, iter 800, train loss 0.616, val perplexity 1.90695
Epoch 8, iter 900, train loss 0.666, val perplexity 1.90702
Epoch 8, iter 1000, train loss 0.636, val perplexity 1.91129
Epoch 8, iter 1100, train loss 0.658, val perplexity 1.90833
Epoch 8, iter 1200, train loss 0.638, val perplexity 1.90275
Epoch 8, iter 1300, train loss 0.577, val perplexity 1.90394
Epoch 8, iter 1400, train loss 0.671, val perplexity 1.90399
Epoch 8, iter 1500, train loss 0.662, val perplexity 1.90220
Epoch 8, iter 1600, train loss 0.662, val perplexity 1.90444
Epoch 8, iter 1700, train loss 0.637, val perplexity 1.90090
Epoch 8, iter 1800, train loss 0.612, val perplexity 1.90160
Epoch 8, iter 1900, train loss 0.663, val perplexity 1.90664
Epoch 8, iter 2000, train loss 0.638, val perplexity 1.90436
Epoch 8, iter 2100, train loss 0.665, val perplexity 1.90422
Epoch 8, iter 2200, train loss 0.617, val perplexity 1.90150
Epoch 8, iter 2300, train loss 0.614, val perplexity 1.90442
Epoch 8, iter 2400, train loss 0.612, val perplexity 1.90103
Epoch 8, iter 2500, train loss 0.643, val perplexity 1.90159
Epoch 8, iter 2600, train loss 0.602, val perplexity 1.90268
Epoch 8, iter 2700, train loss 0.648, val perplexity 1.89956
Epoch 8, iter 2800, train loss 0.622, val perplexity 1.90088
Epoch 8, iter 2900, train loss 0.654, val perplexity 1.90402
Epoch 8, iter 3000, train loss 0.649, val perplexity 1.90302
Epoch 9, iter 0, train loss 0.630, val perplexity 1.90364
Epoch 9, iter 100, train loss 0.608, val perplexity 1.90211
Epoch 9, iter 200, train loss 0.627, val perplexity 1.90158
Epoch 9, iter 300, train loss 0.626, val perplexity 1.90254
Epoch 9, iter 400, train loss 0.613, val perplexity 1.90116
Epoch 9, iter 500, train loss 0.669, val perplexity 1.90234
Epoch 9, iter 600, train loss 0.574, val perplexity 1.90086
Epoch 9, iter 700, train loss 0.604, val perplexity 1.90457
Epoch 9, iter 800, train loss 0.630, val perplexity 1.89663
Epoch 9, iter 900, train loss 0.664, val perplexity 1.89946
Epoch 9, iter 1000, train loss 0.613, val perplexity 1.89861
Epoch 9, iter 1100, train loss 0.603, val perplexity 1.90102
Epoch 9, iter 1200, train loss 0.644, val perplexity 1.89922
Epoch 9, iter 1300, train loss 0.638, val perplexity 1.89704
Epoch 9, iter 1400, train loss 0.623, val perplexity 1.89902
Epoch 9, iter 1500, train loss 0.622, val perplexity 1.89737
Epoch 9, iter 1600, train loss 0.649, val perplexity 1.89919
Epoch 9, iter 1700, train loss 0.644, val perplexity 1.89897
Epoch 9, iter 1800, train loss 0.616, val perplexity 1.89784
Epoch 9, iter 1900, train loss 0.672, val perplexity 1.89690
Epoch 9, iter 2000, train loss 0.681, val perplexity 1.89647
Epoch 9, iter 2100, train loss 0.663, val perplexity 1.90050
Epoch 9, iter 2200, train loss 0.640, val perplexity 1.89355
Epoch 9, iter 2300, train loss 0.641, val perplexity 1.89585
Epoch 9, iter 2400, train loss 0.649, val perplexity 1.89644
Epoch 9, iter 2500, train loss 0.641, val perplexity 1.89561
Epoch 9, iter 2600, train loss 0.674, val perplexity 1.89989
Epoch 9, iter 2700, train loss 0.621, val perplexity 1.89609
Epoch 9, iter 2800, train loss 0.673, val perplexity 1.89582
Epoch 9, iter 2900, train loss 0.606, val perplexity 1.89622
Epoch 9, iter 3000, train loss 0.628, val perplexity 1.89323</code></pre>
</div>
</div>
<div id="cell-118" class="cell" data-execution_count="420">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>generations <span class="op">=</span> []</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.tensor([[tokenizer.token_to_index(<span class="st">'C'</span>)]])</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> a.to(device)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>    generation <span class="op">=</span> gpt.generate(a, max_new_tokens<span class="op">=</span><span class="dv">30</span>).cpu().numpy()</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>    generations.append(tokenizer.decode(generation[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-119" class="cell" data-execution_count="423">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>np.random.choice(generations, <span class="dv">40</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="423">
<pre><code>array(['C1', 'C1', 'C1=C[C@H]1C', 'C1(Cn1', 'C1(COC#N2C[C@@H]2)CC', 'C1(',
       'C1', 'C1', 'C1[Si]Oc2CC', 'C1', 'C1C', 'C1', 'C1C1',
       'C1(/C2[C@H]2[C@@H]1C[C@@H]1[C@@H]1O2C[C@H]1', 'C1', 'C1CC1',
       'C1C', 'C1', 'C1', 'CC2C[C@@H]1C[C@H]2CC[C@H]', 'C1CC1', 'C1',
       'C1(COc[nH]c2', 'CC', 'C1CC1', 'C1C=COC', 'CC',
       'C1[Si]CCC1CN1c2[nH]2CC', 'C1', 'C2)c1C1', 'C1=', 'CCC1', 'CC',
       'C1(/CCO1C1=CCO2)NC[C@@H]c1C', 'CC[C@H](CCO', 'CCC', 'C1CC1',
       'CC2)n1C1', 'CCC1', 'C1(=C2N'], dtype='&lt;U45')</code></pre>
</div>
</div>
<div id="cell-120" class="cell" data-execution_count="424">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'generations.txt'</span>, <span class="st">'w'</span>) <span class="im">as</span> handle:</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> generation <span class="kw">in</span> generations:</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>        handle.write(generation <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Those seem the best we have seen so far!</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>We saw how to build a GPT to generate new SMILES. We generalized a simple bigram model to take into account all past tokens and not just the last one. When we take the tokens into account, we do this by using self-attention, which allows the model to learn the dependencies between tokens.</p>
<p>To further improve the model, we added multiple heads to the self-attention mechanism, which allows the model to learn different dependencies between tokens. Finally, we stacked multiple blocks of self-attention and feedforward layers to create a GPT model.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Much of this discussion (and also the way it is structured, e.g., based on the bigram) is based on the outstanding material created by <a href="https://karpathy.ai/zero-to-hero.html">Andrej Karpathy</a>. In particular, the implementation here follows <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>.</p>
<p>Other useful resources are:</p>
<ul>
<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">Annotated transformer</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Illustrated transformer</a></li>
<li><a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention! Attention?</a></li>
<li><a href="https://bbycroft.net/llm">Interactive attention visualization</a></li>
<li><a href="https://udlbook.github.io/udlbook/">Simon Prince’s book</a> and <a href="https://www.borealisai.com/research-blogs/tutorial-14-transformers-i-introduction/">blog posts</a> have very nice illustrations of the attention mechanism.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kjablonka\.com\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="kjappelbaum/kjappelbaum.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kevin-maik-jablonka/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/kmjablonka">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kjappelbaum">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://mailhide.io/e/o4LeOUlq">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://scholar.google.com/citations?user=R2ntI8IAAAAJ&amp;hl=en">
      <i class="bi bi-mortarboard-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.dev/kjappelbaum/kjappelbaum.github.io/blob/master/blog/posts/building_an_llm/index.ipynb" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>