<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-02-23">
<meta name="description" content="Efficient long-distant errorpropagation">

<title>Kevin’s Homepage - Developing an intuition for backpropagation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-S9W9LVHXJK"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-S9W9LVHXJK', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Kevin’s Homepage - Developing an intuition for backpropagation">
<meta property="og:description" content="<i>Efficient long-distant errorpropagation</i>">
<meta property="og:image" content="">
<meta property="og:site-name" content="Kevin's Homepage">
<meta name="twitter:title" content="Kevin’s Homepage - Developing an intuition for backpropagation">
<meta name="twitter:description" content="<i>Efficient long-distant errorpropagation</i>">
<meta name="twitter:image" content="">
<meta name="twitter:creator" content="@kmjablonka">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand navbar-dark ">
      <div class="navbar-container container-fluid">
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"><i class="bi bi-home" role="img">
</i> 
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../publications.html">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../cv.html">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../research.html">
 <span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../opensource/opensource.html">
 <span class="menu-text">Open Source</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../teaching.html">
 <span class="menu-text">Teaching</span></a>
  </li>  
</ul>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#setting-weights-in-neural-networks" id="toc-setting-weights-in-neural-networks" class="nav-link active" data-scroll-target="#setting-weights-in-neural-networks">Setting weights in neural networks</a>
  <ul class="collapse">
  <li><a href="#option-1-randomly-choose-weights" id="toc-option-1-randomly-choose-weights" class="nav-link" data-scroll-target="#option-1-randomly-choose-weights">Option 1: Randomly choose weights</a></li>
  <li><a href="#option-2-using-numerical-gradients" id="toc-option-2-using-numerical-gradients" class="nav-link" data-scroll-target="#option-2-using-numerical-gradients">Option 2: Using numerical gradients</a></li>
  <li><a href="#option-3-analytical-gradients" id="toc-option-3-analytical-gradients" class="nav-link" data-scroll-target="#option-3-analytical-gradients">Option 3: Analytical gradients</a></li>
  </ul></li>
  <li><a href="#evaluating-analytical-gradients-for-any-function-backpropagation" id="toc-evaluating-analytical-gradients-for-any-function-backpropagation" class="nav-link" data-scroll-target="#evaluating-analytical-gradients-for-any-function-backpropagation">Evaluating analytical gradients for any function: Backpropagation</a>
  <ul class="collapse">
  <li><a href="#calculus-101-rules-for-computing-derivatives" id="toc-calculus-101-rules-for-computing-derivatives" class="nav-link" data-scroll-target="#calculus-101-rules-for-computing-derivatives">Calculus 101: Rules for computing derivatives</a></li>
  <li><a href="#computing-derivatives-as-in-calculus-101" id="toc-computing-derivatives-as-in-calculus-101" class="nav-link" data-scroll-target="#computing-derivatives-as-in-calculus-101">Computing derivatives as in calculus 101</a></li>
  <li><a href="#making-it-efficient-with-caching" id="toc-making-it-efficient-with-caching" class="nav-link" data-scroll-target="#making-it-efficient-with-caching">Making it efficient with caching</a></li>
  <li><a href="#application-to-neural-networks" id="toc-application-to-neural-networks" class="nav-link" data-scroll-target="#application-to-neural-networks">Application to neural networks</a></li>
  </ul></li>
  <li><a href="#lecture" id="toc-lecture" class="nav-link" data-scroll-target="#lecture">Lecture</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further reading</a>
  <ul class="collapse">
  <li><a href="#who-invented-backpropagation" id="toc-who-invented-backpropagation" class="nav-link" data-scroll-target="#who-invented-backpropagation">Who “invented” backpropagation</a></li>
  <li><a href="#backpropagation-and-lagrangian" id="toc-backpropagation-and-lagrangian" class="nav-link" data-scroll-target="#backpropagation-and-lagrangian">Backpropagation and Lagrangian</a></li>
  <li><a href="#forward-vs.-reverse-mode-autodiff" id="toc-forward-vs.-reverse-mode-autodiff" class="nav-link" data-scroll-target="#forward-vs.-reverse-mode-autodiff">Forward vs.&nbsp;reverse mode autodiff</a></li>
  <li><a href="#symbolic-differentiation-vs.-numerical-differentiation-vs.-autodiff" id="toc-symbolic-differentiation-vs.-numerical-differentiation-vs.-autodiff" class="nav-link" data-scroll-target="#symbolic-differentiation-vs.-numerical-differentiation-vs.-autodiff">Symbolic differentiation vs.&nbsp;numerical differentiation vs.&nbsp;autodiff</a></li>
  <li><a href="#dual-numbers" id="toc-dual-numbers" class="nav-link" data-scroll-target="#dual-numbers">Dual numbers</a></li>
  <li><a href="#differentiating-complex-programs" id="toc-differentiating-complex-programs" class="nav-link" data-scroll-target="#differentiating-complex-programs">Differentiating complex programs</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/kjappelbaum/kjappelbaum.github.io/edit/master/blog/posts/backprop/index.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Developing an intuition for backpropagation</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">machine-learning</div>
  </div>
  </div>

<div>
  <div class="description">
    <i>Efficient long-distant errorpropagation</i>
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 23, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="setting-weights-in-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="setting-weights-in-neural-networks">Setting weights in neural networks</h2>
<p>When we build neural networks, we tune weights to ensure that the outputs are close to what we want them to be.</p>
<p>The power of deep learning is that having many layers of weights allows us to learn very complex functions (i.e.&nbsp;mappings from input to output).</p>
<p>Here, we want to understand how to systematically tune the weights to achieve this.</p>
 <style>
        .flex-container {
            display: flex;
            justify-content: center;
            align-items: start; /* Adjust this as needed */
        }
        .slider-container {
            flex: 2;
            padding: 1px;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }
        .visualization-container {
            flex: 2; /* Gives the visualization more room */
            padding: 1px;
        }
        .slider-label {
            margin-bottom: 10px;
            color: white;
        }
    </style>
  
    <meta charset="UTF-8">
    <title>Neural Network Visualization</title>
    <script src="https://d3js.org/d3.v6.min.js"></script>
    <style>
      .slider-label {
        display: block;
        margin-top: 10px;
      }
      #outputLabel {
        margin-top: 10px;
      }

    </style>
  
  
    
    <div class="flex-container">
        <div class="slider-container">
    <div class="slider-label">
      Input:
      <input type="range" min="0" max="1" step="0.01" value="0.5" id="inputSlider">
    </div>
    <div class="slider-label">
      Weight 1-1:
      <input type="range" min="-1" max="1" step="0.01" value="0.5" id="weight1_1Slider">
    </div>
    <div class="slider-label">
      Weight 1-2:
      <input type="range" min="-1" max="1" step="0.01" value="0.5" id="weight1_2Slider">
    </div>
    <div class="slider-label">
      Weight 2-1:
      <input type="range" min="-1" max="1" step="0.01" value="0.5" id="weight2_1Slider">
    </div>
    <div class="slider-label">
      Weight 2-2:
      <input type="range" min="-1" max="1" step="0.01" value="0.5" id="weight2_2Slider">
    </div>
    <div class="slider-label">
      Target Output:
      <input type="range" min="0" max="1" step="0.01" value="0.5" id="targetOutputSlider">
    </div>
    <bf><div id="outputLabel">Loss: 0.0000</div></bf>
    </div>
    <div class="visualization-container"></div>
    <svg id="networkVisualization" width="600" height="400"></svg>
    </div>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        function relu(x) {
          return Math.max(0, x);
        }

        function forwardPass(inputs, weights1, weights2) {
          let hiddenLayerInput = [inputs * weights1[0], inputs * weights1[1]];
          let hiddenLayerOutput = hiddenLayerInput.map(relu);
          let outputLayerInput =
            hiddenLayerOutput[0] * weights2[0] +
            hiddenLayerOutput[1] * weights2[1];
          return outputLayerInput;
        }

        function computeMSELoss(predicted, target) {
          return Math.pow(predicted - target, 2);
        }
        const colorScale = d3.scaleLinear()
            .domain([-1, 0, 1])
            .range(["blue","red"]);


         function drawNetwork(selector, weights1, weights2, inputs, hiddenActivations, outputActivation) {
        const svg = d3.select(selector);
        svg.selectAll("*").remove(); // Clear previous drawing

        const width = +svg.attr("width");
        const height = +svg.attr("height");

        // Define neuron positions
        const positions = {
            input: [{x: width * 0.2, y: height / 2, value: inputs}],
            hidden: [
                {x: width * 0.5, y: height * 0.3, value: hiddenActivations[0]},
                {x: width * 0.5, y: height * 0.7, value: hiddenActivations[1]}
            ],
            output: [{x: width * 0.8, y: height / 2, value: outputActivation[0]}]
        };

        // Draw connections and labels for weights
        positions.input.forEach((inputPos, i) => {
            positions.hidden.forEach((hiddenPos, j) => {
                svg.append("line")
                    .attr("x1", inputPos.x)
                    .attr("y1", inputPos.y)
                    .attr("x2", hiddenPos.x)
                    .attr("y2", hiddenPos.y)
                    .attr("stroke", colorScale(weights1[j]))
                    .attr("stroke-width", Math.abs(weights1[j]) * 2 + 1);

                // Label for weight
                svg.append("text")
                    .attr("x", (inputPos.x + hiddenPos.x) / 2 -10)
                    .attr("y", (inputPos.y + hiddenPos.y) / 2 - (j === 0 ? 20 : -40))
                    .attr("dy", "-5")
                    .attr("text-anchor", "middle")
                    .attr("fill", "white") 
                    .text(`weight 1-${j+1}: ${weights1[j].toFixed(2)}`);
            });
        });

        positions.hidden.forEach((hiddenPos, i) => {
            svg.append("line")
                .attr("x1", hiddenPos.x)
                .attr("y1", hiddenPos.y)
                .attr("x2", positions.output[0].x)
                .attr("y2", positions.output[0].y)
                .attr("stroke", colorScale(weights2[i]))
                .attr("stroke-width", Math.abs(weights2[i]) * 2 + 1);

            // Label for weight
            svg.append("text")
                .attr("x", (hiddenPos.x + positions.output[0].x) / 2 + 10)
                .attr("y", (hiddenPos.y + positions.output[0].y) / 2 - (i === 0 ? 20 : -40))
                .attr("dy", "-5")
                .attr("text-anchor", "middle")
                .attr("fill", "white")
                .text(`weight 2-${i+1}: ${weights2[i].toFixed(2)}`);
        });

        // Draw neurons and labels for activations
        [...positions.input, ...positions.hidden, ...positions.output].forEach(pos => {
            svg.append("circle")
                .attr("cx", pos.x)
                .attr("cy", pos.y)
                .attr("r", 20)
                .attr("fill", colorScale(pos.value))
                .attr("stroke", "black");

            // Label for neuron value
            svg.append("text")
                .attr("x", pos.x)
                .attr("y", pos.y)
                .attr("dy", "5")
                .attr("text-anchor", "middle")
                .attr("fill", "white")
                .text(pos.value.toFixed(2));
        });
    }

        function updateVisualization() {
          let inputs = parseFloat(document.getElementById("inputSlider").value);
          let weights1 = [
            parseFloat(document.getElementById("weight1_1Slider").value),
            parseFloat(document.getElementById("weight1_2Slider").value),
          ];
          let weights2 = [
            parseFloat(document.getElementById("weight2_1Slider").value),
            parseFloat(document.getElementById("weight2_2Slider").value),
          ];
          let targetOutput = parseFloat(
            document.getElementById("targetOutputSlider").value
          );

          let output = forwardPass(inputs, weights1, weights2);
          let loss = computeMSELoss(output, targetOutput);

          document.getElementById(
            "outputLabel"
          ).innerText = `Loss: ${loss.toFixed(
            4
          )}`;

          drawNetwork(
            "#networkVisualization",
            weights1,
            weights2,
            inputs,
            weights1.map(relu),
            [output]
          );
        }

        document.querySelectorAll("input[type=range]").forEach((slider) => {
          slider.addEventListener("input", updateVisualization);
        });

        updateVisualization(); // Initial visualization
      });
    </script>
  
<p>When we think of the tiny neural network in the widget above one might think of many different ways for optimizing the weights (line strenghts) of this model.</p>
<section id="option-1-randomly-choose-weights" class="level3">
<h3 class="anchored" data-anchor-id="option-1-randomly-choose-weights">Option 1: Randomly choose weights</h3>
<p>One option you might try is to randomly try different weight values to then find one that minimizes the difference between ground truth and prediction (i.e., minimizes the loss). While we might be lucky for this toy example, we can imagine that it might take a long time until we guessed all the weights in a billion-parameter model (e.g.&nbsp;GPT-3) correctly.</p>
<p>Using a strategy like a grid search (in which you loop over a range of possible weight values for all weights) will also only work for small models (think of the <span class="math inline">\(100^4\)</span> combinations you would have to just try of 100 trial values for 4 weights).</p>
</section>
<section id="option-2-using-numerical-gradients" class="level3">
<h3 class="anchored" data-anchor-id="option-2-using-numerical-gradients">Option 2: Using numerical gradients</h3>
<p>When we think of our neural network, the loss forms a landscape, that can be very complex. In our simple example below, it looks as follows:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, x)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear(x):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_pass(inputs, weights1, weights2, record_activation<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    hidden_layer_input <span class="op">=</span> np.dot(inputs, weights1)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    hidden_layer_output <span class="op">=</span> relu(hidden_layer_input)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    output_layer_input <span class="op">=</span> np.dot(hidden_layer_output, weights2)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> linear(output_layer_input)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> record_activation:</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, hidden_layer_output</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_mse_loss(predicted, target):</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span>  np.mean(np.square(predicted <span class="op">-</span> target))</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplify the scenario for clear visualization</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the target output and input</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> <span class="fl">1.9</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>input_val <span class="op">=</span> <span class="fl">0.8</span>  <span class="co"># A simple input value to keep the forward pass straightforward</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a range for weight updates that centers around an expected minimum</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>weight_range <span class="op">=</span> <span class="fl">3.5</span>  <span class="co"># Explore weights within [-2, 2] for both weights</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>num_steps <span class="op">=</span> <span class="dv">100</span>  <span class="co"># Increase the number of steps for finer resolution</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>step_size <span class="op">=</span> weight_range <span class="op">/</span> num_steps</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>weight1_1_range <span class="op">=</span> np.linspace(<span class="dv">0</span>, weight_range, <span class="dv">2</span> <span class="op">*</span> num_steps <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># Start from 0 to weight_range</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>weight2_1_range <span class="op">=</span> np.linspace(<span class="op">-</span>weight_range, weight_range, <span class="dv">2</span> <span class="op">*</span> num_steps <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># Keep full range for weight2_1</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>weight1_1_vals, weight2_1_vals <span class="op">=</span> np.meshgrid(weight1_1_range, weight2_1_range)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>fixed_weight1_2 <span class="op">=</span> <span class="fl">1.2</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>fixed_weight2_2 <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> np.zeros((<span class="bu">len</span>(weight1_1_range), <span class="bu">len</span>(weight2_1_range)))</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Recalculate the losses with the updated range</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weight1_1_range)):</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weight2_1_range)):</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        current_weights1 <span class="op">=</span> np.array([weight1_1_vals[i, j], fixed_weight1_2])</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        current_weights2 <span class="op">=</span> np.array([weight2_1_vals[i, j], fixed_weight2_2])</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> forward_pass(np.array([[input_val]]), current_weights1.reshape(<span class="dv">1</span>, <span class="dv">2</span>), current_weights2.reshape(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        losses[i, j] <span class="op">=</span> compute_mse_loss(output, np.array([[target]]))</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 2D contour plot to visualize the loss landscape</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>heatmap <span class="op">=</span> plt.contourf(weight1_1_vals, weight2_1_vals, losses, levels<span class="op">=</span>np.linspace(losses.<span class="bu">min</span>(), losses.<span class="bu">max</span>(), <span class="dv">50</span>), cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss Landscape'</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$w_1^1$ values'</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$w_2^1$ values'</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="580" height="454"></p>
</div>
</div>
<p>To create this plot, we keep two weights fixed, vary two others and then analyze how the loss looks like. We see that there is a clear structure that might remind us of a hilly landscape.</p>
<p>With the random search we have been randomly jumping around on this landscape. But seeing this image, we might also decide that we want to follow the path downhill; ultimately, our goal is to find the valley (the lowest loss). That is, the best value to try next should not be a random one but one downhill from where we are now.</p>
<p>This direction (“downhill”) is the slope of our hilly landscape, i.e.&nbsp;the gradient.</p>
<p><span class="math display">\[
\frac{\mathrm{d}f(x)}{\mathrm{d}x} = \lim_{h\to0} \frac{f(x+h) - f(x)}{h}
\]</span></p>
<p>Based on the formula above, we might decide to compute a gradient numerically using <a href="https://en.wikipedia.org/wiki/Finite_difference">finite differences</a>.</p>
<p>The problem is that we need to perform <em>many evaluations</em> of the loss to make it work (one per weight, which can be a lot for current frontier models). In addition, we add up errors because <span class="math inline">\(h\)</span> will be different from <span class="math inline">\(0\)</span> (truncation error) and because be have to work with machine precision and hence add rounding errors.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we compute numerical gradients, we have two main sources of error. One stems from the fact that <span class="math inline">\(h\)</span> in the euqation above is not exactly 0. This is known as truncation error. On the other hand, the finite difference equation leads to numberical problems (rounding errors) as two almost identical numbers are substracted and then divided by a very small number.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the function and its exact derivative</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">3</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_exact(x):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">3</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Point at which to evaluate the derivative</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a range of h values (logarithmically spaced to cover small to larger values)</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>h_values <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">16</span>, <span class="dv">0</span>, <span class="dv">400</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>numerical_derivatives <span class="op">=</span> []</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate numerical derivative using forward difference for each h</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> h <span class="kw">in</span> h_values:</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    numerical_derivative <span class="op">=</span> (f(x<span class="op">+</span>h) <span class="op">-</span> f(x)) <span class="op">/</span> h</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    numerical_derivatives.append(numerical_derivative)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate exact derivative</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>exact_derivative <span class="op">=</span> df_exact(x)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate errors</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> np.<span class="bu">abs</span>(exact_derivative <span class="op">-</span> np.array(numerical_derivatives))</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>plt.loglog(h_values, errors, label<span class="op">=</span><span class="st">'Absolute Error'</span>, marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, markersize<span class="op">=</span><span class="dv">4</span>, markevery<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Step size $h$'</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Absolute Error'</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error in Numerical Derivative of $x^3$'</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, which<span class="op">=</span><span class="st">"both"</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="599" height="454"></p>
</div>
</div>
</div>
</div>
</section>
<section id="option-3-analytical-gradients" class="level3">
<h3 class="anchored" data-anchor-id="option-3-analytical-gradients">Option 3: Analytical gradients</h3>
<p>Obviously, we could save many evaluations when we could write down the derviates for a given functions. However, for our neural networks we cannot do this by hand.</p>
<p>The question is thus how we <em>efficiently</em> compute the gradient of function such as a neural network.</p>
</section>
</section>
<section id="evaluating-analytical-gradients-for-any-function-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-analytical-gradients-for-any-function-backpropagation">Evaluating analytical gradients for any function: Backpropagation</h2>
<section id="calculus-101-rules-for-computing-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="calculus-101-rules-for-computing-derivatives">Calculus 101: Rules for computing derivatives</h3>
<p>Let’s assume</p>
<p><span class="math display">\[
f(x,y) = xy
\]</span></p>
<p>then the <em>partial derivates</em> are</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x} = y \quad \frac{\partial f}{\partial y} = x
\]</span></p>
<p>An important rule for differentiation we will need to apply frequently, as it focusses on function composition, is the chain rule</p>
<p><span class="math display">\[
(g(f(x)))^{\prime}=(g \circ f)^{\prime}(x)=g^{\prime}(f(x)) f^{\prime}(x)
\]</span></p>
<p>with <span class="math inline">\(g \circ f\)</span> being function composition <span class="math inline">\(x \to f(x) \to g(f(x))\)</span>.</p>
<p>In the multivariate case, we would write</p>
<p><span class="math display">\[
\frac{\mathrm{d}}{\mathrm{d} t} f(x(t), y(t))=\frac{\partial f}{\partial x} \frac{\mathrm{d} x}{\mathrm{~d} t}+\frac{\partial f}{\partial y} \frac{\mathrm{d} y}{\mathrm{~d} t}.
\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Intuitive understanding of chain rule
</div>
</div>
<div class="callout-body-container callout-body">
<p>How do you intuitively understand that? Let’s borrow from <a href="https://ia802808.us.archive.org/7/items/GeorgeSimmonsCalculusWithAnalyticGeometry1996McGrawHillScienceEngineeringMath/George%20Simmons%20-%20Calculus%20With%20Analytic%20Geometry%20%281996%2C%20McGraw-Hill%20Science_Engineering_Math%29.pdf">George F. Simmons</a>:</p>
<blockquote class="blockquote">
<p>If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.</p>
</blockquote>
<p>With</p>
<ul>
<li><span class="math inline">\(x\)</span> the position of the car</li>
<li><span class="math inline">\(y\)</span> the position of the bicycle</li>
<li><span class="math inline">\(z\)</span> the position of the walking man</li>
</ul>
<p>The rate of change in relative positions is given by terms like <span class="math inline">\(\frac{\mathrm{d}x}{\mathrm{d}y}\)</span>, which gives us the change in relative position of bicycle and car. It we now aim to compute the rate of change of relative position of car to the walking man, <span class="math inline">\(\frac{\mathrm{d}x}{\mathrm{d}z}\)</span>, we find</p>
<p><span class="math display">\[
\frac{\mathrm{d}x}{\mathrm{d}x} = \frac{\mathrm{d}x}{\mathrm{d}y} \frac{\mathrm{d}y}{\mathrm{d}z} = \underbrace{2}_{\text{car twice as fast as bicycle}} \cdot \underbrace{4}_{\text{bicycle is four times as fast as walking man}} = 8
\]</span></p>
</div>
</div>
</section>
<section id="computing-derivatives-as-in-calculus-101" class="level3">
<h3 class="anchored" data-anchor-id="computing-derivatives-as-in-calculus-101">Computing derivatives as in calculus 101</h3>
<p>In neural networks, we nest functions. That is, will end up differentiating compound expression of the form</p>
<p><span class="math display">\[
{\displaystyle h(x)=f(g(x))}
\]</span></p>
<p>For instance, you might look at a simple regularized logistic regression:</p>
<p><span class="math display">\[
L = \frac{1}{2}\left(\sigma(wx +b) -t \right)^2 + \frac{\lambda}{2} w^2,
\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is some activation function (e.g.&nbsp;the sigmoid).</p>
<p>If we now want to know what the influence of the weight <span class="math inline">\(w\)</span> is, we can differentiate the loss with respect to <span class="math inline">\(w\)</span>:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial L}{\partial w} &amp;= \frac{\partial}{\partial w} \left[\frac{1}{2}\left(\sigma(wx +b) -t \right)^2 + \frac{\lambda}{2} w^2 \right] \\
&amp;= \frac{1}{2}\frac{\partial}{\partial w} \left(\sigma(wx +b) -t \right)^2 + \frac{\lambda}{2}\frac{\partial}{\partial w} w^2 \\
&amp;= \left(\sigma(wx+b) - t\right)\frac{\partial}{\partial w}\left(\sigma(wx+b)-t\right) + \lambda w \\
&amp;= \left(\sigma(wx+b) - t\right)\sigma'(wx +b)\frac{\partial}{\partial w}(wx+b) + \lambda w \\
&amp;= \left(\sigma(wx+b) - t\right)\sigma'(wx +b)x + \lambda w
\end{align}
\]</span></p>
<p>Puh! That was a lot of copying and pasting and quite error prone. And it might be quite costly to just directly evaluate such an expression (we might end up with an exponentially large expression, “expression swell”).</p>
<p>There must be a better way.</p>
</section>
<section id="making-it-efficient-with-caching" class="level3">
<h3 class="anchored" data-anchor-id="making-it-efficient-with-caching">Making it efficient with caching</h3>
<p>One thing that we can observe is that we need to do the same computation several times. For instance, <span class="math inline">\(wx +b\)</span> is evaluated two times. We code trade off space and time complexity by caching this using an intermediate variable.</p>
<p>If we do this systematically, we can very efficiently compute gradients – in a form that is symmetric to the computation of the function itself (and those with basically the same cost).</p>
<section id="general-computation-with-intermediate-values" class="level4">
<h4 class="anchored" data-anchor-id="general-computation-with-intermediate-values">General computation with intermediate values</h4>
<p>As a simple example, let’s start with</p>
<p><span class="math display">\[
f(x,y,z) = (x+y)z
\]</span></p>
<p>It can be convienient to introduce the following intermediate variable</p>
<p><span class="math display">\[
p = (x + y)
\]</span></p>
<p>We can then write</p>
<p><span class="math display">\[
f = pz
\]</span></p>
<p>and also compute some partial derivatives</p>
<p><span class="math display">\[
\frac{\partial f}{\partial q} = z \quad \frac{\partial f}{\partial z} = q
\]</span></p>
<p>and we also know how to differentiate <span class="math inline">\(p\)</span> for <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\frac{\partial p}{\partial x} = 1 \quad \frac{\partial p}{\partial y} =1.
\]</span></p>
<p>Using the <em>chain rule</em> we can combine those findings, as the chain rule states that we need to multiply the gradients to chain them:</p>
<p><span class="math display">\[
\frac{\partial f(p,z)}{\partial x} = \frac{\partial f(p, x)}{\partial p}  \frac{\partial p(x,y)}{\partial x}
\]</span></p>
<p>This typically means that two numbers are multiplied.</p>
<p>If we try it for the example above we can use the following code. Note how we <em>cache</em> intermediate results (i.e.&nbsp;trade off time- vs.&nbsp;space-complexity).</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the inputs we will use </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="op">-</span><span class="dv">2</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> <span class="op">-</span><span class="dv">4</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># let's compute our intermediate terms</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> x <span class="op">+</span> y </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> t1 <span class="op">*</span> z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can look at the derivatives we got above</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>dt1dx <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>dt1dy <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>dfdt1 <span class="op">=</span> z</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>dfdz <span class="op">=</span> t1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can use the chain rule to combine them</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dfdx <span class="op">=</span> dfdt1 <span class="op">*</span> dt1dx</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>dfdy <span class="op">=</span> dfdt1 <span class="op">*</span> dt1dy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The sensitivity to <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> is hence</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dfdz, dfdy, dfdz)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3 -4.0 3</code></pre>
</div>
</div>
<p>Before we move ahead, realize what we did:</p>
<p>We computed gradients by recursively applying the chain rule, starting at the end:</p>
<ul>
<li>our computation graph is x -&gt; p -&gt; f</li>
<li>we first compute df/dp, then dp/dx. Chaining them gives us df/dx = df/dp dp/dx</li>
</ul>
<p>We can write this in a more general form as follows.</p>
<p>If we assume we have <span class="math inline">\(N\)</span> intermediate variables <span class="math inline">\(t_N\)</span>, with <span class="math inline">\(t_N\)</span> being our output <span class="math inline">\(f\)</span>, by definition we have</p>
<p><span class="math display">\[
\frac{\mathrm{d}{f}}{\mathrm{d}t_N} = 1
\]</span></p>
<p>For the other intermediate variables we have:</p>
<p><span class="math display">\[
\begin{align}
\frac{\mathrm{d}f}{\mathrm{d} t_{n-1}} &amp;= \frac{\mathrm{d}f}{\mathrm{d}f_{n}} \frac{\mathrm{d}f_{n}}{\mathrm{d}t_{n-1}} \\
\frac{\mathrm{d}f}{\mathrm{d} t_{n-2}} &amp;= \frac{\mathrm{d}f}{\mathrm{d}f_{n}} \frac{\mathrm{d}f_{n}}{\mathrm{d}t_{n-1}} \frac{\mathrm{d}f_{n-1}}{\mathrm{d}t_{n-2}} \\
\frac{\mathrm{d}f}{\mathrm{d} t_{n-3}} &amp;= \frac{\mathrm{d}f}{\mathrm{d}f_{n}} \frac{\mathrm{d}f_{n}}{\mathrm{d}t_{n-1}} \frac{\mathrm{d}f_{n-1}}{\mathrm{d}t_{n-2}} \frac{\mathrm{d}f_{n-2}}{\mathrm{d}t_{n-3}} \\
\frac{\mathrm{d}f}{\mathrm{d} t_i} &amp;= \frac{\mathrm{d}f}{\mathrm{d}f_{n}} \frac{\mathrm{d}f_{n}}{\mathrm{d}t_{n-1}} \frac{\mathrm{d}f_{n-1}}{\mathrm{d}t_{n-2}} \ldots \frac{\mathrm{d}f_{i+1}}{\mathrm{d}t_{i}}
\end{align}
\]</span></p>
<p>Note that many of the terms we computed can be reused.</p>
</section>
</section>
<section id="application-to-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="application-to-neural-networks">Application to neural networks</h3>
<p>Neural networks are more complicated circuits – nested functions.</p>
<p>Let’s assume a very simply case</p>
<p><span class="math display">\[
y=\frac{1}{1+\exp (-(wx+b))}.
\]</span></p>
<p>We can write it using the chaining of the following primitive operations (forming our computation graph).</p>
<p><span class="math display">\[
t_1 = wx
\]</span> <span class="math display">\[
t_2 = t_1 + b
\]</span></p>
<p><span class="math display">\[
t_3 = −t_2
\]</span></p>
<p><span class="math display">\[
t_4 = \exp(t_3)
\]</span></p>
<p><span class="math display">\[
t_5 = 1 + t_4
\]</span></p>
<p><span class="math display">\[
t_6 = 1/t_5
\]</span></p>
<p>(this list of evaluations is sometimes called evaluation trace or Wengert list).</p>
<p>As we would like again get the derivative w.r.t to the output like the loss</p>
<p><span class="math display">\[
L = (t_6-y)^2,
\]</span></p>
<p>which we can write down with some more evaluations</p>
<p><span class="math display">\[
t_7 = t_6-t
\]</span></p>
<p><span class="math display">\[
t_8 = t_7^2.
\]</span></p>
<p>We call this evaluation the <em>forward pass</em>.</p>
<p>The beauty of backprop is that the computation for the derivative follows the same structure as the computation of the function itself (and, for example, is not drastically more complex as one might expect). To see this, we can try out:</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial t_8}{\partial t_8} &amp;= 1 \\
\frac{\partial t_8}{\partial t_7} &amp;= 2 t_7 \\
\frac{\partial t_7}{\partial t_6} &amp; = 1 \\
\frac{\partial t_6}{\partial t_5} &amp;=  -1/t_5^2 \\
\frac{\partial t_5}{\partial t_4} &amp;= 1\\
\frac{\partial t_4}{\partial t_3} &amp;= \exp(t_3) t_3 \\
\frac{\partial t_3}{\partial t_2} &amp;= - 1\\
\frac{\partial t_2}{\partial t_1} &amp;= 1 \\
\frac{\partial t_1}{\partial w} &amp;= x
\end{align}
\]</span></p>
<p>Armed with those partial derivatives, we can now multiply them to get the final goal – the derivative of the loss w.r.t. the weight (<span class="math inline">\(\frac{\partial L}{\partial w}\)</span>).</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial t_8}{\partial t_6} &amp;= \frac{\partial t_8}{\partial t_7} \frac{\partial t_7}{\partial t_6} = 2 t_7 \cdot 1 = 2(t_6 -y) \\
\frac{\partial t_8}{\partial t_5} &amp;= \frac{\partial t_8}{\partial t_6} \frac{\partial t_6}{\partial t_5} = 2(t_6 -y) \cdot  \left(-\frac{1}{t_5^2} \right) =  -2/t_5^2 (t_6 -y) \\
\frac{\partial t_8}{\partial t_4} &amp;= \frac{\partial t_8}{\partial t_5} \frac{\partial t_5}{\partial t_4} = -2/t_5^2 (t_6 -y) \cdot 1 = -2/t_5^2 (t_6 -y) \\
\frac{\partial t_8}{\partial t_3} &amp;= \frac{\partial t_8}{\partial t_4} \frac{\partial t_4}{\partial t_3} = -2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 = -2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 \\
\frac{\partial t_8}{\partial t_2} &amp;= \frac{\partial t_8}{\partial t_3} \frac{\partial t_3}{\partial t_2} = -2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 \cdot -1 = 2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 \\
\frac{\partial t_8}{\partial t_1} &amp;= \frac{\partial t_8}{\partial t_2} \frac{\partial t_2}{\partial t_1} =  2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 \\
\frac{\partial t_8}{\partial w} &amp;= \frac{\partial t_8}{\partial t_1} \frac{\partial t_1}{\partial w} = 2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 \cdot x
\end{align}
\]</span></p>
<p>In practice, we would use autodifferentiation using a datastructure as follows to keep track of the computation graph.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># code taken from https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> graphviz <span class="im">import</span> Digraph</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trace(root):</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    nodes, edges <span class="op">=</span> <span class="bu">set</span>(), <span class="bu">set</span>()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(v):</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> nodes:</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>            nodes.add(v)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>                edges.add((child, v))</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                build(child)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    build(root)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nodes, edges</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_dot(root, <span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>, rankdir<span class="op">=</span><span class="st">'LR'</span>):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co">    format: png | svg | ...</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co">    rankdir: TB (top to bottom graph) | LR (left to right)</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> rankdir <span class="kw">in</span> [<span class="st">'LR'</span>, <span class="st">'TB'</span>]</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    nodes, edges <span class="op">=</span> trace(root)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    dot <span class="op">=</span> Digraph(<span class="bu">format</span><span class="op">=</span><span class="bu">format</span>, graph_attr<span class="op">=</span>{<span class="st">'rankdir'</span>: rankdir}) <span class="co">#, node_attr={'rankdir': 'TB'})</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> nodes:</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        dot.node(name<span class="op">=</span><span class="bu">str</span>(<span class="bu">id</span>(n)), label <span class="op">=</span> <span class="st">"{ data </span><span class="sc">%.4f</span><span class="st"> | grad </span><span class="sc">%.4f</span><span class="st"> }"</span> <span class="op">%</span> (n.data, n.grad), shape<span class="op">=</span><span class="st">'record'</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n._op:</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>            dot.node(name<span class="op">=</span><span class="bu">str</span>(<span class="bu">id</span>(n)) <span class="op">+</span> n._op, label<span class="op">=</span>n._op)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>            dot.edge(<span class="bu">str</span>(<span class="bu">id</span>(n)) <span class="op">+</span> n._op, <span class="bu">str</span>(<span class="bu">id</span>(n)))</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n1, n2 <span class="kw">in</span> edges:</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        dot.edge(<span class="bu">str</span>(<span class="bu">id</span>(n1)), <span class="bu">str</span>(<span class="bu">id</span>(n2)) <span class="op">+</span> n2._op)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># taken from micrograd</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Value:</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" stores a single scalar value and its gradient """</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, _children<span class="op">=</span>(), _op<span class="op">=</span><span class="st">''</span>):</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># internal variables used for autograd graph construction</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._backward <span class="op">=</span> <span class="kw">lambda</span>: <span class="va">None</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._prev <span class="op">=</span> <span class="bu">set</span>(_children)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._op <span class="op">=</span> _op <span class="co"># the op that produced this node, for graphviz / debugging / etc</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">+</span> other.data, (<span class="va">self</span>, other), <span class="st">'+'</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># propagate the gradient on out to parents</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># i.e. self and other </span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># since out = self + other, then d(out)/dself = 1 and d(out)/dother = 1</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so we can just add the gradient to both parents</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">=</span> out.grad</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>            other.grad <span class="op">=</span> out.grad</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other):</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">*</span> other.data, (<span class="va">self</span>, other), <span class="st">'*'</span>)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">=</span> other.data <span class="op">*</span> out.grad</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>            other.grad <span class="op">=</span> <span class="va">self</span>.data <span class="op">*</span> out.grad</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__pow__</span>(<span class="va">self</span>, other):</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">isinstance</span>(other, (<span class="bu">int</span>, <span class="bu">float</span>)), <span class="st">"only supporting int/float powers for now"</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data<span class="op">**</span>other, (<span class="va">self</span>,), <span class="ss">f'**</span><span class="sc">{</span>other<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">=</span> (other <span class="op">*</span> <span class="va">self</span>.data<span class="op">**</span>(other<span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> out.grad</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> exp(<span class="va">self</span>):</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(np.exp(<span class="va">self</span>.data), (<span class="va">self</span>,), <span class="st">'exp'</span>)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">=</span> np.exp(<span class="va">self</span>.data) <span class="op">*</span> out.grad</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__neg__</span>(<span class="va">self</span>): <span class="co"># -self</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__radd__</span>(<span class="va">self</span>, other): <span class="co"># other + self</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">+</span> other</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__sub__</span>(<span class="va">self</span>, other): <span class="co"># self - other</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">+</span> (<span class="op">-</span>other)</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rsub__</span>(<span class="va">self</span>, other): <span class="co"># other - self</span></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> other <span class="op">+</span> (<span class="op">-</span><span class="va">self</span>)</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rmul__</span>(<span class="va">self</span>, other): <span class="co"># other * self</span></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> other</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__truediv__</span>(<span class="va">self</span>, other): <span class="co"># self / other</span></span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> other<span class="op">**-</span><span class="dv">1</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rtruediv__</span>(<span class="va">self</span>, other): <span class="co"># other / self</span></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> other <span class="op">*</span> <span class="va">self</span><span class="op">**-</span><span class="dv">1</span></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"Value(data=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>data<span class="sc">}</span><span class="ss">, grad=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>grad<span class="sc">}</span><span class="ss">)"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now write down our expression from before using the <code>Value</code> class</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize some values</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> Value(<span class="fl">2.0</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Value(<span class="fl">0.0</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># define the input</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Value(<span class="fl">1.0</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> Value(<span class="fl">10.0</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># define the computation</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> w <span class="op">*</span> x</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> t1 <span class="op">+</span> b</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>t3 <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> t2</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>t4 <span class="op">=</span> t3.exp()</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>t5 <span class="op">=</span> t4 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>t6 <span class="op">=</span> t5<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>t7 <span class="op">=</span> t6 <span class="op">-</span> target</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>t8 <span class="op">=</span> t7<span class="op">**</span><span class="dv">2</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>draw_dot(t8)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<p><img src="index_files/figure-html/cell-10-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>We need to seed the gradient of the loss</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>t8.grad <span class="op">=</span> <span class="fl">1.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can perform the backward pass by calling the <code>_backward</code> function of the loss node, which will in turn call the <code>_backward</code> functions of all its parents, and so on, until the entire graph has been visited.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>t8._backward()</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 0 0 0 0 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>t7._backward()</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 0 0 0 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>t6._backward()</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 0 0 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>t5._backward()  </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 14.149418952798422 0 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>t4._backward()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 14.149418952798422 1.9149156216104704 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>t3._backward()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 14.149418952798422 1.9149156216104704 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>t2._backward()</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>w._backward()</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 14.149418952798422 1.9149156216104704 0 -1.9149156216104704 0</code></pre>
</div>
</div>
<p>To avoid calling the backward function multiple times, we can implement a <code>backprop</code> function that traverses the graph in reverse topological order and calls the <code>_backward</code> function of each node only once.</p>
<p>Topological sorting can be implemented using the following code</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>topo <span class="op">=</span> []</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_topo(v):</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        visited.add(v)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>            build_topo(child)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        topo.append(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Why does this sorting algorithm work?
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The algorithm is a depth-first search (DFS)</li>
<li>The deepest nodes are added to the <code>topo</code> list first</li>
<li>Recursiveness ensures that nodes another node depends on are added first (<code>topo.append</code> only happens after the recursive call)</li>
</ul>
<p>Note that this algorithm does not work for cyclic graphs.</p>
</div>
</div>
<p>Now, we can simply write</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize some values</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> Value(<span class="fl">2.0</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Value(<span class="fl">0.0</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># define the input</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Value(<span class="fl">1.0</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> Value(<span class="fl">10.0</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co"># define the computation</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> w <span class="op">*</span> x</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> t1 <span class="op">+</span> b</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>t3 <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> t2</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>t4 <span class="op">=</span> t3.exp()</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>t5 <span class="op">=</span> t4 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>t6 <span class="op">=</span> t5<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>t7 <span class="op">=</span> t6 <span class="op">-</span> target</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>t8 <span class="op">=</span> t7<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And now call the topological sorting and then <code>_backward</code> for all nodes</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>t8.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>build_topo(t8)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v <span class="kw">in</span> <span class="bu">reversed</span>(topo):</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    v._backward()</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>w.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>-1.9149156216104704</code></pre>
</div>
</div>
<p>Note that we had to reverse the topological ordering because the deepest dependent of <code>t8</code> was first and we need to work backwards.</p>
</section>
</section>
<section id="lecture" class="level2">
<h2 class="anchored" data-anchor-id="lecture">Lecture</h2>
<p>If you prefer watching a short video over reading you can see me go through the gist of backprop in the following video.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0;">
<iframe src="https://www.loom.com/embed/579ab50060044464832777e6650180f3?sid=0412526a-5a1e-4e24-ab37-691214804dc5" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
</iframe>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ol type="1">
<li><p><a href="https://karpathy.github.io/neuralnets/">Andrej Karpathy “Hacker’s guide to Neural Networks”</a> inspired the comparison between random search and gradient descent. The same ideas are used in the <a href="https://cs231n.github.io/optimization-1/">cs231n lecture notes</a> since he taught this class. The chain rule example is taken from the <a href="https://cs231n.github.io/optimization-2/">c231n lecture notes</a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=PaCmpygFfXo">Andrej Karparthy recorded a lecture in which he builds an autodiff system from scratch</a> and it inspired many parts of the notebooks, some parts (the <code>Value</code> class) are taken from his lecture.</p></li>
<li><p><a href="https://mml-book.github.io/">Deisenroth et al.&nbsp;“Mathematics of Machine Learning”</a> has a beautiful chapter about backprop and autodiff.</p></li>
<li><p><a href="https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6">Mark Saroufim “Automatic Differentiation Step by Step”</a> has an intuitive explaination of dual numbers and has a good resource section, including <a href="https://www.youtube.com/watch?v=Rs0uRQJdIcg&amp;list=WL&amp;index=8&amp;t=149s"></a></p></li>
<li><p><a href="http://arxiv.org/abs/1502.05767">Automatic Differentiation in Machine Learning: a Survey</a> is a great survey that clarifies many terms.</p></li>
<li><p><a href="http://neuralnetworksanddeeplearning.com/chap2.html">Michael Nielsen’s book</a> highlights some of the “hidden” assumptions.</p></li>
<li><p><a href="https://e2eml.school/how_backpropagation_works">Brandon Rohrer</a> has a very intuitive of the chain rule in terms of the shower rate (similar to the bicycle/car/man example above).</p></li>
<li><p><a href="https://dlsyscourse.org/lectures/">Deep Learning Systems Lecture at CMU</a> has a detailed slides on the algorithmic details behind autodiff.</p></li>
<li><p><a href="https://github.com/MikeInnes/diff-zoo/tree/master/src">Differentiation for Hackers</a> has nice Julia code that showcases what makes autodiff special (and different from symbolic and numeric differentiation).</p></li>
<li><p><a href="https://theoryandpractice.org/stats-ds-book/autodiff-tutorial.html">Kyle Cranmer</a> has a useful intro to autodiff. I took the <code>sympy</code> example from there.</p></li>
</ol>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further reading</h2>
<section id="who-invented-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="who-invented-backpropagation">Who “invented” backpropagation</h3>
<p>As with many popular things, there is some debate on “who was first”. You can find some discussion on this <a href="https://people.idsia.ch/~juergen/who-invented-backpropagation.html#BP1">here</a>.</p>
<section id="original-backprop-paper" class="level4">
<h4 class="anchored" data-anchor-id="original-backprop-paper">“Original” Backprop Paper</h4>
<p>In the context of training neural networks, backpropagation was popularized in a beatiful paper by <a href="https://www.nature.com/articles/323533a0">David E. Rumelhart et al.</a> It is beautiful and you should read it.</p>
</section>
</section>
<section id="backpropagation-and-lagrangian" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-and-lagrangian">Backpropagation and Lagrangian</h3>
<p>As <a href="https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/">this blog post by Tim Viera</a> and <a href="https://arc.net/l/quote/mjznlhvx">this paper by Yann LeCun</a> show, the intermediate variables can be recovered by rephrasing the optimization as a constrained optimization using the Lagrangian framework.</p>
</section>
<section id="forward-vs.-reverse-mode-autodiff" class="level3">
<h3 class="anchored" data-anchor-id="forward-vs.-reverse-mode-autodiff">Forward vs.&nbsp;reverse mode autodiff</h3>
<p>If we have a computation graph as follows</p>
<p><code>x -&gt; a -&gt; b -&gt; y</code></p>
<p>we can compute the derivative of the output with respect to the input as</p>
<p><span class="math display">\[
\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{d}y}{\mathrm{d}b}\frac{\mathrm{d}b}{\mathrm{d}a} \frac{\mathrm{d}a}{\mathrm{d}x}
\]</span></p>
<p>since multiplication is associative, we can choose between computing</p>
<p><span class="math display">\[
\frac{\mathrm{d}y}{\mathrm{d}x} = \left( \frac{\mathrm{d}y}{\mathrm{d}b}\frac{\mathrm{d}b}{\mathrm{d}a} \right) \frac{\mathrm{d}a}{\mathrm{d}x}
\]</span></p>
<p>and <span class="math display">\[
\frac{\mathrm{d}y}{\mathrm{d}x} =  \frac{\mathrm{d}y}{\mathrm{d}b}\left(\frac{\mathrm{d}b}{\mathrm{d}a}  \frac{\mathrm{d}a}{\mathrm{d}x} \right)
\]</span></p>
<p>The first mode is called “reverse mode” autodiff as the gradient flow is opposite to the data flow. The second mode is called “forward mode” autodiff as the order of computation is the same for the gradient computation as for the computation of the function itself.</p>
<p>Backpropagation is a special case of reverse mode autodiff.</p>
<p>Which mode is more efficient depends on whether the input dimension is smaller than the output dimension. If the output dimension is smaller than the input dimension (which is the case for training neural networks) the reverse mode is more efficient as only one application of the reverse mode is needed to compute the gradients.</p>
<p>The forward mode, however is of <span class="math inline">\(\mathcal{O(n)}\)</span>, where <span class="math inline">\(n\)</span> is the number of inputs. If the number of inputs is small (or even just one) and the number of outputs is large, e.g.&nbsp;<span class="math inline">\(\mathbb{R} \to \mathbb{R^m}\)</span>, then the forward mode will be more efficient.</p>
</section>
<section id="symbolic-differentiation-vs.-numerical-differentiation-vs.-autodiff" class="level3">
<h3 class="anchored" data-anchor-id="symbolic-differentiation-vs.-numerical-differentiation-vs.-autodiff">Symbolic differentiation vs.&nbsp;numerical differentiation vs.&nbsp;autodiff</h3>
<ul>
<li>Numerical differentiation involves computing a term like <span class="math inline">\(\frac{\partial f}{\partial x_i} \approx \frac{f(x+h) - f(x)}{h}\)</span> for a small <span class="math inline">\(h\)</span>. While this is might be relatively easy to implement, but requires <span class="math inline">\(\mathcal{O(n)}\)</span> evaluations for <span class="math inline">\(n\)</span> gradients, and can be numerically unstable (dividing by small number, subtracting two numbers of almost the same value).</li>
<li>Symbolic differentation can be performed with systems like Maple, Sympy, or Mathematica. This gives us <em>expressions</em> for the derivatives, which might grow exponentially large (in blind application).</li>
</ul>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sympy.symbols(<span class="st">'x'</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> base_function(x): </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span><span class="dv">3</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Autodiff can easily deal with control flows</li>
</ul>
</section>
<section id="dual-numbers" class="level3">
<h3 class="anchored" data-anchor-id="dual-numbers">Dual numbers</h3>
<p>Dual numbers are numbers of the form <span class="math inline">\(v+\dot{v}\epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> has the special property that it is non-zero and <span class="math inline">\(\epsilon^2 = 0\)</span>.</p>
<p>They behave as one might expect:</p>
<p><span class="math display">\[
(v+\dot{v}\epsilon) + (u + \dot{u}\epsilon) = (v + u) + (\dot{v} + \dot{u})\epsilon
\]</span></p>
<p>and</p>
<p><span class="math display">\[
(v+\dot{v}\epsilon)(u+\dot{u}\epsilon) = (vu) + (v\dot{u} + \dot{u}v)\epsilon
\]</span></p>
<p>Now, keep in mind that the Tyalor series of a function $f(x)</p>
<p><span class="math display">\[
f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!} (x-a)^2 + \frac{f'''(a)}{3!} (x-a)^3
\]</span></p>
<p>Now, if <span class="math inline">\(x = a+\dot{v}\epsilon\)</span></p>
<p><span class="math display">\[
f(a + \dot{v}\epsilon) = f(a) + f'(a)(a + \dot{v}\epsilon -a) +  \frac{f''(a)}{2!} (a + \dot{v}\epsilon -a)^2 + \frac{f'''(a)}{3!} (a + \dot{v}\epsilon -a)^3
\]</span></p>
<p>not that, per definition, all terms with <span class="math inline">\(\epsilon^2\)</span> or higher powers will vanish. Therefore, we will be left with</p>
<p><span class="math display">\[
f(a + \dot{v}\epsilon) = f(a) + f'(a)\dot{v}\epsilon
\]</span></p>
<p>That is, we can do something like</p>
<p><span class="math display">\[
\left. \frac{\mathrm{d}f}{\mathrm{d}x}\right|_{x=a} = \text{epsilon coefficient}(\text{dual version}(f)(a+1\epsilon))
\]</span></p>
<p>This means that we directly compute f(x) and the derivative (scaled by <span class="math inline">\(\dot{v}\)</span>). Thus, we can simulatanously compute the values of functions and derivatives. A naiive implementation might look as follows</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DualNumber:</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, real, dual):</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.real <span class="op">=</span> real  <span class="co"># Real part</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dual <span class="op">=</span> dual  <span class="co"># Dual part (coefficient of epsilon)</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>real<span class="sc">}</span><span class="ss"> + </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>dual<span class="sc">}</span><span class="ss">ε"</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Addition with another DualNumber or scalar</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(other, DualNumber):</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.real <span class="op">+</span> other.real, <span class="va">self</span>.dual <span class="op">+</span> other.dual)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.real <span class="op">+</span> other, <span class="va">self</span>.dual)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other):</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multiplication with another DualNumber or scalar</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(other, DualNumber):</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.real <span class="op">*</span> other.real, <span class="va">self</span>.real <span class="op">*</span> other.dual <span class="op">+</span> <span class="va">self</span>.dual <span class="op">*</span> other.real)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.real <span class="op">*</span> other, <span class="va">self</span>.dual <span class="op">*</span> other)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__radd__</span>(<span class="va">self</span>, other):</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.<span class="fu">__add__</span>(other)</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rmul__</span>(<span class="va">self</span>, other):</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.<span class="fu">__mul__</span>(other)</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> exp(<span class="va">self</span>):</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Exponential function</span></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>        exp_real <span class="op">=</span> math.exp(<span class="va">self</span>.real)</span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> DualNumber(exp_real, exp_real <span class="op">*</span> <span class="va">self</span>.dual)</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> square(<span class="va">self</span>):</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Squaring the dual number</span></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> DualNumber(<span class="va">self</span>.real<span class="op">**</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.real <span class="op">*</span> <span class="va">self</span>.dual)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> complex_function(x):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.square() <span class="op">*</span> x.exp() <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>x</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Correcting the differentiation at x = 1</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> DualNumber(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> complex_function(x)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>result.real, result.dual</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>(5.718281828459045, 11.154845485377136)</code></pre>
</div>
</div>
<p>Which is correct if we check using <a href="https://www.wolframalpha.com/input?i=what+is+the+derivative+of+x%5E2+*+exp(x)+++3x+at+x%3D1">WolframAlpha</a>.</p>
</section>
<section id="differentiating-complex-programs" class="level3">
<h3 class="anchored" data-anchor-id="differentiating-complex-programs">Differentiating complex programs</h3>
<p>Autodiff, and thus differentiable programs, are now becoming a first-class citizen in programming languages—see, for example, the <a href="https://github.com/apple/swift/blob/main/docs/DifferentiableProgramming.md">differentiable programming manifesto</a>.</p>
<p>In the field of computational materials science a few nice examples include</p>
<ul>
<li><a href="https://github.com/jax-md/jax-md">jax-md</a>: Which allows one to differentia through full MD simulations, to do things like <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2024083118">the design of kinetic pathways</a></li>
<li><a href="https://doi.org/10.1063/5.0137103">optimization of a Hückel model implemented in jax</a></li>
<li><a href="https://www.nature.com/articles/s41524-023-01080-x">inverse design of pores</a></li>
</ul>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="kjappelbaum/kjappelbaum.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb34" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> 'Developing an intuition for backpropagation'</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> &lt;i&gt;Efficient long-distant errorpropagation&lt;/i&gt;</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> ""</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="an">sidebar:</span><span class="co"> false</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - machine-learning</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2/23/2024"</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## Setting weights in neural networks </span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>When we build neural networks, we tune weights to ensure that the outputs are close to what we want them to be. </span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>The power of deep learning is that having many layers of weights allows us to learn very complex functions (i.e. mappings from input to output). </span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>Here, we want to understand how to systematically tune the weights to achieve this. </span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="in"> &lt;style&gt;</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="in">        .flex-container {</span></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a><span class="in">            display: flex;</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="in">            justify-content: center;</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="in">            align-items: start; /* Adjust this as needed */</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="in">        .slider-container {</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="in">            flex: 2;</span></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="in">            padding: 1px;</span></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a><span class="in">            display: flex;</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="in">            flex-direction: column;</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a><span class="in">            justify-content: center;</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="in">        .visualization-container {</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a><span class="in">            flex: 2; /* Gives the visualization more room */</span></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a><span class="in">            padding: 1px;</span></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="in">        .slider-label {</span></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a><span class="in">            margin-bottom: 10px;</span></span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a><span class="in">            color: white;</span></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/style&gt;</span></span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;head&gt;</span></span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;meta charset="UTF-8" /&gt;</span></span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;title&gt;Neural Network Visualization&lt;/title&gt;</span></span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;script src="https://d3js.org/d3.v6.min.js"&gt;&lt;/script&gt;</span></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;style&gt;</span></span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a><span class="in">      .slider-label {</span></span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a><span class="in">        display: block;</span></span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a><span class="in">        margin-top: 10px;</span></span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a><span class="in">      }</span></span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a><span class="in">      #outputLabel {</span></span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a><span class="in">        margin-top: 10px;</span></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a><span class="in">      }</span></span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/style&gt;</span></span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/head&gt;</span></span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;body&gt;</span></span>
<span id="cb34-62"><a href="#cb34-62" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb34-63"><a href="#cb34-63" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;div class="flex-container"&gt;</span></span>
<span id="cb34-64"><a href="#cb34-64" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;div class="slider-container"&gt;</span></span>
<span id="cb34-65"><a href="#cb34-65" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;div class="slider-label"&gt;</span></span>
<span id="cb34-66"><a href="#cb34-66" aria-hidden="true" tabindex="-1"></a><span class="in">      Input:</span></span>
<span id="cb34-67"><a href="#cb34-67" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;input</span></span>
<span id="cb34-68"><a href="#cb34-68" aria-hidden="true" tabindex="-1"></a><span class="in">        type="range"</span></span>
<span id="cb34-69"><a href="#cb34-69" aria-hidden="true" tabindex="-1"></a><span class="in">        min="0"</span></span>
<span id="cb34-70"><a href="#cb34-70" aria-hidden="true" tabindex="-1"></a><span class="in">        max="1"</span></span>
<span id="cb34-71"><a href="#cb34-71" aria-hidden="true" tabindex="-1"></a><span class="in">        step="0.01"</span></span>
<span id="cb34-72"><a href="#cb34-72" aria-hidden="true" tabindex="-1"></a><span class="in">        value="0.5"</span></span>
<span id="cb34-73"><a href="#cb34-73" aria-hidden="true" tabindex="-1"></a><span class="in">        id="inputSlider"</span></span>
<span id="cb34-74"><a href="#cb34-74" aria-hidden="true" tabindex="-1"></a><span class="in">      /&gt;</span></span>
<span id="cb34-75"><a href="#cb34-75" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/div&gt;</span></span>
<span id="cb34-76"><a href="#cb34-76" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;div class="slider-label"&gt;</span></span>
<span id="cb34-77"><a href="#cb34-77" aria-hidden="true" tabindex="-1"></a><span class="in">      Weight 1-1:</span></span>
<span id="cb34-78"><a href="#cb34-78" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;input</span></span>
<span id="cb34-79"><a href="#cb34-79" aria-hidden="true" tabindex="-1"></a><span class="in">        type="range"</span></span>
<span id="cb34-80"><a href="#cb34-80" aria-hidden="true" tabindex="-1"></a><span class="in">        min="-1"</span></span>
<span id="cb34-81"><a href="#cb34-81" aria-hidden="true" tabindex="-1"></a><span class="in">        max="1"</span></span>
<span id="cb34-82"><a href="#cb34-82" aria-hidden="true" tabindex="-1"></a><span class="in">        step="0.01"</span></span>
<span id="cb34-83"><a href="#cb34-83" aria-hidden="true" tabindex="-1"></a><span class="in">        value="0.5"</span></span>
<span id="cb34-84"><a href="#cb34-84" aria-hidden="true" tabindex="-1"></a><span class="in">        id="weight1_1Slider"</span></span>
<span id="cb34-85"><a href="#cb34-85" aria-hidden="true" tabindex="-1"></a><span class="in">      /&gt;</span></span>
<span id="cb34-86"><a href="#cb34-86" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/div&gt;</span></span>
<span id="cb34-87"><a href="#cb34-87" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;div class="slider-label"&gt;</span></span>
<span id="cb34-88"><a href="#cb34-88" aria-hidden="true" tabindex="-1"></a><span class="in">      Weight 1-2:</span></span>
<span id="cb34-89"><a href="#cb34-89" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;input</span></span>
<span id="cb34-90"><a href="#cb34-90" aria-hidden="true" tabindex="-1"></a><span class="in">        type="range"</span></span>
<span id="cb34-91"><a href="#cb34-91" aria-hidden="true" tabindex="-1"></a><span class="in">        min="-1"</span></span>
<span id="cb34-92"><a href="#cb34-92" aria-hidden="true" tabindex="-1"></a><span class="in">        max="1"</span></span>
<span id="cb34-93"><a href="#cb34-93" aria-hidden="true" tabindex="-1"></a><span class="in">        step="0.01"</span></span>
<span id="cb34-94"><a href="#cb34-94" aria-hidden="true" tabindex="-1"></a><span class="in">        value="0.5"</span></span>
<span id="cb34-95"><a href="#cb34-95" aria-hidden="true" tabindex="-1"></a><span class="in">        id="weight1_2Slider"</span></span>
<span id="cb34-96"><a href="#cb34-96" aria-hidden="true" tabindex="-1"></a><span class="in">      /&gt;</span></span>
<span id="cb34-97"><a href="#cb34-97" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/div&gt;</span></span>
<span id="cb34-98"><a href="#cb34-98" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;div class="slider-label"&gt;</span></span>
<span id="cb34-99"><a href="#cb34-99" aria-hidden="true" tabindex="-1"></a><span class="in">      Weight 2-1:</span></span>
<span id="cb34-100"><a href="#cb34-100" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;input</span></span>
<span id="cb34-101"><a href="#cb34-101" aria-hidden="true" tabindex="-1"></a><span class="in">        type="range"</span></span>
<span id="cb34-102"><a href="#cb34-102" aria-hidden="true" tabindex="-1"></a><span class="in">        min="-1"</span></span>
<span id="cb34-103"><a href="#cb34-103" aria-hidden="true" tabindex="-1"></a><span class="in">        max="1"</span></span>
<span id="cb34-104"><a href="#cb34-104" aria-hidden="true" tabindex="-1"></a><span class="in">        step="0.01"</span></span>
<span id="cb34-105"><a href="#cb34-105" aria-hidden="true" tabindex="-1"></a><span class="in">        value="0.5"</span></span>
<span id="cb34-106"><a href="#cb34-106" aria-hidden="true" tabindex="-1"></a><span class="in">        id="weight2_1Slider"</span></span>
<span id="cb34-107"><a href="#cb34-107" aria-hidden="true" tabindex="-1"></a><span class="in">      /&gt;</span></span>
<span id="cb34-108"><a href="#cb34-108" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/div&gt;</span></span>
<span id="cb34-109"><a href="#cb34-109" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;div class="slider-label"&gt;</span></span>
<span id="cb34-110"><a href="#cb34-110" aria-hidden="true" tabindex="-1"></a><span class="in">      Weight 2-2:</span></span>
<span id="cb34-111"><a href="#cb34-111" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;input</span></span>
<span id="cb34-112"><a href="#cb34-112" aria-hidden="true" tabindex="-1"></a><span class="in">        type="range"</span></span>
<span id="cb34-113"><a href="#cb34-113" aria-hidden="true" tabindex="-1"></a><span class="in">        min="-1"</span></span>
<span id="cb34-114"><a href="#cb34-114" aria-hidden="true" tabindex="-1"></a><span class="in">        max="1"</span></span>
<span id="cb34-115"><a href="#cb34-115" aria-hidden="true" tabindex="-1"></a><span class="in">        step="0.01"</span></span>
<span id="cb34-116"><a href="#cb34-116" aria-hidden="true" tabindex="-1"></a><span class="in">        value="0.5"</span></span>
<span id="cb34-117"><a href="#cb34-117" aria-hidden="true" tabindex="-1"></a><span class="in">        id="weight2_2Slider"</span></span>
<span id="cb34-118"><a href="#cb34-118" aria-hidden="true" tabindex="-1"></a><span class="in">      /&gt;</span></span>
<span id="cb34-119"><a href="#cb34-119" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/div&gt;</span></span>
<span id="cb34-120"><a href="#cb34-120" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;div class="slider-label"&gt;</span></span>
<span id="cb34-121"><a href="#cb34-121" aria-hidden="true" tabindex="-1"></a><span class="in">      Target Output:</span></span>
<span id="cb34-122"><a href="#cb34-122" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;input</span></span>
<span id="cb34-123"><a href="#cb34-123" aria-hidden="true" tabindex="-1"></a><span class="in">        type="range"</span></span>
<span id="cb34-124"><a href="#cb34-124" aria-hidden="true" tabindex="-1"></a><span class="in">        min="0"</span></span>
<span id="cb34-125"><a href="#cb34-125" aria-hidden="true" tabindex="-1"></a><span class="in">        max="1"</span></span>
<span id="cb34-126"><a href="#cb34-126" aria-hidden="true" tabindex="-1"></a><span class="in">        step="0.01"</span></span>
<span id="cb34-127"><a href="#cb34-127" aria-hidden="true" tabindex="-1"></a><span class="in">        value="0.5"</span></span>
<span id="cb34-128"><a href="#cb34-128" aria-hidden="true" tabindex="-1"></a><span class="in">        id="targetOutputSlider"</span></span>
<span id="cb34-129"><a href="#cb34-129" aria-hidden="true" tabindex="-1"></a><span class="in">      /&gt;</span></span>
<span id="cb34-130"><a href="#cb34-130" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/div&gt;</span></span>
<span id="cb34-131"><a href="#cb34-131" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;bf&gt;&lt;div id="outputLabel"&gt;Loss: 0.0000&lt;/div&gt;&lt;/bf&gt;</span></span>
<span id="cb34-132"><a href="#cb34-132" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/div&gt;</span></span>
<span id="cb34-133"><a href="#cb34-133" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;div class="visualization-container"&gt;&lt;/div&gt;</span></span>
<span id="cb34-134"><a href="#cb34-134" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;svg id="networkVisualization" width="600" height="400"&gt;&lt;/svg&gt;</span></span>
<span id="cb34-135"><a href="#cb34-135" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/div&gt;</span></span>
<span id="cb34-136"><a href="#cb34-136" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;script&gt;</span></span>
<span id="cb34-137"><a href="#cb34-137" aria-hidden="true" tabindex="-1"></a><span class="in">      document.addEventListener("DOMContentLoaded", function () {</span></span>
<span id="cb34-138"><a href="#cb34-138" aria-hidden="true" tabindex="-1"></a><span class="in">        function relu(x) {</span></span>
<span id="cb34-139"><a href="#cb34-139" aria-hidden="true" tabindex="-1"></a><span class="in">          return Math.max(0, x);</span></span>
<span id="cb34-140"><a href="#cb34-140" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb34-141"><a href="#cb34-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-142"><a href="#cb34-142" aria-hidden="true" tabindex="-1"></a><span class="in">        function forwardPass(inputs, weights1, weights2) {</span></span>
<span id="cb34-143"><a href="#cb34-143" aria-hidden="true" tabindex="-1"></a><span class="in">          let hiddenLayerInput = [inputs * weights1[0], inputs * weights1[1]];</span></span>
<span id="cb34-144"><a href="#cb34-144" aria-hidden="true" tabindex="-1"></a><span class="in">          let hiddenLayerOutput = hiddenLayerInput.map(relu);</span></span>
<span id="cb34-145"><a href="#cb34-145" aria-hidden="true" tabindex="-1"></a><span class="in">          let outputLayerInput =</span></span>
<span id="cb34-146"><a href="#cb34-146" aria-hidden="true" tabindex="-1"></a><span class="in">            hiddenLayerOutput[0] * weights2[0] +</span></span>
<span id="cb34-147"><a href="#cb34-147" aria-hidden="true" tabindex="-1"></a><span class="in">            hiddenLayerOutput[1] * weights2[1];</span></span>
<span id="cb34-148"><a href="#cb34-148" aria-hidden="true" tabindex="-1"></a><span class="in">          return outputLayerInput;</span></span>
<span id="cb34-149"><a href="#cb34-149" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb34-150"><a href="#cb34-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-151"><a href="#cb34-151" aria-hidden="true" tabindex="-1"></a><span class="in">        function computeMSELoss(predicted, target) {</span></span>
<span id="cb34-152"><a href="#cb34-152" aria-hidden="true" tabindex="-1"></a><span class="in">          return Math.pow(predicted - target, 2);</span></span>
<span id="cb34-153"><a href="#cb34-153" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb34-154"><a href="#cb34-154" aria-hidden="true" tabindex="-1"></a><span class="in">        const colorScale = d3.scaleLinear()</span></span>
<span id="cb34-155"><a href="#cb34-155" aria-hidden="true" tabindex="-1"></a><span class="in">            .domain([-1, 0, 1])</span></span>
<span id="cb34-156"><a href="#cb34-156" aria-hidden="true" tabindex="-1"></a><span class="in">            .range(["blue","red"]);</span></span>
<span id="cb34-157"><a href="#cb34-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-158"><a href="#cb34-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-159"><a href="#cb34-159" aria-hidden="true" tabindex="-1"></a><span class="in">         function drawNetwork(selector, weights1, weights2, inputs, hiddenActivations, outputActivation) {</span></span>
<span id="cb34-160"><a href="#cb34-160" aria-hidden="true" tabindex="-1"></a><span class="in">        const svg = d3.select(selector);</span></span>
<span id="cb34-161"><a href="#cb34-161" aria-hidden="true" tabindex="-1"></a><span class="in">        svg.selectAll("*").remove(); // Clear previous drawing</span></span>
<span id="cb34-162"><a href="#cb34-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-163"><a href="#cb34-163" aria-hidden="true" tabindex="-1"></a><span class="in">        const width = +svg.attr("width");</span></span>
<span id="cb34-164"><a href="#cb34-164" aria-hidden="true" tabindex="-1"></a><span class="in">        const height = +svg.attr("height");</span></span>
<span id="cb34-165"><a href="#cb34-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-166"><a href="#cb34-166" aria-hidden="true" tabindex="-1"></a><span class="in">        // Define neuron positions</span></span>
<span id="cb34-167"><a href="#cb34-167" aria-hidden="true" tabindex="-1"></a><span class="in">        const positions = {</span></span>
<span id="cb34-168"><a href="#cb34-168" aria-hidden="true" tabindex="-1"></a><span class="in">            input: [{x: width * 0.2, y: height / 2, value: inputs}],</span></span>
<span id="cb34-169"><a href="#cb34-169" aria-hidden="true" tabindex="-1"></a><span class="in">            hidden: [</span></span>
<span id="cb34-170"><a href="#cb34-170" aria-hidden="true" tabindex="-1"></a><span class="in">                {x: width * 0.5, y: height * 0.3, value: hiddenActivations[0]},</span></span>
<span id="cb34-171"><a href="#cb34-171" aria-hidden="true" tabindex="-1"></a><span class="in">                {x: width * 0.5, y: height * 0.7, value: hiddenActivations[1]}</span></span>
<span id="cb34-172"><a href="#cb34-172" aria-hidden="true" tabindex="-1"></a><span class="in">            ],</span></span>
<span id="cb34-173"><a href="#cb34-173" aria-hidden="true" tabindex="-1"></a><span class="in">            output: [{x: width * 0.8, y: height / 2, value: outputActivation[0]}]</span></span>
<span id="cb34-174"><a href="#cb34-174" aria-hidden="true" tabindex="-1"></a><span class="in">        };</span></span>
<span id="cb34-175"><a href="#cb34-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-176"><a href="#cb34-176" aria-hidden="true" tabindex="-1"></a><span class="in">        // Draw connections and labels for weights</span></span>
<span id="cb34-177"><a href="#cb34-177" aria-hidden="true" tabindex="-1"></a><span class="in">        positions.input.forEach((inputPos, i) =&gt; {</span></span>
<span id="cb34-178"><a href="#cb34-178" aria-hidden="true" tabindex="-1"></a><span class="in">            positions.hidden.forEach((hiddenPos, j) =&gt; {</span></span>
<span id="cb34-179"><a href="#cb34-179" aria-hidden="true" tabindex="-1"></a><span class="in">                svg.append("line")</span></span>
<span id="cb34-180"><a href="#cb34-180" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("x1", inputPos.x)</span></span>
<span id="cb34-181"><a href="#cb34-181" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("y1", inputPos.y)</span></span>
<span id="cb34-182"><a href="#cb34-182" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("x2", hiddenPos.x)</span></span>
<span id="cb34-183"><a href="#cb34-183" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("y2", hiddenPos.y)</span></span>
<span id="cb34-184"><a href="#cb34-184" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("stroke", colorScale(weights1[j]))</span></span>
<span id="cb34-185"><a href="#cb34-185" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("stroke-width", Math.abs(weights1[j]) * 2 + 1);</span></span>
<span id="cb34-186"><a href="#cb34-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-187"><a href="#cb34-187" aria-hidden="true" tabindex="-1"></a><span class="in">                // Label for weight</span></span>
<span id="cb34-188"><a href="#cb34-188" aria-hidden="true" tabindex="-1"></a><span class="in">                svg.append("text")</span></span>
<span id="cb34-189"><a href="#cb34-189" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("x", (inputPos.x + hiddenPos.x) / 2 -10)</span></span>
<span id="cb34-190"><a href="#cb34-190" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("y", (inputPos.y + hiddenPos.y) / 2 - (j === 0 ? 20 : -40))</span></span>
<span id="cb34-191"><a href="#cb34-191" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("dy", "-5")</span></span>
<span id="cb34-192"><a href="#cb34-192" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("text-anchor", "middle")</span></span>
<span id="cb34-193"><a href="#cb34-193" aria-hidden="true" tabindex="-1"></a><span class="in">                    .attr("fill", "white") </span></span>
<span id="cb34-194"><a href="#cb34-194" aria-hidden="true" tabindex="-1"></a><span class="in">                    .text(`weight 1-${j+1}: ${weights1[j].toFixed(2)}`);</span></span>
<span id="cb34-195"><a href="#cb34-195" aria-hidden="true" tabindex="-1"></a><span class="in">            });</span></span>
<span id="cb34-196"><a href="#cb34-196" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb34-197"><a href="#cb34-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-198"><a href="#cb34-198" aria-hidden="true" tabindex="-1"></a><span class="in">        positions.hidden.forEach((hiddenPos, i) =&gt; {</span></span>
<span id="cb34-199"><a href="#cb34-199" aria-hidden="true" tabindex="-1"></a><span class="in">            svg.append("line")</span></span>
<span id="cb34-200"><a href="#cb34-200" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("x1", hiddenPos.x)</span></span>
<span id="cb34-201"><a href="#cb34-201" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("y1", hiddenPos.y)</span></span>
<span id="cb34-202"><a href="#cb34-202" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("x2", positions.output[0].x)</span></span>
<span id="cb34-203"><a href="#cb34-203" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("y2", positions.output[0].y)</span></span>
<span id="cb34-204"><a href="#cb34-204" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("stroke", colorScale(weights2[i]))</span></span>
<span id="cb34-205"><a href="#cb34-205" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("stroke-width", Math.abs(weights2[i]) * 2 + 1);</span></span>
<span id="cb34-206"><a href="#cb34-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-207"><a href="#cb34-207" aria-hidden="true" tabindex="-1"></a><span class="in">            // Label for weight</span></span>
<span id="cb34-208"><a href="#cb34-208" aria-hidden="true" tabindex="-1"></a><span class="in">            svg.append("text")</span></span>
<span id="cb34-209"><a href="#cb34-209" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("x", (hiddenPos.x + positions.output[0].x) / 2 + 10)</span></span>
<span id="cb34-210"><a href="#cb34-210" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("y", (hiddenPos.y + positions.output[0].y) / 2 - (i === 0 ? 20 : -40))</span></span>
<span id="cb34-211"><a href="#cb34-211" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("dy", "-5")</span></span>
<span id="cb34-212"><a href="#cb34-212" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("text-anchor", "middle")</span></span>
<span id="cb34-213"><a href="#cb34-213" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("fill", "white")</span></span>
<span id="cb34-214"><a href="#cb34-214" aria-hidden="true" tabindex="-1"></a><span class="in">                .text(`weight 2-${i+1}: ${weights2[i].toFixed(2)}`);</span></span>
<span id="cb34-215"><a href="#cb34-215" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb34-216"><a href="#cb34-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-217"><a href="#cb34-217" aria-hidden="true" tabindex="-1"></a><span class="in">        // Draw neurons and labels for activations</span></span>
<span id="cb34-218"><a href="#cb34-218" aria-hidden="true" tabindex="-1"></a><span class="in">        [...positions.input, ...positions.hidden, ...positions.output].forEach(pos =&gt; {</span></span>
<span id="cb34-219"><a href="#cb34-219" aria-hidden="true" tabindex="-1"></a><span class="in">            svg.append("circle")</span></span>
<span id="cb34-220"><a href="#cb34-220" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("cx", pos.x)</span></span>
<span id="cb34-221"><a href="#cb34-221" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("cy", pos.y)</span></span>
<span id="cb34-222"><a href="#cb34-222" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("r", 20)</span></span>
<span id="cb34-223"><a href="#cb34-223" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("fill", colorScale(pos.value))</span></span>
<span id="cb34-224"><a href="#cb34-224" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("stroke", "black");</span></span>
<span id="cb34-225"><a href="#cb34-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-226"><a href="#cb34-226" aria-hidden="true" tabindex="-1"></a><span class="in">            // Label for neuron value</span></span>
<span id="cb34-227"><a href="#cb34-227" aria-hidden="true" tabindex="-1"></a><span class="in">            svg.append("text")</span></span>
<span id="cb34-228"><a href="#cb34-228" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("x", pos.x)</span></span>
<span id="cb34-229"><a href="#cb34-229" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("y", pos.y)</span></span>
<span id="cb34-230"><a href="#cb34-230" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("dy", "5")</span></span>
<span id="cb34-231"><a href="#cb34-231" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("text-anchor", "middle")</span></span>
<span id="cb34-232"><a href="#cb34-232" aria-hidden="true" tabindex="-1"></a><span class="in">                .attr("fill", "white")</span></span>
<span id="cb34-233"><a href="#cb34-233" aria-hidden="true" tabindex="-1"></a><span class="in">                .text(pos.value.toFixed(2));</span></span>
<span id="cb34-234"><a href="#cb34-234" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb34-235"><a href="#cb34-235" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb34-236"><a href="#cb34-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-237"><a href="#cb34-237" aria-hidden="true" tabindex="-1"></a><span class="in">        function updateVisualization() {</span></span>
<span id="cb34-238"><a href="#cb34-238" aria-hidden="true" tabindex="-1"></a><span class="in">          let inputs = parseFloat(document.getElementById("inputSlider").value);</span></span>
<span id="cb34-239"><a href="#cb34-239" aria-hidden="true" tabindex="-1"></a><span class="in">          let weights1 = [</span></span>
<span id="cb34-240"><a href="#cb34-240" aria-hidden="true" tabindex="-1"></a><span class="in">            parseFloat(document.getElementById("weight1_1Slider").value),</span></span>
<span id="cb34-241"><a href="#cb34-241" aria-hidden="true" tabindex="-1"></a><span class="in">            parseFloat(document.getElementById("weight1_2Slider").value),</span></span>
<span id="cb34-242"><a href="#cb34-242" aria-hidden="true" tabindex="-1"></a><span class="in">          ];</span></span>
<span id="cb34-243"><a href="#cb34-243" aria-hidden="true" tabindex="-1"></a><span class="in">          let weights2 = [</span></span>
<span id="cb34-244"><a href="#cb34-244" aria-hidden="true" tabindex="-1"></a><span class="in">            parseFloat(document.getElementById("weight2_1Slider").value),</span></span>
<span id="cb34-245"><a href="#cb34-245" aria-hidden="true" tabindex="-1"></a><span class="in">            parseFloat(document.getElementById("weight2_2Slider").value),</span></span>
<span id="cb34-246"><a href="#cb34-246" aria-hidden="true" tabindex="-1"></a><span class="in">          ];</span></span>
<span id="cb34-247"><a href="#cb34-247" aria-hidden="true" tabindex="-1"></a><span class="in">          let targetOutput = parseFloat(</span></span>
<span id="cb34-248"><a href="#cb34-248" aria-hidden="true" tabindex="-1"></a><span class="in">            document.getElementById("targetOutputSlider").value</span></span>
<span id="cb34-249"><a href="#cb34-249" aria-hidden="true" tabindex="-1"></a><span class="in">          );</span></span>
<span id="cb34-250"><a href="#cb34-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-251"><a href="#cb34-251" aria-hidden="true" tabindex="-1"></a><span class="in">          let output = forwardPass(inputs, weights1, weights2);</span></span>
<span id="cb34-252"><a href="#cb34-252" aria-hidden="true" tabindex="-1"></a><span class="in">          let loss = computeMSELoss(output, targetOutput);</span></span>
<span id="cb34-253"><a href="#cb34-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-254"><a href="#cb34-254" aria-hidden="true" tabindex="-1"></a><span class="in">          document.getElementById(</span></span>
<span id="cb34-255"><a href="#cb34-255" aria-hidden="true" tabindex="-1"></a><span class="in">            "outputLabel"</span></span>
<span id="cb34-256"><a href="#cb34-256" aria-hidden="true" tabindex="-1"></a><span class="in">          ).innerText = `Loss: ${loss.toFixed(</span></span>
<span id="cb34-257"><a href="#cb34-257" aria-hidden="true" tabindex="-1"></a><span class="in">            4</span></span>
<span id="cb34-258"><a href="#cb34-258" aria-hidden="true" tabindex="-1"></a><span class="in">          )}`;</span></span>
<span id="cb34-259"><a href="#cb34-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-260"><a href="#cb34-260" aria-hidden="true" tabindex="-1"></a><span class="in">          drawNetwork(</span></span>
<span id="cb34-261"><a href="#cb34-261" aria-hidden="true" tabindex="-1"></a><span class="in">            "#networkVisualization",</span></span>
<span id="cb34-262"><a href="#cb34-262" aria-hidden="true" tabindex="-1"></a><span class="in">            weights1,</span></span>
<span id="cb34-263"><a href="#cb34-263" aria-hidden="true" tabindex="-1"></a><span class="in">            weights2,</span></span>
<span id="cb34-264"><a href="#cb34-264" aria-hidden="true" tabindex="-1"></a><span class="in">            inputs,</span></span>
<span id="cb34-265"><a href="#cb34-265" aria-hidden="true" tabindex="-1"></a><span class="in">            weights1.map(relu),</span></span>
<span id="cb34-266"><a href="#cb34-266" aria-hidden="true" tabindex="-1"></a><span class="in">            [output]</span></span>
<span id="cb34-267"><a href="#cb34-267" aria-hidden="true" tabindex="-1"></a><span class="in">          );</span></span>
<span id="cb34-268"><a href="#cb34-268" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb34-269"><a href="#cb34-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-270"><a href="#cb34-270" aria-hidden="true" tabindex="-1"></a><span class="in">        document.querySelectorAll("input[type=range]").forEach((slider) =&gt; {</span></span>
<span id="cb34-271"><a href="#cb34-271" aria-hidden="true" tabindex="-1"></a><span class="in">          slider.addEventListener("input", updateVisualization);</span></span>
<span id="cb34-272"><a href="#cb34-272" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb34-273"><a href="#cb34-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-274"><a href="#cb34-274" aria-hidden="true" tabindex="-1"></a><span class="in">        updateVisualization(); // Initial visualization</span></span>
<span id="cb34-275"><a href="#cb34-275" aria-hidden="true" tabindex="-1"></a><span class="in">      });</span></span>
<span id="cb34-276"><a href="#cb34-276" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/script&gt;</span></span>
<span id="cb34-277"><a href="#cb34-277" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/body&gt;</span></span>
<span id="cb34-278"><a href="#cb34-278" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-279"><a href="#cb34-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-280"><a href="#cb34-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-281"><a href="#cb34-281" aria-hidden="true" tabindex="-1"></a>When we think of the tiny neural network in the widget above one might think of many different ways for optimizing the weights (line strenghts) of this model.</span>
<span id="cb34-282"><a href="#cb34-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-283"><a href="#cb34-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-284"><a href="#cb34-284" aria-hidden="true" tabindex="-1"></a><span class="fu">### Option 1: Randomly choose weights </span></span>
<span id="cb34-285"><a href="#cb34-285" aria-hidden="true" tabindex="-1"></a>One option you might try is to randomly try different weight values to then find one that minimizes the difference between ground truth and prediction (i.e., minimizes the loss). </span>
<span id="cb34-286"><a href="#cb34-286" aria-hidden="true" tabindex="-1"></a>While we might be lucky for this toy example, we can imagine that it might take a long time until we guessed all the weights in a billion-parameter model (e.g. GPT-3) correctly. </span>
<span id="cb34-287"><a href="#cb34-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-288"><a href="#cb34-288" aria-hidden="true" tabindex="-1"></a>Using a strategy like a grid search (in which you loop over a range of possible weight values for all weights) will also only work for small models (think of the $100^4$ combinations you would have to just try of 100 trial values for 4 weights). </span>
<span id="cb34-289"><a href="#cb34-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-290"><a href="#cb34-290" aria-hidden="true" tabindex="-1"></a><span class="fu">### Option 2: Using numerical gradients </span></span>
<span id="cb34-291"><a href="#cb34-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-292"><a href="#cb34-292" aria-hidden="true" tabindex="-1"></a>When we think of our neural network, the loss forms a landscape, that can be very complex. In our simple example below, it looks as follows:</span>
<span id="cb34-293"><a href="#cb34-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-296"><a href="#cb34-296" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-297"><a href="#cb34-297" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-298"><a href="#cb34-298" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb34-299"><a href="#cb34-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-300"><a href="#cb34-300" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb34-301"><a href="#cb34-301" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, x)</span>
<span id="cb34-302"><a href="#cb34-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-303"><a href="#cb34-303" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear(x):</span>
<span id="cb34-304"><a href="#cb34-304" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb34-305"><a href="#cb34-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-306"><a href="#cb34-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-307"><a href="#cb34-307" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_pass(inputs, weights1, weights2, record_activation<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb34-308"><a href="#cb34-308" aria-hidden="true" tabindex="-1"></a>    hidden_layer_input <span class="op">=</span> np.dot(inputs, weights1)</span>
<span id="cb34-309"><a href="#cb34-309" aria-hidden="true" tabindex="-1"></a>    hidden_layer_output <span class="op">=</span> relu(hidden_layer_input)</span>
<span id="cb34-310"><a href="#cb34-310" aria-hidden="true" tabindex="-1"></a>    output_layer_input <span class="op">=</span> np.dot(hidden_layer_output, weights2)</span>
<span id="cb34-311"><a href="#cb34-311" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> linear(output_layer_input)</span>
<span id="cb34-312"><a href="#cb34-312" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> record_activation:</span>
<span id="cb34-313"><a href="#cb34-313" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, hidden_layer_output</span>
<span id="cb34-314"><a href="#cb34-314" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb34-315"><a href="#cb34-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-316"><a href="#cb34-316" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_mse_loss(predicted, target):</span>
<span id="cb34-317"><a href="#cb34-317" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span>  np.mean(np.square(predicted <span class="op">-</span> target))</span>
<span id="cb34-318"><a href="#cb34-318" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span>
<span id="cb34-319"><a href="#cb34-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-320"><a href="#cb34-320" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplify the scenario for clear visualization</span></span>
<span id="cb34-321"><a href="#cb34-321" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the target output and input</span></span>
<span id="cb34-322"><a href="#cb34-322" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> <span class="fl">1.9</span></span>
<span id="cb34-323"><a href="#cb34-323" aria-hidden="true" tabindex="-1"></a>input_val <span class="op">=</span> <span class="fl">0.8</span>  <span class="co"># A simple input value to keep the forward pass straightforward</span></span>
<span id="cb34-324"><a href="#cb34-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-325"><a href="#cb34-325" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a range for weight updates that centers around an expected minimum</span></span>
<span id="cb34-326"><a href="#cb34-326" aria-hidden="true" tabindex="-1"></a>weight_range <span class="op">=</span> <span class="fl">3.5</span>  <span class="co"># Explore weights within [-2, 2] for both weights</span></span>
<span id="cb34-327"><a href="#cb34-327" aria-hidden="true" tabindex="-1"></a>num_steps <span class="op">=</span> <span class="dv">100</span>  <span class="co"># Increase the number of steps for finer resolution</span></span>
<span id="cb34-328"><a href="#cb34-328" aria-hidden="true" tabindex="-1"></a>step_size <span class="op">=</span> weight_range <span class="op">/</span> num_steps</span>
<span id="cb34-329"><a href="#cb34-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-330"><a href="#cb34-330" aria-hidden="true" tabindex="-1"></a>weight1_1_range <span class="op">=</span> np.linspace(<span class="dv">0</span>, weight_range, <span class="dv">2</span> <span class="op">*</span> num_steps <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># Start from 0 to weight_range</span></span>
<span id="cb34-331"><a href="#cb34-331" aria-hidden="true" tabindex="-1"></a>weight2_1_range <span class="op">=</span> np.linspace(<span class="op">-</span>weight_range, weight_range, <span class="dv">2</span> <span class="op">*</span> num_steps <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># Keep full range for weight2_1</span></span>
<span id="cb34-332"><a href="#cb34-332" aria-hidden="true" tabindex="-1"></a>weight1_1_vals, weight2_1_vals <span class="op">=</span> np.meshgrid(weight1_1_range, weight2_1_range)</span>
<span id="cb34-333"><a href="#cb34-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-334"><a href="#cb34-334" aria-hidden="true" tabindex="-1"></a>fixed_weight1_2 <span class="op">=</span> <span class="fl">1.2</span></span>
<span id="cb34-335"><a href="#cb34-335" aria-hidden="true" tabindex="-1"></a>fixed_weight2_2 <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb34-336"><a href="#cb34-336" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> np.zeros((<span class="bu">len</span>(weight1_1_range), <span class="bu">len</span>(weight2_1_range)))</span>
<span id="cb34-337"><a href="#cb34-337" aria-hidden="true" tabindex="-1"></a><span class="co"># Recalculate the losses with the updated range</span></span>
<span id="cb34-338"><a href="#cb34-338" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weight1_1_range)):</span>
<span id="cb34-339"><a href="#cb34-339" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weight2_1_range)):</span>
<span id="cb34-340"><a href="#cb34-340" aria-hidden="true" tabindex="-1"></a>        current_weights1 <span class="op">=</span> np.array([weight1_1_vals[i, j], fixed_weight1_2])</span>
<span id="cb34-341"><a href="#cb34-341" aria-hidden="true" tabindex="-1"></a>        current_weights2 <span class="op">=</span> np.array([weight2_1_vals[i, j], fixed_weight2_2])</span>
<span id="cb34-342"><a href="#cb34-342" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> forward_pass(np.array([[input_val]]), current_weights1.reshape(<span class="dv">1</span>, <span class="dv">2</span>), current_weights2.reshape(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb34-343"><a href="#cb34-343" aria-hidden="true" tabindex="-1"></a>        losses[i, j] <span class="op">=</span> compute_mse_loss(output, np.array([[target]]))</span>
<span id="cb34-344"><a href="#cb34-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-345"><a href="#cb34-345" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 2D contour plot to visualize the loss landscape</span></span>
<span id="cb34-346"><a href="#cb34-346" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb34-347"><a href="#cb34-347" aria-hidden="true" tabindex="-1"></a>heatmap <span class="op">=</span> plt.contourf(weight1_1_vals, weight2_1_vals, losses, levels<span class="op">=</span>np.linspace(losses.<span class="bu">min</span>(), losses.<span class="bu">max</span>(), <span class="dv">50</span>), cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb34-348"><a href="#cb34-348" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb34-349"><a href="#cb34-349" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Loss Landscape'</span>)</span>
<span id="cb34-350"><a href="#cb34-350" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$w_1^1$ values'</span>)</span>
<span id="cb34-351"><a href="#cb34-351" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$w_2^1$ values'</span>)</span>
<span id="cb34-352"><a href="#cb34-352" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb34-353"><a href="#cb34-353" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-354"><a href="#cb34-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-355"><a href="#cb34-355" aria-hidden="true" tabindex="-1"></a>To create this plot, we keep two weights fixed, vary two others and then analyze how the loss looks like.</span>
<span id="cb34-356"><a href="#cb34-356" aria-hidden="true" tabindex="-1"></a>We see that there is a clear structure that might remind us of a hilly landscape. </span>
<span id="cb34-357"><a href="#cb34-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-358"><a href="#cb34-358" aria-hidden="true" tabindex="-1"></a>With the random search we have been randomly jumping around on this landscape. But seeing this image, we might also decide that we want to follow the path downhill; ultimately, our goal is to find the valley (the lowest loss).</span>
<span id="cb34-359"><a href="#cb34-359" aria-hidden="true" tabindex="-1"></a>That is, the best value to try next should not be a random one but one downhill from where we are now.</span>
<span id="cb34-360"><a href="#cb34-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-361"><a href="#cb34-361" aria-hidden="true" tabindex="-1"></a>This direction ("downhill") is the slope of our hilly landscape, i.e. the gradient.</span>
<span id="cb34-362"><a href="#cb34-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-363"><a href="#cb34-363" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-364"><a href="#cb34-364" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}f(x)}{\mathrm{d}x} = \lim_{h\to0} \frac{f(x+h) - f(x)}{h}</span>
<span id="cb34-365"><a href="#cb34-365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-366"><a href="#cb34-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-367"><a href="#cb34-367" aria-hidden="true" tabindex="-1"></a>Based on the formula above, we might decide to compute a gradient numerically using <span class="co">[</span><span class="ot">finite differences</span><span class="co">](https://en.wikipedia.org/wiki/Finite_difference)</span>.</span>
<span id="cb34-368"><a href="#cb34-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-369"><a href="#cb34-369" aria-hidden="true" tabindex="-1"></a>The problem is that we need to perform _many evaluations_ of the loss to make it work (one per weight, which can be a lot for current frontier models). In addition, we add up errors because $h$ will be different from $0$ (truncation error) and because be have to work with machine precision and hence add rounding errors.</span>
<span id="cb34-370"><a href="#cb34-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-371"><a href="#cb34-371" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb34-372"><a href="#cb34-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-373"><a href="#cb34-373" aria-hidden="true" tabindex="-1"></a>If we compute numerical gradients, we have two main sources of error. One stems from the fact that $h$ in the euqation above is not exactly 0. This is known as truncation error. On the other hand, the finite difference equation leads to numberical problems (rounding errors) as two almost identical numbers are substracted and then divided by a very small number. </span>
<span id="cb34-374"><a href="#cb34-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-377"><a href="#cb34-377" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-378"><a href="#cb34-378" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-379"><a href="#cb34-379" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb34-380"><a href="#cb34-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-381"><a href="#cb34-381" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the function and its exact derivative</span></span>
<span id="cb34-382"><a href="#cb34-382" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb34-383"><a href="#cb34-383" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">3</span></span>
<span id="cb34-384"><a href="#cb34-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-385"><a href="#cb34-385" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> df_exact(x):</span>
<span id="cb34-386"><a href="#cb34-386" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">3</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb34-387"><a href="#cb34-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-388"><a href="#cb34-388" aria-hidden="true" tabindex="-1"></a><span class="co"># Point at which to evaluate the derivative</span></span>
<span id="cb34-389"><a href="#cb34-389" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb34-390"><a href="#cb34-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-391"><a href="#cb34-391" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a range of h values (logarithmically spaced to cover small to larger values)</span></span>
<span id="cb34-392"><a href="#cb34-392" aria-hidden="true" tabindex="-1"></a>h_values <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">16</span>, <span class="dv">0</span>, <span class="dv">400</span>)</span>
<span id="cb34-393"><a href="#cb34-393" aria-hidden="true" tabindex="-1"></a>numerical_derivatives <span class="op">=</span> []</span>
<span id="cb34-394"><a href="#cb34-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-395"><a href="#cb34-395" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate numerical derivative using forward difference for each h</span></span>
<span id="cb34-396"><a href="#cb34-396" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> h <span class="kw">in</span> h_values:</span>
<span id="cb34-397"><a href="#cb34-397" aria-hidden="true" tabindex="-1"></a>    numerical_derivative <span class="op">=</span> (f(x<span class="op">+</span>h) <span class="op">-</span> f(x)) <span class="op">/</span> h</span>
<span id="cb34-398"><a href="#cb34-398" aria-hidden="true" tabindex="-1"></a>    numerical_derivatives.append(numerical_derivative)</span>
<span id="cb34-399"><a href="#cb34-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-400"><a href="#cb34-400" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate exact derivative</span></span>
<span id="cb34-401"><a href="#cb34-401" aria-hidden="true" tabindex="-1"></a>exact_derivative <span class="op">=</span> df_exact(x)</span>
<span id="cb34-402"><a href="#cb34-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-403"><a href="#cb34-403" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate errors</span></span>
<span id="cb34-404"><a href="#cb34-404" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> np.<span class="bu">abs</span>(exact_derivative <span class="op">-</span> np.array(numerical_derivatives))</span>
<span id="cb34-405"><a href="#cb34-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-406"><a href="#cb34-406" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb34-407"><a href="#cb34-407" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb34-408"><a href="#cb34-408" aria-hidden="true" tabindex="-1"></a>plt.loglog(h_values, errors, label<span class="op">=</span><span class="st">'Absolute Error'</span>, marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, markersize<span class="op">=</span><span class="dv">4</span>, markevery<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb34-409"><a href="#cb34-409" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Step size $h$'</span>)</span>
<span id="cb34-410"><a href="#cb34-410" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Absolute Error'</span>)</span>
<span id="cb34-411"><a href="#cb34-411" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error in Numerical Derivative of $x^3$'</span>)</span>
<span id="cb34-412"><a href="#cb34-412" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb34-413"><a href="#cb34-413" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, which<span class="op">=</span><span class="st">"both"</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb34-414"><a href="#cb34-414" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb34-415"><a href="#cb34-415" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-416"><a href="#cb34-416" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-417"><a href="#cb34-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-418"><a href="#cb34-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-419"><a href="#cb34-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-420"><a href="#cb34-420" aria-hidden="true" tabindex="-1"></a><span class="fu">### Option 3: Analytical gradients</span></span>
<span id="cb34-421"><a href="#cb34-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-422"><a href="#cb34-422" aria-hidden="true" tabindex="-1"></a>Obviously, we could save many evaluations when we could write down the derviates for a given functions. </span>
<span id="cb34-423"><a href="#cb34-423" aria-hidden="true" tabindex="-1"></a>However, for our neural networks we cannot do this by hand. </span>
<span id="cb34-424"><a href="#cb34-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-425"><a href="#cb34-425" aria-hidden="true" tabindex="-1"></a>The question is thus how we _efficiently_ compute the gradient of function such as a neural network.</span>
<span id="cb34-426"><a href="#cb34-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-427"><a href="#cb34-427" aria-hidden="true" tabindex="-1"></a><span class="fu">## Evaluating analytical gradients for any function: Backpropagation</span></span>
<span id="cb34-428"><a href="#cb34-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-429"><a href="#cb34-429" aria-hidden="true" tabindex="-1"></a><span class="fu">### Calculus 101: Rules for computing derivatives</span></span>
<span id="cb34-430"><a href="#cb34-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-431"><a href="#cb34-431" aria-hidden="true" tabindex="-1"></a>Let's assume </span>
<span id="cb34-432"><a href="#cb34-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-433"><a href="#cb34-433" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-434"><a href="#cb34-434" aria-hidden="true" tabindex="-1"></a>f(x,y) = xy</span>
<span id="cb34-435"><a href="#cb34-435" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-436"><a href="#cb34-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-437"><a href="#cb34-437" aria-hidden="true" tabindex="-1"></a>then the _partial derivates_ are </span>
<span id="cb34-438"><a href="#cb34-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-439"><a href="#cb34-439" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-440"><a href="#cb34-440" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial x} = y \quad \frac{\partial f}{\partial y} = x</span>
<span id="cb34-441"><a href="#cb34-441" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-442"><a href="#cb34-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-443"><a href="#cb34-443" aria-hidden="true" tabindex="-1"></a>An important rule for differentiation we will need to apply frequently, as it focusses on function composition, is the chain rule </span>
<span id="cb34-444"><a href="#cb34-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-445"><a href="#cb34-445" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-446"><a href="#cb34-446" aria-hidden="true" tabindex="-1"></a>(g(f(x)))^{\prime}=(g \circ f)^{\prime}(x)=g^{\prime}(f(x)) f^{\prime}(x)</span>
<span id="cb34-447"><a href="#cb34-447" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-448"><a href="#cb34-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-449"><a href="#cb34-449" aria-hidden="true" tabindex="-1"></a>with $g \circ f$ being function composition $x \to f(x) \to g(f(x))$.</span>
<span id="cb34-450"><a href="#cb34-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-451"><a href="#cb34-451" aria-hidden="true" tabindex="-1"></a>In the multivariate case, we would write</span>
<span id="cb34-452"><a href="#cb34-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-453"><a href="#cb34-453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-454"><a href="#cb34-454" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}}{\mathrm{d} t} f(x(t), y(t))=\frac{\partial f}{\partial x} \frac{\mathrm{d} x}{\mathrm{~d} t}+\frac{\partial f}{\partial y} \frac{\mathrm{d} y}{\mathrm{~d} t}.</span>
<span id="cb34-455"><a href="#cb34-455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-456"><a href="#cb34-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-457"><a href="#cb34-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-458"><a href="#cb34-458" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb34-459"><a href="#cb34-459" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Intuitive understanding of chain rule </span></span>
<span id="cb34-460"><a href="#cb34-460" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-461"><a href="#cb34-461" aria-hidden="true" tabindex="-1"></a>How do you intuitively understand that? Let's borrow from <span class="co">[</span><span class="ot">George F. Simmons</span><span class="co">](https://ia802808.us.archive.org/7/items/GeorgeSimmonsCalculusWithAnalyticGeometry1996McGrawHillScienceEngineeringMath/George%20Simmons%20-%20Calculus%20With%20Analytic%20Geometry%20%281996%2C%20McGraw-Hill%20Science_Engineering_Math%29.pdf)</span>:</span>
<span id="cb34-462"><a href="#cb34-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-463"><a href="#cb34-463" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.</span></span>
<span id="cb34-464"><a href="#cb34-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-465"><a href="#cb34-465" aria-hidden="true" tabindex="-1"></a>With </span>
<span id="cb34-466"><a href="#cb34-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-467"><a href="#cb34-467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$x$ the position of the car</span>
<span id="cb34-468"><a href="#cb34-468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y$ the position of the bicycle </span>
<span id="cb34-469"><a href="#cb34-469" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$z$ the position of the walking man </span>
<span id="cb34-470"><a href="#cb34-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-471"><a href="#cb34-471" aria-hidden="true" tabindex="-1"></a>The rate of change in relative positions is given by terms like $\frac{\mathrm{d}x}{\mathrm{d}y}$, which gives us the change in relative position of bicycle and car. It we now aim to compute the rate of change of relative position of car to the walking man, $\frac{\mathrm{d}x}{\mathrm{d}z}$, we find </span>
<span id="cb34-472"><a href="#cb34-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-473"><a href="#cb34-473" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-474"><a href="#cb34-474" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}x}{\mathrm{d}x} = \frac{\mathrm{d}x}{\mathrm{d}y} \frac{\mathrm{d}y}{\mathrm{d}z} = \underbrace{2}_{\text{car twice as fast as bicycle}} \cdot \underbrace{4}_{\text{bicycle is four times as fast as walking man}} = 8 </span>
<span id="cb34-475"><a href="#cb34-475" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-476"><a href="#cb34-476" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-477"><a href="#cb34-477" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-478"><a href="#cb34-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-479"><a href="#cb34-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-480"><a href="#cb34-480" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computing derivatives as in calculus 101</span></span>
<span id="cb34-481"><a href="#cb34-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-482"><a href="#cb34-482" aria-hidden="true" tabindex="-1"></a>In neural networks, we nest functions. That is, will end up differentiating compound expression of the form </span>
<span id="cb34-483"><a href="#cb34-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-484"><a href="#cb34-484" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-485"><a href="#cb34-485" aria-hidden="true" tabindex="-1"></a>{\displaystyle h(x)=f(g(x))}</span>
<span id="cb34-486"><a href="#cb34-486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-487"><a href="#cb34-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-488"><a href="#cb34-488" aria-hidden="true" tabindex="-1"></a>For instance, you might look at a simple regularized logistic regression: </span>
<span id="cb34-489"><a href="#cb34-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-490"><a href="#cb34-490" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-491"><a href="#cb34-491" aria-hidden="true" tabindex="-1"></a>L = \frac{1}{2}\left(\sigma(wx +b) -t \right)^2 + \frac{\lambda}{2} w^2,</span>
<span id="cb34-492"><a href="#cb34-492" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-493"><a href="#cb34-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-494"><a href="#cb34-494" aria-hidden="true" tabindex="-1"></a>where $\sigma$ is some activation function (e.g. the sigmoid).</span>
<span id="cb34-495"><a href="#cb34-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-496"><a href="#cb34-496" aria-hidden="true" tabindex="-1"></a>If we now want to know what the influence of the weight $w$ is, we can differentiate the loss with respect to $w$: </span>
<span id="cb34-497"><a href="#cb34-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-498"><a href="#cb34-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-499"><a href="#cb34-499" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-500"><a href="#cb34-500" aria-hidden="true" tabindex="-1"></a>\frac{\partial L}{\partial w} &amp;= \frac{\partial}{\partial w} \left<span class="co">[</span><span class="ot">\frac{1}{2}\left(\sigma(wx +b) -t \right)^2 + \frac{\lambda}{2} w^2 \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-501"><a href="#cb34-501" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{2}\frac{\partial}{\partial w} \left(\sigma(wx +b) -t \right)^2 + \frac{\lambda}{2}\frac{\partial}{\partial w} w^2 <span class="sc">\\</span> </span>
<span id="cb34-502"><a href="#cb34-502" aria-hidden="true" tabindex="-1"></a>&amp;= \left(\sigma(wx+b) - t\right)\frac{\partial}{\partial w}\left(\sigma(wx+b)-t\right) + \lambda w <span class="sc">\\</span></span>
<span id="cb34-503"><a href="#cb34-503" aria-hidden="true" tabindex="-1"></a>&amp;= \left(\sigma(wx+b) - t\right)\sigma'(wx +b)\frac{\partial}{\partial w}(wx+b) + \lambda w <span class="sc">\\</span> </span>
<span id="cb34-504"><a href="#cb34-504" aria-hidden="true" tabindex="-1"></a>&amp;= \left(\sigma(wx+b) - t\right)\sigma'(wx +b)x + \lambda w</span>
<span id="cb34-505"><a href="#cb34-505" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-506"><a href="#cb34-506" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-507"><a href="#cb34-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-508"><a href="#cb34-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-509"><a href="#cb34-509" aria-hidden="true" tabindex="-1"></a>Puh! That was a lot of copying and pasting and quite error prone. And it might be quite costly to just directly evaluate such an expression (we might end up with an exponentially large expression, "expression swell").</span>
<span id="cb34-510"><a href="#cb34-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-511"><a href="#cb34-511" aria-hidden="true" tabindex="-1"></a>There must be a better way.</span>
<span id="cb34-512"><a href="#cb34-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-513"><a href="#cb34-513" aria-hidden="true" tabindex="-1"></a><span class="fu">### Making it efficient with caching </span></span>
<span id="cb34-514"><a href="#cb34-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-515"><a href="#cb34-515" aria-hidden="true" tabindex="-1"></a>One thing that we can observe is that we need to do the same computation several times. For instance, $wx +b$ is evaluated two times. We code trade off space and time complexity by caching this using an intermediate variable. </span>
<span id="cb34-516"><a href="#cb34-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-517"><a href="#cb34-517" aria-hidden="true" tabindex="-1"></a>If we do this systematically, we can very efficiently compute gradients -- in a form that is symmetric to the computation of the function itself (and those with basically the same cost). </span>
<span id="cb34-518"><a href="#cb34-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-519"><a href="#cb34-519" aria-hidden="true" tabindex="-1"></a><span class="fu">#### General computation with intermediate values</span></span>
<span id="cb34-520"><a href="#cb34-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-521"><a href="#cb34-521" aria-hidden="true" tabindex="-1"></a>As a simple example, let's start with </span>
<span id="cb34-522"><a href="#cb34-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-523"><a href="#cb34-523" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-524"><a href="#cb34-524" aria-hidden="true" tabindex="-1"></a>f(x,y,z) = (x+y)z</span>
<span id="cb34-525"><a href="#cb34-525" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-526"><a href="#cb34-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-527"><a href="#cb34-527" aria-hidden="true" tabindex="-1"></a>It can be convienient to introduce the following intermediate variable</span>
<span id="cb34-528"><a href="#cb34-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-529"><a href="#cb34-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-530"><a href="#cb34-530" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-531"><a href="#cb34-531" aria-hidden="true" tabindex="-1"></a>p = (x + y) </span>
<span id="cb34-532"><a href="#cb34-532" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-533"><a href="#cb34-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-534"><a href="#cb34-534" aria-hidden="true" tabindex="-1"></a>We can then write </span>
<span id="cb34-535"><a href="#cb34-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-536"><a href="#cb34-536" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-537"><a href="#cb34-537" aria-hidden="true" tabindex="-1"></a>f = pz</span>
<span id="cb34-538"><a href="#cb34-538" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-539"><a href="#cb34-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-540"><a href="#cb34-540" aria-hidden="true" tabindex="-1"></a>and also compute some partial derivatives </span>
<span id="cb34-541"><a href="#cb34-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-542"><a href="#cb34-542" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-543"><a href="#cb34-543" aria-hidden="true" tabindex="-1"></a>\frac{\partial f}{\partial q} = z \quad \frac{\partial f}{\partial z} = q</span>
<span id="cb34-544"><a href="#cb34-544" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-545"><a href="#cb34-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-546"><a href="#cb34-546" aria-hidden="true" tabindex="-1"></a>and we also know how to differentiate $p$ for $x$ and $y$:</span>
<span id="cb34-547"><a href="#cb34-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-548"><a href="#cb34-548" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-549"><a href="#cb34-549" aria-hidden="true" tabindex="-1"></a>\frac{\partial p}{\partial x} = 1 \quad \frac{\partial p}{\partial y} =1. </span>
<span id="cb34-550"><a href="#cb34-550" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-551"><a href="#cb34-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-552"><a href="#cb34-552" aria-hidden="true" tabindex="-1"></a>Using the _chain rule_ we can combine those findings, as the chain rule states that we need to multiply the gradients to chain them: </span>
<span id="cb34-553"><a href="#cb34-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-554"><a href="#cb34-554" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-555"><a href="#cb34-555" aria-hidden="true" tabindex="-1"></a>\frac{\partial f(p,z)}{\partial x} = \frac{\partial f(p, x)}{\partial p}  \frac{\partial p(x,y)}{\partial x} </span>
<span id="cb34-556"><a href="#cb34-556" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-557"><a href="#cb34-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-558"><a href="#cb34-558" aria-hidden="true" tabindex="-1"></a>This typically means that two numbers are multiplied. </span>
<span id="cb34-559"><a href="#cb34-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-560"><a href="#cb34-560" aria-hidden="true" tabindex="-1"></a>If we try it for the example above we can use the following code. Note how we _cache_ intermediate results (i.e. trade off time- vs. space-complexity). </span>
<span id="cb34-561"><a href="#cb34-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-564"><a href="#cb34-564" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-565"><a href="#cb34-565" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-566"><a href="#cb34-566" aria-hidden="true" tabindex="-1"></a><span class="co"># the inputs we will use </span></span>
<span id="cb34-567"><a href="#cb34-567" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="op">-</span><span class="dv">2</span></span>
<span id="cb34-568"><a href="#cb34-568" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb34-569"><a href="#cb34-569" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> <span class="op">-</span><span class="dv">4</span></span>
<span id="cb34-570"><a href="#cb34-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-571"><a href="#cb34-571" aria-hidden="true" tabindex="-1"></a><span class="co"># let's compute our intermediate terms</span></span>
<span id="cb34-572"><a href="#cb34-572" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> x <span class="op">+</span> y </span>
<span id="cb34-573"><a href="#cb34-573" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> t1 <span class="op">*</span> z</span>
<span id="cb34-574"><a href="#cb34-574" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-575"><a href="#cb34-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-576"><a href="#cb34-576" aria-hidden="true" tabindex="-1"></a>Now, we can look at the derivatives we got above</span>
<span id="cb34-577"><a href="#cb34-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-580"><a href="#cb34-580" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-581"><a href="#cb34-581" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-582"><a href="#cb34-582" aria-hidden="true" tabindex="-1"></a>dt1dx <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb34-583"><a href="#cb34-583" aria-hidden="true" tabindex="-1"></a>dt1dy <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb34-584"><a href="#cb34-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-585"><a href="#cb34-585" aria-hidden="true" tabindex="-1"></a>dfdt1 <span class="op">=</span> z</span>
<span id="cb34-586"><a href="#cb34-586" aria-hidden="true" tabindex="-1"></a>dfdz <span class="op">=</span> t1</span>
<span id="cb34-587"><a href="#cb34-587" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-588"><a href="#cb34-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-589"><a href="#cb34-589" aria-hidden="true" tabindex="-1"></a>Now, we can use the chain rule to combine them </span>
<span id="cb34-590"><a href="#cb34-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-593"><a href="#cb34-593" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-594"><a href="#cb34-594" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-595"><a href="#cb34-595" aria-hidden="true" tabindex="-1"></a>dfdx <span class="op">=</span> dfdt1 <span class="op">*</span> dt1dx</span>
<span id="cb34-596"><a href="#cb34-596" aria-hidden="true" tabindex="-1"></a>dfdy <span class="op">=</span> dfdt1 <span class="op">*</span> dt1dy</span>
<span id="cb34-597"><a href="#cb34-597" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-598"><a href="#cb34-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-599"><a href="#cb34-599" aria-hidden="true" tabindex="-1"></a>The sensitivity to $x$, $y$, and $z$ is hence</span>
<span id="cb34-600"><a href="#cb34-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-603"><a href="#cb34-603" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-604"><a href="#cb34-604" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-605"><a href="#cb34-605" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dfdz, dfdy, dfdz)</span>
<span id="cb34-606"><a href="#cb34-606" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-607"><a href="#cb34-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-608"><a href="#cb34-608" aria-hidden="true" tabindex="-1"></a>Before we move ahead, realize what we did: </span>
<span id="cb34-609"><a href="#cb34-609" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-610"><a href="#cb34-610" aria-hidden="true" tabindex="-1"></a>We computed gradients by recursively applying the chain rule, starting at the end: </span>
<span id="cb34-611"><a href="#cb34-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-612"><a href="#cb34-612" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>our computation graph is x -&gt; p -&gt; f</span>
<span id="cb34-613"><a href="#cb34-613" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we first compute df/dp, then dp/dx. Chaining them gives us df/dx = df/dp dp/dx</span>
<span id="cb34-614"><a href="#cb34-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-615"><a href="#cb34-615" aria-hidden="true" tabindex="-1"></a>We can write this in a more general form as follows.</span>
<span id="cb34-616"><a href="#cb34-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-617"><a href="#cb34-617" aria-hidden="true" tabindex="-1"></a>If we assume we have $N$ intermediate variables $t_N$, with $t_N$ being our output $f$, by definition we have</span>
<span id="cb34-618"><a href="#cb34-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-619"><a href="#cb34-619" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-620"><a href="#cb34-620" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}{f}}{\mathrm{d}t_N} = 1</span>
<span id="cb34-621"><a href="#cb34-621" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-622"><a href="#cb34-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-623"><a href="#cb34-623" aria-hidden="true" tabindex="-1"></a>For the other intermediate variables we have: </span>
<span id="cb34-624"><a href="#cb34-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-625"><a href="#cb34-625" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-626"><a href="#cb34-626" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-627"><a href="#cb34-627" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}f}{\mathrm{d} t_{n-1}} &amp;= \frac{\mathrm{d}f}{\mathrm{d}f_{n}} \frac{\mathrm{d}f_{n}}{\mathrm{d}t_{n-1}} <span class="sc">\\</span></span>
<span id="cb34-628"><a href="#cb34-628" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}f}{\mathrm{d} t_{n-2}} &amp;= \frac{\mathrm{d}f}{\mathrm{d}f_{n}} \frac{\mathrm{d}f_{n}}{\mathrm{d}t_{n-1}} \frac{\mathrm{d}f_{n-1}}{\mathrm{d}t_{n-2}} <span class="sc">\\</span></span>
<span id="cb34-629"><a href="#cb34-629" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}f}{\mathrm{d} t_{n-3}} &amp;= \frac{\mathrm{d}f}{\mathrm{d}f_{n}} \frac{\mathrm{d}f_{n}}{\mathrm{d}t_{n-1}} \frac{\mathrm{d}f_{n-1}}{\mathrm{d}t_{n-2}} \frac{\mathrm{d}f_{n-2}}{\mathrm{d}t_{n-3}} <span class="sc">\\</span></span>
<span id="cb34-630"><a href="#cb34-630" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}f}{\mathrm{d} t_i} &amp;= \frac{\mathrm{d}f}{\mathrm{d}f_{n}} \frac{\mathrm{d}f_{n}}{\mathrm{d}t_{n-1}} \frac{\mathrm{d}f_{n-1}}{\mathrm{d}t_{n-2}} \ldots \frac{\mathrm{d}f_{i+1}}{\mathrm{d}t_{i}}</span>
<span id="cb34-631"><a href="#cb34-631" aria-hidden="true" tabindex="-1"></a>\end{align} </span>
<span id="cb34-632"><a href="#cb34-632" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-633"><a href="#cb34-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-634"><a href="#cb34-634" aria-hidden="true" tabindex="-1"></a>Note that many of the terms we computed can be reused.</span>
<span id="cb34-635"><a href="#cb34-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-636"><a href="#cb34-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-637"><a href="#cb34-637" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application to neural networks</span></span>
<span id="cb34-638"><a href="#cb34-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-639"><a href="#cb34-639" aria-hidden="true" tabindex="-1"></a>Neural networks are more complicated circuits -- nested functions.</span>
<span id="cb34-640"><a href="#cb34-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-641"><a href="#cb34-641" aria-hidden="true" tabindex="-1"></a>Let's assume a very simply case</span>
<span id="cb34-642"><a href="#cb34-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-643"><a href="#cb34-643" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-644"><a href="#cb34-644" aria-hidden="true" tabindex="-1"></a>y=\frac{1}{1+\exp (-(wx+b))}.</span>
<span id="cb34-645"><a href="#cb34-645" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-646"><a href="#cb34-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-647"><a href="#cb34-647" aria-hidden="true" tabindex="-1"></a>We can write it using the chaining of the following primitive operations (forming our computation graph). </span>
<span id="cb34-648"><a href="#cb34-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-649"><a href="#cb34-649" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-650"><a href="#cb34-650" aria-hidden="true" tabindex="-1"></a>t_1 = wx</span>
<span id="cb34-651"><a href="#cb34-651" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-652"><a href="#cb34-652" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-653"><a href="#cb34-653" aria-hidden="true" tabindex="-1"></a>t_2 = t_1 + b</span>
<span id="cb34-654"><a href="#cb34-654" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-655"><a href="#cb34-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-656"><a href="#cb34-656" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-657"><a href="#cb34-657" aria-hidden="true" tabindex="-1"></a>t_3 = −t_2</span>
<span id="cb34-658"><a href="#cb34-658" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-659"><a href="#cb34-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-660"><a href="#cb34-660" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-661"><a href="#cb34-661" aria-hidden="true" tabindex="-1"></a>t_4 = \exp(t_3)</span>
<span id="cb34-662"><a href="#cb34-662" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-663"><a href="#cb34-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-664"><a href="#cb34-664" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-665"><a href="#cb34-665" aria-hidden="true" tabindex="-1"></a>t_5 = 1 + t_4</span>
<span id="cb34-666"><a href="#cb34-666" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-667"><a href="#cb34-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-668"><a href="#cb34-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-669"><a href="#cb34-669" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-670"><a href="#cb34-670" aria-hidden="true" tabindex="-1"></a>t_6 = 1/t_5</span>
<span id="cb34-671"><a href="#cb34-671" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-672"><a href="#cb34-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-673"><a href="#cb34-673" aria-hidden="true" tabindex="-1"></a>(this list of evaluations is sometimes called evaluation trace or Wengert list).</span>
<span id="cb34-674"><a href="#cb34-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-675"><a href="#cb34-675" aria-hidden="true" tabindex="-1"></a>As we would like again get the derivative w.r.t to the output like the loss</span>
<span id="cb34-676"><a href="#cb34-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-677"><a href="#cb34-677" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-678"><a href="#cb34-678" aria-hidden="true" tabindex="-1"></a>L = (t_6-y)^2, </span>
<span id="cb34-679"><a href="#cb34-679" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-680"><a href="#cb34-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-681"><a href="#cb34-681" aria-hidden="true" tabindex="-1"></a>which we can write down with some more evaluations </span>
<span id="cb34-682"><a href="#cb34-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-683"><a href="#cb34-683" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-684"><a href="#cb34-684" aria-hidden="true" tabindex="-1"></a>t_7 = t_6-t</span>
<span id="cb34-685"><a href="#cb34-685" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-686"><a href="#cb34-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-687"><a href="#cb34-687" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-688"><a href="#cb34-688" aria-hidden="true" tabindex="-1"></a>t_8 = t_7^2.</span>
<span id="cb34-689"><a href="#cb34-689" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-690"><a href="#cb34-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-691"><a href="#cb34-691" aria-hidden="true" tabindex="-1"></a>We call this evaluation the _forward pass_.</span>
<span id="cb34-692"><a href="#cb34-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-693"><a href="#cb34-693" aria-hidden="true" tabindex="-1"></a>The beauty of backprop is that the computation for the derivative follows the same structure as the computation of the function itself (and, for example, is not drastically more complex as one might expect). To see this, we can try out: </span>
<span id="cb34-694"><a href="#cb34-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-695"><a href="#cb34-695" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-696"><a href="#cb34-696" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-697"><a href="#cb34-697" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial t_8} &amp;= 1 <span class="sc">\\</span></span>
<span id="cb34-698"><a href="#cb34-698" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial t_7} &amp;= 2 t_7 <span class="sc">\\</span></span>
<span id="cb34-699"><a href="#cb34-699" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_7}{\partial t_6} &amp; = 1 <span class="sc">\\</span></span>
<span id="cb34-700"><a href="#cb34-700" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_6}{\partial t_5} &amp;=  -1/t_5^2 <span class="sc">\\</span></span>
<span id="cb34-701"><a href="#cb34-701" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_5}{\partial t_4} &amp;= 1<span class="sc">\\</span></span>
<span id="cb34-702"><a href="#cb34-702" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_4}{\partial t_3} &amp;= \exp(t_3) t_3 <span class="sc">\\</span></span>
<span id="cb34-703"><a href="#cb34-703" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_3}{\partial t_2} &amp;= - 1<span class="sc">\\</span></span>
<span id="cb34-704"><a href="#cb34-704" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_2}{\partial t_1} &amp;= 1 <span class="sc">\\</span></span>
<span id="cb34-705"><a href="#cb34-705" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_1}{\partial w} &amp;= x</span>
<span id="cb34-706"><a href="#cb34-706" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-707"><a href="#cb34-707" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-708"><a href="#cb34-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-709"><a href="#cb34-709" aria-hidden="true" tabindex="-1"></a>Armed with those partial derivatives, we can now multiply them to get the final goal -- the derivative of the loss w.r.t. the weight ($\frac{\partial L}{\partial w}$).</span>
<span id="cb34-710"><a href="#cb34-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-711"><a href="#cb34-711" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-712"><a href="#cb34-712" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-713"><a href="#cb34-713" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial t_6} &amp;= \frac{\partial t_8}{\partial t_7} \frac{\partial t_7}{\partial t_6} = 2 t_7 \cdot 1 = 2(t_6 -y) <span class="sc">\\</span></span>
<span id="cb34-714"><a href="#cb34-714" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial t_5} &amp;= \frac{\partial t_8}{\partial t_6} \frac{\partial t_6}{\partial t_5} = 2(t_6 -y) \cdot  \left(-\frac{1}{t_5^2} \right) =  -2/t_5^2 (t_6 -y) <span class="sc">\\</span></span>
<span id="cb34-715"><a href="#cb34-715" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial t_4} &amp;= \frac{\partial t_8}{\partial t_5} \frac{\partial t_5}{\partial t_4} = -2/t_5^2 (t_6 -y) \cdot 1 = -2/t_5^2 (t_6 -y) <span class="sc">\\</span></span>
<span id="cb34-716"><a href="#cb34-716" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial t_3} &amp;= \frac{\partial t_8}{\partial t_4} \frac{\partial t_4}{\partial t_3} = -2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 = -2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 <span class="sc">\\</span></span>
<span id="cb34-717"><a href="#cb34-717" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial t_2} &amp;= \frac{\partial t_8}{\partial t_3} \frac{\partial t_3}{\partial t_2} = -2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 \cdot -1 = 2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 <span class="sc">\\</span></span>
<span id="cb34-718"><a href="#cb34-718" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial t_1} &amp;= \frac{\partial t_8}{\partial t_2} \frac{\partial t_2}{\partial t_1} =  2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 <span class="sc">\\</span></span>
<span id="cb34-719"><a href="#cb34-719" aria-hidden="true" tabindex="-1"></a>\frac{\partial t_8}{\partial w} &amp;= \frac{\partial t_8}{\partial t_1} \frac{\partial t_1}{\partial w} = 2/t_5^2 (t_6 -y) \cdot \exp(t_3) t_3 \cdot x</span>
<span id="cb34-720"><a href="#cb34-720" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-721"><a href="#cb34-721" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-722"><a href="#cb34-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-723"><a href="#cb34-723" aria-hidden="true" tabindex="-1"></a>In practice, we would use autodifferentiation using a datastructure as follows to keep track of the computation graph.</span>
<span id="cb34-724"><a href="#cb34-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-727"><a href="#cb34-727" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-728"><a href="#cb34-728" aria-hidden="true" tabindex="-1"></a><span class="co"># code taken from https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb</span></span>
<span id="cb34-729"><a href="#cb34-729" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> graphviz <span class="im">import</span> Digraph</span>
<span id="cb34-730"><a href="#cb34-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-731"><a href="#cb34-731" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trace(root):</span>
<span id="cb34-732"><a href="#cb34-732" aria-hidden="true" tabindex="-1"></a>    nodes, edges <span class="op">=</span> <span class="bu">set</span>(), <span class="bu">set</span>()</span>
<span id="cb34-733"><a href="#cb34-733" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(v):</span>
<span id="cb34-734"><a href="#cb34-734" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> nodes:</span>
<span id="cb34-735"><a href="#cb34-735" aria-hidden="true" tabindex="-1"></a>            nodes.add(v)</span>
<span id="cb34-736"><a href="#cb34-736" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb34-737"><a href="#cb34-737" aria-hidden="true" tabindex="-1"></a>                edges.add((child, v))</span>
<span id="cb34-738"><a href="#cb34-738" aria-hidden="true" tabindex="-1"></a>                build(child)</span>
<span id="cb34-739"><a href="#cb34-739" aria-hidden="true" tabindex="-1"></a>    build(root)</span>
<span id="cb34-740"><a href="#cb34-740" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nodes, edges</span>
<span id="cb34-741"><a href="#cb34-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-742"><a href="#cb34-742" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_dot(root, <span class="bu">format</span><span class="op">=</span><span class="st">'svg'</span>, rankdir<span class="op">=</span><span class="st">'LR'</span>):</span>
<span id="cb34-743"><a href="#cb34-743" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb34-744"><a href="#cb34-744" aria-hidden="true" tabindex="-1"></a><span class="co">    format: png | svg | ...</span></span>
<span id="cb34-745"><a href="#cb34-745" aria-hidden="true" tabindex="-1"></a><span class="co">    rankdir: TB (top to bottom graph) | LR (left to right)</span></span>
<span id="cb34-746"><a href="#cb34-746" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb34-747"><a href="#cb34-747" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> rankdir <span class="kw">in</span> [<span class="st">'LR'</span>, <span class="st">'TB'</span>]</span>
<span id="cb34-748"><a href="#cb34-748" aria-hidden="true" tabindex="-1"></a>    nodes, edges <span class="op">=</span> trace(root)</span>
<span id="cb34-749"><a href="#cb34-749" aria-hidden="true" tabindex="-1"></a>    dot <span class="op">=</span> Digraph(<span class="bu">format</span><span class="op">=</span><span class="bu">format</span>, graph_attr<span class="op">=</span>{<span class="st">'rankdir'</span>: rankdir}) <span class="co">#, node_attr={'rankdir': 'TB'})</span></span>
<span id="cb34-750"><a href="#cb34-750" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-751"><a href="#cb34-751" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> nodes:</span>
<span id="cb34-752"><a href="#cb34-752" aria-hidden="true" tabindex="-1"></a>        dot.node(name<span class="op">=</span><span class="bu">str</span>(<span class="bu">id</span>(n)), label <span class="op">=</span> <span class="st">"{ data </span><span class="sc">%.4f</span><span class="st"> | grad </span><span class="sc">%.4f</span><span class="st"> }"</span> <span class="op">%</span> (n.data, n.grad), shape<span class="op">=</span><span class="st">'record'</span>)</span>
<span id="cb34-753"><a href="#cb34-753" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n._op:</span>
<span id="cb34-754"><a href="#cb34-754" aria-hidden="true" tabindex="-1"></a>            dot.node(name<span class="op">=</span><span class="bu">str</span>(<span class="bu">id</span>(n)) <span class="op">+</span> n._op, label<span class="op">=</span>n._op)</span>
<span id="cb34-755"><a href="#cb34-755" aria-hidden="true" tabindex="-1"></a>            dot.edge(<span class="bu">str</span>(<span class="bu">id</span>(n)) <span class="op">+</span> n._op, <span class="bu">str</span>(<span class="bu">id</span>(n)))</span>
<span id="cb34-756"><a href="#cb34-756" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-757"><a href="#cb34-757" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n1, n2 <span class="kw">in</span> edges:</span>
<span id="cb34-758"><a href="#cb34-758" aria-hidden="true" tabindex="-1"></a>        dot.edge(<span class="bu">str</span>(<span class="bu">id</span>(n1)), <span class="bu">str</span>(<span class="bu">id</span>(n2)) <span class="op">+</span> n2._op)</span>
<span id="cb34-759"><a href="#cb34-759" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-760"><a href="#cb34-760" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dot</span>
<span id="cb34-761"><a href="#cb34-761" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-762"><a href="#cb34-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-763"><a href="#cb34-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-766"><a href="#cb34-766" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-767"><a href="#cb34-767" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-768"><a href="#cb34-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-769"><a href="#cb34-769" aria-hidden="true" tabindex="-1"></a><span class="co"># taken from micrograd</span></span>
<span id="cb34-770"><a href="#cb34-770" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-771"><a href="#cb34-771" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Value:</span>
<span id="cb34-772"><a href="#cb34-772" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" stores a single scalar value and its gradient """</span></span>
<span id="cb34-773"><a href="#cb34-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-774"><a href="#cb34-774" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, _children<span class="op">=</span>(), _op<span class="op">=</span><span class="st">''</span>):</span>
<span id="cb34-775"><a href="#cb34-775" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<span id="cb34-776"><a href="#cb34-776" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-777"><a href="#cb34-777" aria-hidden="true" tabindex="-1"></a>        <span class="co"># internal variables used for autograd graph construction</span></span>
<span id="cb34-778"><a href="#cb34-778" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._backward <span class="op">=</span> <span class="kw">lambda</span>: <span class="va">None</span></span>
<span id="cb34-779"><a href="#cb34-779" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._prev <span class="op">=</span> <span class="bu">set</span>(_children)</span>
<span id="cb34-780"><a href="#cb34-780" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._op <span class="op">=</span> _op <span class="co"># the op that produced this node, for graphviz / debugging / etc</span></span>
<span id="cb34-781"><a href="#cb34-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-782"><a href="#cb34-782" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</span>
<span id="cb34-783"><a href="#cb34-783" aria-hidden="true" tabindex="-1"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other)</span>
<span id="cb34-784"><a href="#cb34-784" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">+</span> other.data, (<span class="va">self</span>, other), <span class="st">'+'</span>)</span>
<span id="cb34-785"><a href="#cb34-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-786"><a href="#cb34-786" aria-hidden="true" tabindex="-1"></a>        <span class="co"># propagate the gradient on out to parents</span></span>
<span id="cb34-787"><a href="#cb34-787" aria-hidden="true" tabindex="-1"></a>        <span class="co"># i.e. self and other </span></span>
<span id="cb34-788"><a href="#cb34-788" aria-hidden="true" tabindex="-1"></a>        <span class="co"># since out = self + other, then d(out)/dself = 1 and d(out)/dother = 1</span></span>
<span id="cb34-789"><a href="#cb34-789" aria-hidden="true" tabindex="-1"></a>        <span class="co"># so we can just add the gradient to both parents</span></span>
<span id="cb34-790"><a href="#cb34-790" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb34-791"><a href="#cb34-791" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">=</span> out.grad</span>
<span id="cb34-792"><a href="#cb34-792" aria-hidden="true" tabindex="-1"></a>            other.grad <span class="op">=</span> out.grad</span>
<span id="cb34-793"><a href="#cb34-793" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb34-794"><a href="#cb34-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-795"><a href="#cb34-795" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb34-796"><a href="#cb34-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-797"><a href="#cb34-797" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other):</span>
<span id="cb34-798"><a href="#cb34-798" aria-hidden="true" tabindex="-1"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other)</span>
<span id="cb34-799"><a href="#cb34-799" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">*</span> other.data, (<span class="va">self</span>, other), <span class="st">'*'</span>)</span>
<span id="cb34-800"><a href="#cb34-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-801"><a href="#cb34-801" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb34-802"><a href="#cb34-802" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">=</span> other.data <span class="op">*</span> out.grad</span>
<span id="cb34-803"><a href="#cb34-803" aria-hidden="true" tabindex="-1"></a>            other.grad <span class="op">=</span> <span class="va">self</span>.data <span class="op">*</span> out.grad</span>
<span id="cb34-804"><a href="#cb34-804" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb34-805"><a href="#cb34-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-806"><a href="#cb34-806" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb34-807"><a href="#cb34-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-808"><a href="#cb34-808" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__pow__</span>(<span class="va">self</span>, other):</span>
<span id="cb34-809"><a href="#cb34-809" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">isinstance</span>(other, (<span class="bu">int</span>, <span class="bu">float</span>)), <span class="st">"only supporting int/float powers for now"</span></span>
<span id="cb34-810"><a href="#cb34-810" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data<span class="op">**</span>other, (<span class="va">self</span>,), <span class="ss">f'**</span><span class="sc">{</span>other<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb34-811"><a href="#cb34-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-812"><a href="#cb34-812" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb34-813"><a href="#cb34-813" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">=</span> (other <span class="op">*</span> <span class="va">self</span>.data<span class="op">**</span>(other<span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> out.grad</span>
<span id="cb34-814"><a href="#cb34-814" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb34-815"><a href="#cb34-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-816"><a href="#cb34-816" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb34-817"><a href="#cb34-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-818"><a href="#cb34-818" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> exp(<span class="va">self</span>):</span>
<span id="cb34-819"><a href="#cb34-819" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(np.exp(<span class="va">self</span>.data), (<span class="va">self</span>,), <span class="st">'exp'</span>)</span>
<span id="cb34-820"><a href="#cb34-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-821"><a href="#cb34-821" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb34-822"><a href="#cb34-822" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">=</span> np.exp(<span class="va">self</span>.data) <span class="op">*</span> out.grad</span>
<span id="cb34-823"><a href="#cb34-823" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb34-824"><a href="#cb34-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-825"><a href="#cb34-825" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb34-826"><a href="#cb34-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-827"><a href="#cb34-827" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__neg__</span>(<span class="va">self</span>): <span class="co"># -self</span></span>
<span id="cb34-828"><a href="#cb34-828" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb34-829"><a href="#cb34-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-830"><a href="#cb34-830" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__radd__</span>(<span class="va">self</span>, other): <span class="co"># other + self</span></span>
<span id="cb34-831"><a href="#cb34-831" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">+</span> other</span>
<span id="cb34-832"><a href="#cb34-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-833"><a href="#cb34-833" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__sub__</span>(<span class="va">self</span>, other): <span class="co"># self - other</span></span>
<span id="cb34-834"><a href="#cb34-834" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">+</span> (<span class="op">-</span>other)</span>
<span id="cb34-835"><a href="#cb34-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-836"><a href="#cb34-836" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rsub__</span>(<span class="va">self</span>, other): <span class="co"># other - self</span></span>
<span id="cb34-837"><a href="#cb34-837" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> other <span class="op">+</span> (<span class="op">-</span><span class="va">self</span>)</span>
<span id="cb34-838"><a href="#cb34-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-839"><a href="#cb34-839" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rmul__</span>(<span class="va">self</span>, other): <span class="co"># other * self</span></span>
<span id="cb34-840"><a href="#cb34-840" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> other</span>
<span id="cb34-841"><a href="#cb34-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-842"><a href="#cb34-842" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__truediv__</span>(<span class="va">self</span>, other): <span class="co"># self / other</span></span>
<span id="cb34-843"><a href="#cb34-843" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span> <span class="op">*</span> other<span class="op">**-</span><span class="dv">1</span></span>
<span id="cb34-844"><a href="#cb34-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-845"><a href="#cb34-845" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rtruediv__</span>(<span class="va">self</span>, other): <span class="co"># other / self</span></span>
<span id="cb34-846"><a href="#cb34-846" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> other <span class="op">*</span> <span class="va">self</span><span class="op">**-</span><span class="dv">1</span></span>
<span id="cb34-847"><a href="#cb34-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-848"><a href="#cb34-848" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb34-849"><a href="#cb34-849" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"Value(data=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>data<span class="sc">}</span><span class="ss">, grad=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>grad<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb34-850"><a href="#cb34-850" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-851"><a href="#cb34-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-852"><a href="#cb34-852" aria-hidden="true" tabindex="-1"></a>We can now write down our expression from before using the <span class="in">`Value`</span> class</span>
<span id="cb34-853"><a href="#cb34-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-854"><a href="#cb34-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-857"><a href="#cb34-857" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-858"><a href="#cb34-858" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-859"><a href="#cb34-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-860"><a href="#cb34-860" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize some values</span></span>
<span id="cb34-861"><a href="#cb34-861" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> Value(<span class="fl">2.0</span>)</span>
<span id="cb34-862"><a href="#cb34-862" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Value(<span class="fl">0.0</span>)</span>
<span id="cb34-863"><a href="#cb34-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-864"><a href="#cb34-864" aria-hidden="true" tabindex="-1"></a><span class="co"># define the input</span></span>
<span id="cb34-865"><a href="#cb34-865" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Value(<span class="fl">1.0</span>)</span>
<span id="cb34-866"><a href="#cb34-866" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> Value(<span class="fl">10.0</span>)</span>
<span id="cb34-867"><a href="#cb34-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-868"><a href="#cb34-868" aria-hidden="true" tabindex="-1"></a><span class="co"># define the computation</span></span>
<span id="cb34-869"><a href="#cb34-869" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> w <span class="op">*</span> x</span>
<span id="cb34-870"><a href="#cb34-870" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> t1 <span class="op">+</span> b</span>
<span id="cb34-871"><a href="#cb34-871" aria-hidden="true" tabindex="-1"></a>t3 <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> t2</span>
<span id="cb34-872"><a href="#cb34-872" aria-hidden="true" tabindex="-1"></a>t4 <span class="op">=</span> t3.exp()</span>
<span id="cb34-873"><a href="#cb34-873" aria-hidden="true" tabindex="-1"></a>t5 <span class="op">=</span> t4 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb34-874"><a href="#cb34-874" aria-hidden="true" tabindex="-1"></a>t6 <span class="op">=</span> t5<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb34-875"><a href="#cb34-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-876"><a href="#cb34-876" aria-hidden="true" tabindex="-1"></a>t7 <span class="op">=</span> t6 <span class="op">-</span> target</span>
<span id="cb34-877"><a href="#cb34-877" aria-hidden="true" tabindex="-1"></a>t8 <span class="op">=</span> t7<span class="op">**</span><span class="dv">2</span></span>
<span id="cb34-878"><a href="#cb34-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-879"><a href="#cb34-879" aria-hidden="true" tabindex="-1"></a>draw_dot(t8)</span>
<span id="cb34-880"><a href="#cb34-880" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-881"><a href="#cb34-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-882"><a href="#cb34-882" aria-hidden="true" tabindex="-1"></a>We need to seed the gradient of the loss</span>
<span id="cb34-883"><a href="#cb34-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-886"><a href="#cb34-886" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-887"><a href="#cb34-887" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-888"><a href="#cb34-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-889"><a href="#cb34-889" aria-hidden="true" tabindex="-1"></a>t8.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb34-890"><a href="#cb34-890" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-891"><a href="#cb34-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-892"><a href="#cb34-892" aria-hidden="true" tabindex="-1"></a>Now, we can perform the backward pass by calling the <span class="in">`_backward`</span> function of the loss node, which will in turn call the <span class="in">`_backward`</span> functions of all its parents, and so on, until the entire graph has been visited.</span>
<span id="cb34-893"><a href="#cb34-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-896"><a href="#cb34-896" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-897"><a href="#cb34-897" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-898"><a href="#cb34-898" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb34-899"><a href="#cb34-899" aria-hidden="true" tabindex="-1"></a>t8._backward()</span>
<span id="cb34-900"><a href="#cb34-900" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span>
<span id="cb34-901"><a href="#cb34-901" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-902"><a href="#cb34-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-905"><a href="#cb34-905" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-906"><a href="#cb34-906" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-907"><a href="#cb34-907" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb34-908"><a href="#cb34-908" aria-hidden="true" tabindex="-1"></a>t7._backward()</span>
<span id="cb34-909"><a href="#cb34-909" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span>
<span id="cb34-910"><a href="#cb34-910" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-911"><a href="#cb34-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-912"><a href="#cb34-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-915"><a href="#cb34-915" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-916"><a href="#cb34-916" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-917"><a href="#cb34-917" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb34-918"><a href="#cb34-918" aria-hidden="true" tabindex="-1"></a>t6._backward()</span>
<span id="cb34-919"><a href="#cb34-919" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span>
<span id="cb34-920"><a href="#cb34-920" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-921"><a href="#cb34-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-924"><a href="#cb34-924" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-925"><a href="#cb34-925" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-926"><a href="#cb34-926" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb34-927"><a href="#cb34-927" aria-hidden="true" tabindex="-1"></a>t5._backward()  </span>
<span id="cb34-928"><a href="#cb34-928" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span>
<span id="cb34-929"><a href="#cb34-929" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-930"><a href="#cb34-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-933"><a href="#cb34-933" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-934"><a href="#cb34-934" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-935"><a href="#cb34-935" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb34-936"><a href="#cb34-936" aria-hidden="true" tabindex="-1"></a>t4._backward()</span>
<span id="cb34-937"><a href="#cb34-937" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span>
<span id="cb34-938"><a href="#cb34-938" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-939"><a href="#cb34-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-942"><a href="#cb34-942" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-943"><a href="#cb34-943" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-944"><a href="#cb34-944" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb34-945"><a href="#cb34-945" aria-hidden="true" tabindex="-1"></a>t3._backward()</span>
<span id="cb34-946"><a href="#cb34-946" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span>
<span id="cb34-947"><a href="#cb34-947" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-948"><a href="#cb34-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-949"><a href="#cb34-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-952"><a href="#cb34-952" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-953"><a href="#cb34-953" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-954"><a href="#cb34-954" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb34-955"><a href="#cb34-955" aria-hidden="true" tabindex="-1"></a>t2._backward()</span>
<span id="cb34-956"><a href="#cb34-956" aria-hidden="true" tabindex="-1"></a>w._backward()</span>
<span id="cb34-957"><a href="#cb34-957" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span>
<span id="cb34-958"><a href="#cb34-958" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-959"><a href="#cb34-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-960"><a href="#cb34-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-961"><a href="#cb34-961" aria-hidden="true" tabindex="-1"></a>To avoid calling the backward function multiple times, we can implement a <span class="in">`backprop`</span> function that traverses the graph in reverse topological order and calls the <span class="in">`_backward`</span> function of each node only once. </span>
<span id="cb34-962"><a href="#cb34-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-963"><a href="#cb34-963" aria-hidden="true" tabindex="-1"></a>Topological sorting can be implemented using the following code</span>
<span id="cb34-964"><a href="#cb34-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-967"><a href="#cb34-967" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-968"><a href="#cb34-968" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-969"><a href="#cb34-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-970"><a href="#cb34-970" aria-hidden="true" tabindex="-1"></a>topo <span class="op">=</span> []</span>
<span id="cb34-971"><a href="#cb34-971" aria-hidden="true" tabindex="-1"></a>visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb34-972"><a href="#cb34-972" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_topo(v):</span>
<span id="cb34-973"><a href="#cb34-973" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb34-974"><a href="#cb34-974" aria-hidden="true" tabindex="-1"></a>        visited.add(v)</span>
<span id="cb34-975"><a href="#cb34-975" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> child <span class="kw">in</span> v._prev:</span>
<span id="cb34-976"><a href="#cb34-976" aria-hidden="true" tabindex="-1"></a>            build_topo(child)</span>
<span id="cb34-977"><a href="#cb34-977" aria-hidden="true" tabindex="-1"></a>        topo.append(v)</span>
<span id="cb34-978"><a href="#cb34-978" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-979"><a href="#cb34-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-980"><a href="#cb34-980" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb34-981"><a href="#cb34-981" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why does this sorting algorithm work?</span></span>
<span id="cb34-982"><a href="#cb34-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-983"><a href="#cb34-983" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The algorithm is a depth-first search (DFS)</span>
<span id="cb34-984"><a href="#cb34-984" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The deepest nodes are added to the <span class="in">`topo`</span> list first</span>
<span id="cb34-985"><a href="#cb34-985" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Recursiveness ensures that nodes another node depends on are added first (<span class="in">`topo.append`</span> only happens after the recursive call)</span>
<span id="cb34-986"><a href="#cb34-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-987"><a href="#cb34-987" aria-hidden="true" tabindex="-1"></a>Note that this algorithm does not work for cyclic graphs.</span>
<span id="cb34-988"><a href="#cb34-988" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb34-989"><a href="#cb34-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-990"><a href="#cb34-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-991"><a href="#cb34-991" aria-hidden="true" tabindex="-1"></a>Now, we can simply write</span>
<span id="cb34-992"><a href="#cb34-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-993"><a href="#cb34-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-996"><a href="#cb34-996" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-997"><a href="#cb34-997" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-998"><a href="#cb34-998" aria-hidden="true" tabindex="-1"></a><span class="co"># #| </span></span>
<span id="cb34-999"><a href="#cb34-999" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize some values</span></span>
<span id="cb34-1000"><a href="#cb34-1000" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> Value(<span class="fl">2.0</span>)</span>
<span id="cb34-1001"><a href="#cb34-1001" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> Value(<span class="fl">0.0</span>)</span>
<span id="cb34-1002"><a href="#cb34-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1003"><a href="#cb34-1003" aria-hidden="true" tabindex="-1"></a><span class="co"># define the input</span></span>
<span id="cb34-1004"><a href="#cb34-1004" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Value(<span class="fl">1.0</span>)</span>
<span id="cb34-1005"><a href="#cb34-1005" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> Value(<span class="fl">10.0</span>)</span>
<span id="cb34-1006"><a href="#cb34-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1007"><a href="#cb34-1007" aria-hidden="true" tabindex="-1"></a><span class="co"># define the computation</span></span>
<span id="cb34-1008"><a href="#cb34-1008" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> w <span class="op">*</span> x</span>
<span id="cb34-1009"><a href="#cb34-1009" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> t1 <span class="op">+</span> b</span>
<span id="cb34-1010"><a href="#cb34-1010" aria-hidden="true" tabindex="-1"></a>t3 <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> t2</span>
<span id="cb34-1011"><a href="#cb34-1011" aria-hidden="true" tabindex="-1"></a>t4 <span class="op">=</span> t3.exp()</span>
<span id="cb34-1012"><a href="#cb34-1012" aria-hidden="true" tabindex="-1"></a>t5 <span class="op">=</span> t4 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb34-1013"><a href="#cb34-1013" aria-hidden="true" tabindex="-1"></a>t6 <span class="op">=</span> t5<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb34-1014"><a href="#cb34-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1015"><a href="#cb34-1015" aria-hidden="true" tabindex="-1"></a>t7 <span class="op">=</span> t6 <span class="op">-</span> target</span>
<span id="cb34-1016"><a href="#cb34-1016" aria-hidden="true" tabindex="-1"></a>t8 <span class="op">=</span> t7<span class="op">**</span><span class="dv">2</span></span>
<span id="cb34-1017"><a href="#cb34-1017" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-1018"><a href="#cb34-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1019"><a href="#cb34-1019" aria-hidden="true" tabindex="-1"></a>And now call the topological sorting and then <span class="in">`_backward`</span> for all nodes</span>
<span id="cb34-1020"><a href="#cb34-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1023"><a href="#cb34-1023" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-1024"><a href="#cb34-1024" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-1025"><a href="#cb34-1025" aria-hidden="true" tabindex="-1"></a>t8.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb34-1026"><a href="#cb34-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1027"><a href="#cb34-1027" aria-hidden="true" tabindex="-1"></a>build_topo(t8)</span>
<span id="cb34-1028"><a href="#cb34-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1029"><a href="#cb34-1029" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> v <span class="kw">in</span> <span class="bu">reversed</span>(topo):</span>
<span id="cb34-1030"><a href="#cb34-1030" aria-hidden="true" tabindex="-1"></a>    v._backward()</span>
<span id="cb34-1031"><a href="#cb34-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1032"><a href="#cb34-1032" aria-hidden="true" tabindex="-1"></a>w.grad</span>
<span id="cb34-1033"><a href="#cb34-1033" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-1034"><a href="#cb34-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1035"><a href="#cb34-1035" aria-hidden="true" tabindex="-1"></a>Note that we had to reverse the topological ordering because the deepest dependent of <span class="in">`t8`</span> was first and we need to work backwards.</span>
<span id="cb34-1036"><a href="#cb34-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1037"><a href="#cb34-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1038"><a href="#cb34-1038" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lecture </span></span>
<span id="cb34-1039"><a href="#cb34-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1040"><a href="#cb34-1040" aria-hidden="true" tabindex="-1"></a>If you prefer watching a short video over reading you can see me go through the gist of backprop in the following video.</span>
<span id="cb34-1041"><a href="#cb34-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1042"><a href="#cb34-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1043"><a href="#cb34-1043" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">style</span><span class="ot">=</span><span class="st">"position: relative; padding-bottom: 56.25%; height: 0;"</span><span class="kw">&gt;&lt;iframe</span> <span class="er">src</span><span class="ot">=</span><span class="st">"https://www.loom.com/embed/579ab50060044464832777e6650180f3?sid=0412526a-5a1e-4e24-ab37-691214804dc5"</span> <span class="er">frameborder</span><span class="ot">=</span><span class="st">"0"</span> <span class="er">webkitallowfullscreen</span> <span class="er">mozallowfullscreen</span> <span class="er">allowfullscreen</span> <span class="er">style</span><span class="ot">=</span><span class="st">"position: absolute; top: 0; left: 0; width: 100%; height: 100%;"</span><span class="kw">&gt;&lt;/iframe&gt;&lt;/div&gt;</span></span>
<span id="cb34-1044"><a href="#cb34-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1045"><a href="#cb34-1045" aria-hidden="true" tabindex="-1"></a><span class="fu">## Resources </span></span>
<span id="cb34-1046"><a href="#cb34-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1047"><a href="#cb34-1047" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="co">[</span><span class="ot">Andrej Karpathy "Hacker's guide to Neural Networks"</span><span class="co">](https://karpathy.github.io/neuralnets/)</span> inspired the comparison between random search and gradient descent. The same ideas are used in the <span class="co">[</span><span class="ot">cs231n lecture notes</span><span class="co">](https://cs231n.github.io/optimization-1/)</span> since he taught this class. The chain rule example is taken from the <span class="co">[</span><span class="ot">c231n lecture notes</span><span class="co">](https://cs231n.github.io/optimization-2/)</span></span>
<span id="cb34-1048"><a href="#cb34-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1049"><a href="#cb34-1049" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="co">[</span><span class="ot">Andrej Karparthy recorded a lecture in which he builds an autodiff system from scratch</span><span class="co">](https://www.youtube.com/watch?v=PaCmpygFfXo)</span> and it inspired many parts of the notebooks, some parts (the <span class="in">`Value`</span> class) are taken from his lecture.</span>
<span id="cb34-1050"><a href="#cb34-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1051"><a href="#cb34-1051" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="co">[</span><span class="ot">Deisenroth et al. "Mathematics of Machine Learning"</span><span class="co">](https://mml-book.github.io/)</span> has a beautiful chapter about backprop and autodiff.</span>
<span id="cb34-1052"><a href="#cb34-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1053"><a href="#cb34-1053" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span><span class="co">[</span><span class="ot">Mark Saroufim "Automatic Differentiation Step by Step"</span><span class="co">](https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6)</span> has an intuitive explaination of dual numbers and has a good resource section, including [](https://www.youtube.com/watch?v=Rs0uRQJdIcg&amp;list=WL&amp;index=8&amp;t=149s)</span>
<span id="cb34-1054"><a href="#cb34-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1055"><a href="#cb34-1055" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span><span class="co">[</span><span class="ot">Automatic Differentiation in Machine Learning: a Survey</span><span class="co">](http://arxiv.org/abs/1502.05767)</span> is a great survey that clarifies many terms. </span>
<span id="cb34-1056"><a href="#cb34-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1057"><a href="#cb34-1057" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span><span class="co">[</span><span class="ot">Michael Nielsen's book</span><span class="co">](http://neuralnetworksanddeeplearning.com/chap2.html)</span> highlights some of the "hidden" assumptions. </span>
<span id="cb34-1058"><a href="#cb34-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1059"><a href="#cb34-1059" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span><span class="co">[</span><span class="ot">Brandon Rohrer</span><span class="co">](https://e2eml.school/how_backpropagation_works)</span> has a very intuitive of the chain rule in terms of the shower rate (similar to the bicycle/car/man example above).</span>
<span id="cb34-1060"><a href="#cb34-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1061"><a href="#cb34-1061" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span><span class="co">[</span><span class="ot">Deep Learning Systems Lecture at CMU</span><span class="co">](https://dlsyscourse.org/lectures/)</span> has a detailed slides on the algorithmic details behind autodiff.</span>
<span id="cb34-1062"><a href="#cb34-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1063"><a href="#cb34-1063" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span><span class="co">[</span><span class="ot">Differentiation for Hackers</span><span class="co">](https://github.com/MikeInnes/diff-zoo/tree/master/src)</span> has nice Julia code that showcases what makes autodiff special (and different from symbolic and numeric differentiation).</span>
<span id="cb34-1064"><a href="#cb34-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1065"><a href="#cb34-1065" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span><span class="co">[</span><span class="ot">Kyle Cranmer</span><span class="co">](https://theoryandpractice.org/stats-ds-book/autodiff-tutorial.html)</span> has a useful intro to autodiff. I took the <span class="in">`sympy`</span> example from there.</span>
<span id="cb34-1066"><a href="#cb34-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1067"><a href="#cb34-1067" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further reading</span></span>
<span id="cb34-1068"><a href="#cb34-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1069"><a href="#cb34-1069" aria-hidden="true" tabindex="-1"></a><span class="fu">### Who "invented" backpropagation</span></span>
<span id="cb34-1070"><a href="#cb34-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1071"><a href="#cb34-1071" aria-hidden="true" tabindex="-1"></a>As with many popular things, there is some debate on "who was first". You can find some discussion on this <span class="co">[</span><span class="ot">here</span><span class="co">](https://people.idsia.ch/~juergen/who-invented-backpropagation.html#BP1)</span>.</span>
<span id="cb34-1072"><a href="#cb34-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1073"><a href="#cb34-1073" aria-hidden="true" tabindex="-1"></a><span class="fu">#### "Original" Backprop Paper</span></span>
<span id="cb34-1074"><a href="#cb34-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1075"><a href="#cb34-1075" aria-hidden="true" tabindex="-1"></a>In the context of training neural networks, backpropagation was popularized in a beatiful paper by <span class="co">[</span><span class="ot">David E. Rumelhart et al.</span><span class="co">](https://www.nature.com/articles/323533a0)</span> It is beautiful and you should read it. </span>
<span id="cb34-1076"><a href="#cb34-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1077"><a href="#cb34-1077" aria-hidden="true" tabindex="-1"></a><span class="fu">### Backpropagation and Lagrangian </span></span>
<span id="cb34-1078"><a href="#cb34-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1079"><a href="#cb34-1079" aria-hidden="true" tabindex="-1"></a>As <span class="co">[</span><span class="ot">this blog post by Tim Viera</span><span class="co">](https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/)</span> and <span class="co">[</span><span class="ot">this paper by Yann LeCun</span><span class="co">](https://arc.net/l/quote/mjznlhvx)</span> show, the intermediate variables can be recovered by rephrasing the optimization as a constrained optimization using the Lagrangian framework.</span>
<span id="cb34-1080"><a href="#cb34-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1081"><a href="#cb34-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1082"><a href="#cb34-1082" aria-hidden="true" tabindex="-1"></a><span class="fu">### Forward vs. reverse mode autodiff</span></span>
<span id="cb34-1083"><a href="#cb34-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1084"><a href="#cb34-1084" aria-hidden="true" tabindex="-1"></a>If we have a computation graph as follows</span>
<span id="cb34-1085"><a href="#cb34-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1086"><a href="#cb34-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1087"><a href="#cb34-1087" aria-hidden="true" tabindex="-1"></a><span class="in">`x -&gt; a -&gt; b -&gt; y `</span></span>
<span id="cb34-1088"><a href="#cb34-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1089"><a href="#cb34-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1090"><a href="#cb34-1090" aria-hidden="true" tabindex="-1"></a>we can compute the derivative of the output with respect to the input as </span>
<span id="cb34-1091"><a href="#cb34-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1092"><a href="#cb34-1092" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1093"><a href="#cb34-1093" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\mathrm{d}y}{\mathrm{d}b}\frac{\mathrm{d}b}{\mathrm{d}a} \frac{\mathrm{d}a}{\mathrm{d}x}</span>
<span id="cb34-1094"><a href="#cb34-1094" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1095"><a href="#cb34-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1096"><a href="#cb34-1096" aria-hidden="true" tabindex="-1"></a>since multiplication is associative, we can choose between computing</span>
<span id="cb34-1097"><a href="#cb34-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1098"><a href="#cb34-1098" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1099"><a href="#cb34-1099" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}y}{\mathrm{d}x} = \left( \frac{\mathrm{d}y}{\mathrm{d}b}\frac{\mathrm{d}b}{\mathrm{d}a} \right) \frac{\mathrm{d}a}{\mathrm{d}x}</span>
<span id="cb34-1100"><a href="#cb34-1100" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1101"><a href="#cb34-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1102"><a href="#cb34-1102" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb34-1103"><a href="#cb34-1103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1104"><a href="#cb34-1104" aria-hidden="true" tabindex="-1"></a>\frac{\mathrm{d}y}{\mathrm{d}x} =  \frac{\mathrm{d}y}{\mathrm{d}b}\left(\frac{\mathrm{d}b}{\mathrm{d}a}  \frac{\mathrm{d}a}{\mathrm{d}x} \right)</span>
<span id="cb34-1105"><a href="#cb34-1105" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1106"><a href="#cb34-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1107"><a href="#cb34-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1108"><a href="#cb34-1108" aria-hidden="true" tabindex="-1"></a>The first mode is called "reverse mode" autodiff as the gradient flow is opposite to the data flow. </span>
<span id="cb34-1109"><a href="#cb34-1109" aria-hidden="true" tabindex="-1"></a>The second mode is called "forward mode" autodiff as the order of computation is the same for the gradient computation as for the computation of the function itself.</span>
<span id="cb34-1110"><a href="#cb34-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1111"><a href="#cb34-1111" aria-hidden="true" tabindex="-1"></a>Backpropagation is a special case of reverse mode autodiff.</span>
<span id="cb34-1112"><a href="#cb34-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1113"><a href="#cb34-1113" aria-hidden="true" tabindex="-1"></a>Which mode is more efficient depends on whether the input dimension is smaller than the output dimension.</span>
<span id="cb34-1114"><a href="#cb34-1114" aria-hidden="true" tabindex="-1"></a>If the output dimension is smaller than the input dimension (which is the case for training neural networks) the reverse mode is more efficient as only one application of the reverse mode is needed to compute the gradients. </span>
<span id="cb34-1115"><a href="#cb34-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1116"><a href="#cb34-1116" aria-hidden="true" tabindex="-1"></a>The forward mode, however is of $\mathcal{O(n)}$, where $n$ is the number of inputs. If the number of inputs is small (or even just one) and the number of outputs is large, e.g. $\mathbb{R} \to \mathbb{R^m}$, then the forward mode will be more efficient.</span>
<span id="cb34-1117"><a href="#cb34-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1118"><a href="#cb34-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1119"><a href="#cb34-1119" aria-hidden="true" tabindex="-1"></a><span class="fu">### Symbolic differentiation vs. numerical differentiation vs. autodiff</span></span>
<span id="cb34-1120"><a href="#cb34-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1121"><a href="#cb34-1121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Numerical differentiation involves computing a term like $\frac{\partial f}{\partial x_i} \approx \frac{f(x+h) - f(x)}{h}$ for a small $h$. While this is might be relatively easy to implement, but requires $\mathcal{O(n)}$ evaluations for $n$ gradients, and can be numerically unstable (dividing by small number, subtracting two numbers of almost the same value).</span>
<span id="cb34-1122"><a href="#cb34-1122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Symbolic differentation can be performed with systems like Maple, Sympy, or Mathematica. This gives us _expressions_ for the derivatives, which might grow exponentially large (in blind application).</span>
<span id="cb34-1123"><a href="#cb34-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1126"><a href="#cb34-1126" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-1127"><a href="#cb34-1127" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy </span>
<span id="cb34-1128"><a href="#cb34-1128" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sympy.symbols(<span class="st">'x'</span>)</span>
<span id="cb34-1129"><a href="#cb34-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1130"><a href="#cb34-1130" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> base_function(x): </span>
<span id="cb34-1131"><a href="#cb34-1131" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span><span class="dv">3</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">4</span></span>
<span id="cb34-1132"><a href="#cb34-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1133"><a href="#cb34-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1134"><a href="#cb34-1134" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-1135"><a href="#cb34-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1136"><a href="#cb34-1136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Autodiff can easily deal with control flows</span>
<span id="cb34-1137"><a href="#cb34-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1138"><a href="#cb34-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1139"><a href="#cb34-1139" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dual numbers </span></span>
<span id="cb34-1140"><a href="#cb34-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1141"><a href="#cb34-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1142"><a href="#cb34-1142" aria-hidden="true" tabindex="-1"></a>Dual numbers are numbers of the form $v+\dot{v}\epsilon$, where $\epsilon$ has the special property that it is non-zero and $\epsilon^2 = 0$.</span>
<span id="cb34-1143"><a href="#cb34-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1144"><a href="#cb34-1144" aria-hidden="true" tabindex="-1"></a>They behave as one might expect: </span>
<span id="cb34-1145"><a href="#cb34-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1146"><a href="#cb34-1146" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1147"><a href="#cb34-1147" aria-hidden="true" tabindex="-1"></a>(v+\dot{v}\epsilon) + (u + \dot{u}\epsilon) = (v + u) + (\dot{v} + \dot{u})\epsilon</span>
<span id="cb34-1148"><a href="#cb34-1148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1149"><a href="#cb34-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1150"><a href="#cb34-1150" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb34-1151"><a href="#cb34-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1152"><a href="#cb34-1152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1153"><a href="#cb34-1153" aria-hidden="true" tabindex="-1"></a>(v+\dot{v}\epsilon)(u+\dot{u}\epsilon) = (vu) + (v\dot{u} + \dot{u}v)\epsilon</span>
<span id="cb34-1154"><a href="#cb34-1154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1155"><a href="#cb34-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1156"><a href="#cb34-1156" aria-hidden="true" tabindex="-1"></a>Now, keep in mind that the Tyalor series of a function $f(x)</span>
<span id="cb34-1157"><a href="#cb34-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1158"><a href="#cb34-1158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1159"><a href="#cb34-1159" aria-hidden="true" tabindex="-1"></a>f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!} (x-a)^2 + \frac{f'''(a)}{3!} (x-a)^3 </span>
<span id="cb34-1160"><a href="#cb34-1160" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1161"><a href="#cb34-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1162"><a href="#cb34-1162" aria-hidden="true" tabindex="-1"></a>Now, if  $x = a+\dot{v}\epsilon$</span>
<span id="cb34-1163"><a href="#cb34-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1164"><a href="#cb34-1164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1165"><a href="#cb34-1165" aria-hidden="true" tabindex="-1"></a>f(a + \dot{v}\epsilon) = f(a) + f'(a)(a + \dot{v}\epsilon -a) +  \frac{f''(a)}{2!} (a + \dot{v}\epsilon -a)^2 + \frac{f'''(a)}{3!} (a + \dot{v}\epsilon -a)^3 </span>
<span id="cb34-1166"><a href="#cb34-1166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1167"><a href="#cb34-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1168"><a href="#cb34-1168" aria-hidden="true" tabindex="-1"></a>not that, per definition, all terms with $\epsilon^2$ or higher powers will vanish. Therefore, we will be left with </span>
<span id="cb34-1169"><a href="#cb34-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1170"><a href="#cb34-1170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1171"><a href="#cb34-1171" aria-hidden="true" tabindex="-1"></a>f(a + \dot{v}\epsilon) = f(a) + f'(a)\dot{v}\epsilon</span>
<span id="cb34-1172"><a href="#cb34-1172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1173"><a href="#cb34-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1174"><a href="#cb34-1174" aria-hidden="true" tabindex="-1"></a>That is, we can do something like </span>
<span id="cb34-1175"><a href="#cb34-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1176"><a href="#cb34-1176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1177"><a href="#cb34-1177" aria-hidden="true" tabindex="-1"></a>\left. \frac{\mathrm{d}f}{\mathrm{d}x}\right|_{x=a} = \text{epsilon coefficient}(\text{dual version}(f)(a+1\epsilon))</span>
<span id="cb34-1178"><a href="#cb34-1178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-1179"><a href="#cb34-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1180"><a href="#cb34-1180" aria-hidden="true" tabindex="-1"></a>This means that we directly compute f(x) and the derivative (scaled by $\dot{v}$). Thus, we can simulatanously compute the values of functions and derivatives. A naiive implementation might look as follows</span>
<span id="cb34-1181"><a href="#cb34-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1182"><a href="#cb34-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1185"><a href="#cb34-1185" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-1186"><a href="#cb34-1186" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-1187"><a href="#cb34-1187" aria-hidden="true" tabindex="-1"></a><span class="co">#| </span></span>
<span id="cb34-1188"><a href="#cb34-1188" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math </span>
<span id="cb34-1189"><a href="#cb34-1189" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DualNumber:</span>
<span id="cb34-1190"><a href="#cb34-1190" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, real, dual):</span>
<span id="cb34-1191"><a href="#cb34-1191" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.real <span class="op">=</span> real  <span class="co"># Real part</span></span>
<span id="cb34-1192"><a href="#cb34-1192" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dual <span class="op">=</span> dual  <span class="co"># Dual part (coefficient of epsilon)</span></span>
<span id="cb34-1193"><a href="#cb34-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1194"><a href="#cb34-1194" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb34-1195"><a href="#cb34-1195" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>real<span class="sc">}</span><span class="ss"> + </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>dual<span class="sc">}</span><span class="ss">ε"</span></span>
<span id="cb34-1196"><a href="#cb34-1196" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-1197"><a href="#cb34-1197" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</span>
<span id="cb34-1198"><a href="#cb34-1198" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Addition with another DualNumber or scalar</span></span>
<span id="cb34-1199"><a href="#cb34-1199" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(other, DualNumber):</span>
<span id="cb34-1200"><a href="#cb34-1200" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.real <span class="op">+</span> other.real, <span class="va">self</span>.dual <span class="op">+</span> other.dual)</span>
<span id="cb34-1201"><a href="#cb34-1201" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb34-1202"><a href="#cb34-1202" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.real <span class="op">+</span> other, <span class="va">self</span>.dual)</span>
<span id="cb34-1203"><a href="#cb34-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1204"><a href="#cb34-1204" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other):</span>
<span id="cb34-1205"><a href="#cb34-1205" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multiplication with another DualNumber or scalar</span></span>
<span id="cb34-1206"><a href="#cb34-1206" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(other, DualNumber):</span>
<span id="cb34-1207"><a href="#cb34-1207" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.real <span class="op">*</span> other.real, <span class="va">self</span>.real <span class="op">*</span> other.dual <span class="op">+</span> <span class="va">self</span>.dual <span class="op">*</span> other.real)</span>
<span id="cb34-1208"><a href="#cb34-1208" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb34-1209"><a href="#cb34-1209" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DualNumber(<span class="va">self</span>.real <span class="op">*</span> other, <span class="va">self</span>.dual <span class="op">*</span> other)</span>
<span id="cb34-1210"><a href="#cb34-1210" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-1211"><a href="#cb34-1211" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__radd__</span>(<span class="va">self</span>, other):</span>
<span id="cb34-1212"><a href="#cb34-1212" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.<span class="fu">__add__</span>(other)</span>
<span id="cb34-1213"><a href="#cb34-1213" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-1214"><a href="#cb34-1214" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__rmul__</span>(<span class="va">self</span>, other):</span>
<span id="cb34-1215"><a href="#cb34-1215" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.<span class="fu">__mul__</span>(other)</span>
<span id="cb34-1216"><a href="#cb34-1216" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-1217"><a href="#cb34-1217" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> exp(<span class="va">self</span>):</span>
<span id="cb34-1218"><a href="#cb34-1218" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Exponential function</span></span>
<span id="cb34-1219"><a href="#cb34-1219" aria-hidden="true" tabindex="-1"></a>        exp_real <span class="op">=</span> math.exp(<span class="va">self</span>.real)</span>
<span id="cb34-1220"><a href="#cb34-1220" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> DualNumber(exp_real, exp_real <span class="op">*</span> <span class="va">self</span>.dual)</span>
<span id="cb34-1221"><a href="#cb34-1221" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-1222"><a href="#cb34-1222" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> square(<span class="va">self</span>):</span>
<span id="cb34-1223"><a href="#cb34-1223" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Squaring the dual number</span></span>
<span id="cb34-1224"><a href="#cb34-1224" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> DualNumber(<span class="va">self</span>.real<span class="op">**</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.real <span class="op">*</span> <span class="va">self</span>.dual)</span>
<span id="cb34-1225"><a href="#cb34-1225" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-1226"><a href="#cb34-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1227"><a href="#cb34-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1230"><a href="#cb34-1230" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb34-1231"><a href="#cb34-1231" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb34-1232"><a href="#cb34-1232" aria-hidden="true" tabindex="-1"></a><span class="co">#| </span></span>
<span id="cb34-1233"><a href="#cb34-1233" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> complex_function(x):</span>
<span id="cb34-1234"><a href="#cb34-1234" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.square() <span class="op">*</span> x.exp() <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>x</span>
<span id="cb34-1235"><a href="#cb34-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1236"><a href="#cb34-1236" aria-hidden="true" tabindex="-1"></a><span class="co"># Correcting the differentiation at x = 1</span></span>
<span id="cb34-1237"><a href="#cb34-1237" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> DualNumber(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb34-1238"><a href="#cb34-1238" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> complex_function(x)</span>
<span id="cb34-1239"><a href="#cb34-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1240"><a href="#cb34-1240" aria-hidden="true" tabindex="-1"></a>result.real, result.dual</span>
<span id="cb34-1241"><a href="#cb34-1241" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-1242"><a href="#cb34-1242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1243"><a href="#cb34-1243" aria-hidden="true" tabindex="-1"></a>Which is correct if we check using <span class="co">[</span><span class="ot">WolframAlpha</span><span class="co">]</span>(https://www.wolframalpha.com/input?i=what+is+the+derivative+of+x%5E2+*+exp(x)+++3x+at+x%3D1).</span>
<span id="cb34-1244"><a href="#cb34-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1245"><a href="#cb34-1245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1246"><a href="#cb34-1246" aria-hidden="true" tabindex="-1"></a><span class="fu">### Differentiating complex programs</span></span>
<span id="cb34-1247"><a href="#cb34-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1248"><a href="#cb34-1248" aria-hidden="true" tabindex="-1"></a>Autodiff, and thus differentiable programs, are now becoming a first-class citizen in programming languages---see, for example, the <span class="co">[</span><span class="ot">differentiable programming manifesto</span><span class="co">](https://github.com/apple/swift/blob/main/docs/DifferentiableProgramming.md)</span>.</span>
<span id="cb34-1249"><a href="#cb34-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1250"><a href="#cb34-1250" aria-hidden="true" tabindex="-1"></a>In the field of computational materials science a few nice examples include </span>
<span id="cb34-1251"><a href="#cb34-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-1252"><a href="#cb34-1252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">jax-md</span><span class="co">](https://github.com/jax-md/jax-md)</span>: Which allows one to differentia through full MD simulations, to do things like <span class="co">[</span><span class="ot">the design of kinetic pathways</span><span class="co">](https://www.pnas.org/doi/abs/10.1073/pnas.2024083118)</span></span>
<span id="cb34-1253"><a href="#cb34-1253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">optimization of a Hückel model implemented in jax</span><span class="co">](https://doi.org/10.1063/5.0137103)</span></span>
<span id="cb34-1254"><a href="#cb34-1254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">inverse design of pores</span><span class="co">](https://www.nature.com/articles/s41524-023-01080-x)</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">
        <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kevin-maik-jablonka/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/kmjablonka">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kjappelbaum">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://mailhide.io/e/o4LeOUlq">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://scholar.google.com/citations?user=R2ntI8IAAAAJ&amp;hl=en">
      <i class="bi bi-mortarboard-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
      </div>
  </div>
</footer>



</body></html>