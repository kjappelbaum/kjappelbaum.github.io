<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-05-02">
<meta name="description" content="Developing an understanding for how LLMs work.">

<title>Kevin’s Homepage - Building a GPT that can generate molecules from scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-S9W9LVHXJK"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-S9W9LVHXJK', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Kevin’s Homepage - Building a GPT that can generate molecules from scratch">
<meta property="og:description" content="<i>Developing an understanding for how LLMs work.</i>">
<meta property="og:image" content="">
<meta property="og:site-name" content="Kevin's Homepage">
<meta name="twitter:title" content="Kevin’s Homepage - Building a GPT that can generate molecules from scratch">
<meta name="twitter:description" content="<i>Developing an understanding for how LLMs work.</i>">
<meta name="twitter:image" content="">
<meta name="twitter:creator" content="@kmjablonka">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand navbar-dark ">
      <div class="navbar-container container-fluid">
          <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"><i class="bi bi-home" role="img">
</i> 
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../publications.html">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../cv.html">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../research.html">
 <span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../opensource/opensource.html">
 <span class="menu-text">Open Source</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../teaching.html">
 <span class="menu-text">Teaching</span></a>
  </li>  
</ul>
            <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#dealing-with-smiles" id="toc-dealing-with-smiles" class="nav-link active" data-scroll-target="#dealing-with-smiles">Dealing with SMILES</a>
  <ul class="collapse">
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization">Tokenization</a></li>
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">Embeddings</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding">Positional encoding</a></li>
  <li><a href="#language-modeling-dataset" id="toc-language-modeling-dataset" class="nav-link" data-scroll-target="#language-modeling-dataset">Language modeling dataset</a></li>
  </ul></li>
  <li><a href="#a-simple-bigram-model" id="toc-a-simple-bigram-model" class="nav-link" data-scroll-target="#a-simple-bigram-model">A simple bigram model</a>
  <ul class="collapse">
  <li><a href="#training-the-model" id="toc-training-the-model" class="nav-link" data-scroll-target="#training-the-model">Training the model</a></li>
  </ul></li>
  <li><a href="#making-tokens-talk-using-attention" id="toc-making-tokens-talk-using-attention" class="nav-link" data-scroll-target="#making-tokens-talk-using-attention">Making tokens talk using attention</a>
  <ul class="collapse">
  <li><a href="#refactoring-into-a-module" id="toc-refactoring-into-a-module" class="nav-link" data-scroll-target="#refactoring-into-a-module">Refactoring into a module</a></li>
  <li><a href="#revamped-bigram-model" id="toc-revamped-bigram-model" class="nav-link" data-scroll-target="#revamped-bigram-model">Revamped Bigram Model</a></li>
  </ul></li>
  <li><a href="#interlude-additional-perspectives-on-attention" id="toc-interlude-additional-perspectives-on-attention" class="nav-link" data-scroll-target="#interlude-additional-perspectives-on-attention">Interlude: Additional perspectives on attention</a>
  <ul class="collapse">
  <li><a href="#attention-as-gnn" id="toc-attention-as-gnn" class="nav-link" data-scroll-target="#attention-as-gnn">Attention as GNN</a></li>
  <li><a href="#attention-as-kernel-smoothing" id="toc-attention-as-kernel-smoothing" class="nav-link" data-scroll-target="#attention-as-kernel-smoothing">Attention as Kernel smoothing</a></li>
  </ul></li>
  <li><a href="#adding-more-expressive-power-with-more-heads-and-fully-connected-layers" id="toc-adding-more-expressive-power-with-more-heads-and-fully-connected-layers" class="nav-link" data-scroll-target="#adding-more-expressive-power-with-more-heads-and-fully-connected-layers">Adding more expressive power with more heads and fully connected layers</a></li>
  <li><a href="#abstracting-transformers-into-blocks" id="toc-abstracting-transformers-into-blocks" class="nav-link" data-scroll-target="#abstracting-transformers-into-blocks">Abstracting transformers into blocks</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.dev/kjappelbaum/kjappelbaum.github.io/blob/master/blog/posts/building_an_llm/index.ipynb" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Building a GPT that can generate molecules from scratch</h1>
  <div class="quarto-categories">
    <div class="quarto-category">machine-learning</div>
    <div class="quarto-category">llm</div>
  </div>
  </div>

<div>
  <div class="description">
    <i>Developing an understanding for how LLMs work.</i>
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 2, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Molecules can be represented in multitude of ways. One of the most widely used representations is to use text, for example in the so-called SMILES notation. In SMILES notation, a molecule is represented as a string of characters, where each character represents an atom or a bond. For example, the SMILES notation for ethanol is <code>CCO</code>. The one for benzene is <code>c1ccccc1</code>. You see that hydrogen atoms are typically omitted in SMILES notation, and that lower case letters are used for aromatic atoms. There is a <a href="http://opensmiles.org/opensmiles.html">full grammar for SMILES notation</a> and <a href="https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf">various alternative representations</a>, but we will stick to this simple version for this notebook.</p>
<p>Important problems that our final solution will need to be able to solve are:</p>
<ul>
<li>dealing with inputs of different lengths (e.g, different number of atoms in different molecules)</li>
<li>incorporating information about the semantic meaning of the atoms in the molecule (to obtain meaningful molecules, the model, e.g., should probably “know” what kind of bonds carbon can form)</li>
<li>dealing with the interaction between atoms in the molecule (not all arrangements of atoms are equally likely)</li>
</ul>
<div class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rdkit <span class="im">import</span> Chem</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> exp</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_num_parameters(model):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return the number of trainable parameters in the model."""</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_num_parameters_per_layer(model):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return the number of trainable parameters in the model per layer."""</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> {}</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, p <span class="kw">in</span> model.named_parameters():</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> p.requires_grad:</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            layers[name] <span class="op">=</span> p.numel()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> layers</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_device():</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> torch.backends.mps.is_built():</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> <span class="st">'mps'</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="st">'cuda'</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> <span class="st">'cpu'</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> device</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> set_device()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="dealing-with-smiles" class="level2">
<h2 class="anchored" data-anchor-id="dealing-with-smiles">Dealing with SMILES</h2>
<p>Before we can do anything, we need to obtain data. For doing so, we will need a dataset of SMILES strings. We will use the <a href="https://zinc.docking.org/">ZINC dataset</a> which is a public database of commercially-available compounds. We will use the <code>250k</code> subset of the dataset which contains 250,000 compounds.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget <span class="st">'https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/zinc15_250K_2D.tar.gz'</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>tar <span class="op">-</span>xzf zinc15_250K_2D.tar.gz</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>/Users/kevinmaikjablonka/.zshenv:.:1: no such file or directory: /Users/kevinmaikjablonka/.cargo/env
--2024-05-02 12:20:55--  https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/zinc15_250K_2D.tar.gz
Resolving deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)... 52.219.120.49, 52.219.120.145, 52.219.193.50, ...
Connecting to deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)|52.219.120.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 6941580 (6.6M) [application/x-gzip]
Saving to: ‘zinc15_250K_2D.tar.gz’

zinc15_250K_2D.tar. 100%[===================&gt;]   6.62M  1.25MB/s    in 14s     

2024-05-02 12:21:11 (497 KB/s) - ‘zinc15_250K_2D.tar.gz’ saved [6941580/6941580]

/Users/kevinmaikjablonka/.zshenv:.:1: no such file or directory: /Users/kevinmaikjablonka/.cargo/env</code></pre>
</div>
</div>
<p>After downloading and extracting the dataset, we can load it into memory and take a look at some molecules.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'zinc15_250K_2D.csv'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Chem.MolFromSmiles(df[<span class="st">'smiles'</span>][<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<p><img src="index_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Before we continue any further, we will also create train/valid and test sets.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>train, valid, test <span class="op">=</span> torch.utils.data.random_split(df[<span class="st">'smiles'</span>], [<span class="dv">200000</span>, <span class="dv">25000</span>, <span class="dv">25000</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<p>For training a language model, we will need to split the SMILES into tokens. Tokens are the smallest units of text that the model will work with. The model will learn to predict a molecule token by token. There is not one correct way to do this, but one very common way is to split the SMILES into “chemical tokens”. For this, <a href="https://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc02339e">Philippe Schwaller wrote down a regular expression</a>.</p>
<p>Commonly used other tokenization methods are:</p>
<ul>
<li><a href="https://github.com/google/sentencepiece">SentencePiece</a></li>
<li><a href="https://github.com/openai/tiktoken">Byte-Pair Encoding (BPE)</a></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some try to move completely away from tokenization and <a href="https://byte-gpt.github.io/">directly</a> <a href="https://www.youtube.com/watch?v=kcd0BTKJuXk">model</a> <a href="https://arxiv.org/abs/2105.13626">bytes</a>.</p>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(smiles: <span class="bu">str</span>) <span class="op">-&gt;</span> List[<span class="bu">str</span>]:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Tokenize a SMILES</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">        smiles (str): SMILES string</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">        List[str]: List of tokens</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    SMI_REGEX_PATTERN <span class="op">=</span> <span class="vs">r"""(\[[^\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;&gt;?|\*|\$|\%[0-9]</span><span class="sc">{2}</span><span class="vs">|[0-9])"""</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> re.findall(SMI_REGEX_PATTERN, smiles)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The molecule, CCO (ethanol), is tokenized as [‘C’, ‘C’, ‘O’].</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>tokenize(<span class="st">'CCO'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>['C', 'C', 'O']</code></pre>
</div>
</div>
<section id="converting-tokens-into-ids" class="level4">
<h4 class="anchored" data-anchor-id="converting-tokens-into-ids">Converting tokens into IDs</h4>
<p>For inputing tokens into a model, we will need to convert them into numbers.</p>
<p>To do so, we will set up a “vocabulary” which is a dictionary that maps tokens to integers. The vocabulary also defines the tokens that are known to the model.</p>
</section>
<section id="special-tokens" class="level4">
<h4 class="anchored" data-anchor-id="special-tokens">Special tokens</h4>
<p>Our model will be fed sequences of fixed length. Our SMILES, however, are of variable length. We will have to pad them to a fixed length. We will use a padding token for this purpose. That is, we will add a specific “[PAD]” token to the vocabulary which only serves the purpose of padding.</p>
<p>Often, we also add other tokens such as <code>[EOS]</code> (end of sequence) or <code>[BOS]</code> (beginning of sequence).</p>
<p>They are typically used as follows:</p>
<ul>
<li><code>[BOS]</code> is added at the beginning of each sequence</li>
<li><code>[EOS]</code> is added at the end of each sequence</li>
<li><code>[PAD]</code> is added to the end of each sequence to pad it to a fixed length</li>
<li><code>[UNK]</code> is used to replace tokens that are not in the vocabulary</li>
</ul>
<p>We can put all of this together in a <code>Tokenizer</code> class.</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Tokenizer:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokens: List[<span class="bu">str</span>], eos: <span class="bu">str</span> <span class="op">=</span> <span class="st">'[EOS]'</span>, bos: <span class="bu">str</span> <span class="op">=</span> <span class="st">'[BOS]'</span>, pad: <span class="bu">str</span> <span class="op">=</span> <span class="st">'[PAD]'</span>, unk: <span class="bu">str</span> <span class="op">=</span> <span class="st">'[UNK]'</span>):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokens <span class="op">=</span> [pad, bos, eos, unk] <span class="op">+</span> tokens</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._token_to_index <span class="op">=</span> {token: index <span class="cf">for</span> index, token <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.tokens)}</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.index_to_token <span class="op">=</span> {index: token <span class="cf">for</span> index, token <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.tokens)}</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> token_to_index(<span class="va">self</span>, token: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>._token_to_index[token]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>._token_to_index[<span class="st">'[UNK]'</span>]</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.tokens)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, item):</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.token_to_index[item]</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__contains__</span>(<span class="va">self</span>, item):</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item <span class="kw">in</span> <span class="va">self</span>.tokens</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, smiles: <span class="bu">str</span>, add_sos: <span class="bu">bool</span><span class="op">=</span><span class="va">False</span>, add_eos: <span class="bu">bool</span><span class="op">=</span><span class="va">False</span>) <span class="op">-&gt;</span> List[<span class="bu">int</span>]:</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co">        Encode a SMILES into a list of indices</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="co">            smiles (str): SMILES string</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co">            add_sos (bool): Add start of sentence token</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co">            add_eos (bool): Add end of sentence token</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co">            List[int]: List of indices</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> []</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> add_sos:</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>            tokens.append(<span class="va">self</span>.token_to_index(<span class="st">'[BOS]'</span>))</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">+=</span> [<span class="va">self</span>.token_to_index(token) <span class="cf">for</span> token <span class="kw">in</span> tokenize(smiles)]</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> add_eos:</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>            tokens.append(<span class="va">self</span>.token_to_index(<span class="st">'[EOS]'</span>))</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tokens</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, indices: List[<span class="bu">int</span>], strip_special_tokens: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>) <span class="op">-&gt;</span> <span class="bu">str</span>: </span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Decode a list of indices into a SMILES</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a><span class="co">            indices (List[int]): List of indices</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a><span class="co">            str: SMILES string</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>        decoded <span class="op">=</span> <span class="st">''</span>.join([<span class="va">self</span>.index_to_token[index] <span class="cf">for</span> index <span class="kw">in</span> indices])</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> strip_special_tokens:</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> decoded.replace(<span class="st">'[PAD]'</span>, <span class="st">''</span>).replace(<span class="st">'[BOS]'</span>, <span class="st">''</span>).replace(<span class="st">'[EOS]'</span>, <span class="st">''</span>)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> decoded</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To instantiate the tokenizer, we need to pass the list of tokens that we want to use. (This is sometimes called “training” the tokenizer, but in this case, we are just defining the tokens that we want to use.) We will use the following tokens:</p>
<div class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>lengths <span class="op">=</span> []</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> smiles <span class="kw">in</span> train.dataset.values:</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    tokens_ <span class="op">=</span> tokenize(smiles)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    tokens.update(tokens_)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    lengths.append(<span class="bu">len</span>(tokens_))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plt.hist(lengths, bins<span class="op">=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>(array([3.0000e+00, 4.0000e+00, 7.0000e+00, 0.0000e+00, 2.3000e+01,
        5.6000e+01, 0.0000e+00, 7.8000e+01, 2.0100e+02, 0.0000e+00,
        3.8900e+02, 8.0200e+02, 1.4320e+03, 0.0000e+00, 2.5760e+03,
        3.9450e+03, 0.0000e+00, 5.8570e+03, 8.0820e+03, 0.0000e+00,
        1.0313e+04, 1.2675e+04, 1.4914e+04, 0.0000e+00, 1.7137e+04,
        1.8718e+04, 0.0000e+00, 2.0510e+04, 2.0796e+04, 0.0000e+00,
        2.1073e+04, 2.0330e+04, 1.8396e+04, 0.0000e+00, 1.6193e+04,
        1.2172e+04, 0.0000e+00, 9.8210e+03, 5.8470e+03, 0.0000e+00,
        3.9460e+03, 2.1220e+03, 9.6800e+02, 0.0000e+00, 4.1200e+02,
        1.4500e+02, 0.0000e+00, 4.6000e+01, 1.0000e+01, 1.0000e+00]),
 array([17. , 17.7, 18.4, 19.1, 19.8, 20.5, 21.2, 21.9, 22.6, 23.3, 24. ,
        24.7, 25.4, 26.1, 26.8, 27.5, 28.2, 28.9, 29.6, 30.3, 31. , 31.7,
        32.4, 33.1, 33.8, 34.5, 35.2, 35.9, 36.6, 37.3, 38. , 38.7, 39.4,
        40.1, 40.8, 41.5, 42.2, 42.9, 43.6, 44.3, 45. , 45.7, 46.4, 47.1,
        47.8, 48.5, 49.2, 49.9, 50.6, 51.3, 52. ]),
 &lt;BarContainer object of 50 artists&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(<span class="bu">list</span>(tokens))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>tokenizer.encode(<span class="st">'CCO'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>[45, 45, 38]</code></pre>
</div>
</div>
</section>
</section>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">Embeddings</h3>
<p>Currently, we only encode the SMILES strings into a list of indices. There is no inherent meaning to the indices themselves, and we can improve modeling by representing each index as a vector. We call those vectors embeddings, but they are nothing more than a vector representation–like a feature vector–for each index.</p>
<p>Ideally, those vectors ensure that similar indices are close to each other in the embedding space. There are many ways to create those embeddings. But for now it is only important to know this concept.</p>
</section>
<section id="positional-encoding" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding">Positional encoding</h3>
<p>The embeddings we just created contain only information about their identity. However, they contain no information about their position in the sequence.</p>
<p>To add positional information, we can add a positional encoding to the embeddings. Again, there are many ways to do this.</p>
<p>A very simple way is called <em>absolute positional encoding</em>. For this we simply add the position index to the embedding vector.</p>
<p>For example</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>B, T, C <span class="op">=</span> <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span> <span class="co"># batch size, sequence length, embedding size</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(B, T, C)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> torch.arange(T).unsqueeze(<span class="dv">0</span>).repeat(B, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="language-modeling-dataset" class="level3">
<h3 class="anchored" data-anchor-id="language-modeling-dataset">Language modeling dataset</h3>
<p>A dataset class is a class that inherits from <code>torch.utils.data.Dataset</code>. It is used to load data into a model.</p>
<p>The most important methods of a dataset class are:</p>
<ul>
<li><code>__len__</code>: This method returns the length of the dataset. It is used by the <code>DataLoader</code> to determine how many batches to load.</li>
<li><code>__getitem__</code>: This method returns a single sample from the dataset. It is used by the <code>DataLoader</code> to load a batch of samples.</li>
</ul>
<div class="cell" data-execution_count="155">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalLanguageModelingDataset(Dataset):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, texts, tokenizer, max_length):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tokenizer <span class="op">=</span> tokenizer</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.texts <span class="op">=</span> texts</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_length <span class="op">=</span> max_length</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inputs <span class="op">=</span> []</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> []</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> text <span class="kw">in</span> texts:</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>            input_ids <span class="op">=</span> np.array(tokenizer.encode(text))</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># make next token the target create datasets with sliding windows</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(input_ids) <span class="op">&lt;</span> max_length:</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(input_ids)):</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.inputs.append(input_ids[:i])</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.targets.append([input_ids[i]])</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.inputs)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        input_ids <span class="op">=</span> <span class="va">self</span>.inputs[idx]</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        target_ids <span class="op">=</span> <span class="va">self</span>.targets[idx]</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Padding input and target</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        input_ids <span class="op">=</span> <span class="va">self</span>._pad(input_ids, <span class="va">self</span>.max_length)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        target_ids <span class="op">=</span> <span class="va">self</span>._pad(target_ids, <span class="va">self</span>.max_length, pad_value<span class="op">=-</span><span class="dv">100</span>)  <span class="co"># -100 is often used to ignore loss calculation</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>  torch.tensor(input_ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>), torch.tensor(target_ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pad(<span class="va">self</span>, sequence, max_len, pad_value<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>        padded_sequence <span class="op">=</span> np.full(max_len, pad_value)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        padded_sequence[:<span class="bu">len</span>(sequence)] <span class="op">=</span> sequence</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> padded_sequence</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You hopefully note something very interesting in this dataset: Based on one SMILES, we can create multiple training examples, because we can slide a window over the SMILES and predict the next token. (Note that our implementation is relatively naiive and is optimized to make this point clear. In practice, you should use dedicated methods, e.g., from the <code>transformers</code> library, to create language model datasets.)</p>
</section>
</section>
<section id="a-simple-bigram-model" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-bigram-model">A simple bigram model</h2>
<p>The simplest language model is a bigram model. In a bigram model, we predict the next token based on the previous token. A bigram model is the simplest form of <code>n-gram</code> model. In an <code>n-gram</code> model, we predict the next token based on the previous <code>n</code> tokens.</p>
<p><span class="math inline">\(N\)</span>-gram models are a simple but effective way to model language. The idea is to predict the next word in a sentence given the previous <span class="math inline">\(n-1\)</span> words. For example, in a 2-gram (bigram) model, we would predict the next word given only the previous word. In a 3-gram model, we would predict the next word given the previous two words. In general, we would predict the next word given the previous <span class="math inline">\(n-1\)</span> words.</p>
<p>Formally, we can write down the bigram model as follows:</p>
<p><span class="math display">\[
p(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
\]</span></p>
<p>where <span class="math inline">\(w_i\)</span> is the <span class="math inline">\(i\)</span>-th word in the sentence, <span class="math inline">\(C(w_{i-1}, w_i)\)</span> is the number of times the bigram <span class="math inline">\(w_{i-1}, w_i\)</span> occurs in the training set, and <span class="math inline">\(C(w_{i-1})\)</span> is the number of times the word <span class="math inline">\(w_{i-1}\)</span> occurs in the training set.</p>
<p>Since the bigram model only considers the previous word/token, we only need a lookup table.</p>
<p>Such lookup tables are implemented in PyTorch as <code>nn.Embedding</code>. Keep in mind that an embedding layer is nothing fancy. It works like inputting a one-hot encoded vector in a linear layer:</p>
<p><img src="https://pbs.twimg.com/media/FlzZE_dWIAEKnAW?format=jpg&amp;name=medium.png" class="img-fluid"></p>
<p>Using the <code>Embedding</code> layer, we can create a simple Bigram model.</p>
<div class="cell" data-execution_count="156">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BigramModel(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">40</span>):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># "learnable dictionary" that maps one token to another token</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mapping_layer <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the forward pass only consists of a lookup in the mapping layer</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mapping_layer(x)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y): </span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x has shape (batch_size, sequence_length)</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> predictions.shape</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predictions has shape (batch_size, sequence_length, vocab_size)</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> predictions.view(B<span class="op">*</span>T, C)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># y has the shape (batch_size, sequence_length)</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B<span class="op">*</span>T)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we use cross entropy loss to train the model</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.cross_entropy(predictions, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="157">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>bigram <span class="op">=</span> BigramModel(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Given a token ID, the model predict how likely each token of the vocabulary is to be the next. Right now, the model is not trained, so it will predict the next token randomly.</p>
<div class="cell" data-execution_count="158">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>F.softmax(bigram(torch.tensor([<span class="dv">1</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/ipykernel_51170/730608109.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  F.softmax(bigram(torch.tensor([1])))</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="158">
<pre><code>tensor([[0.1001, 0.0581, 0.2631, 0.0668, 0.0498, 0.0121, 0.0129, 0.1628, 0.0420,
         0.2324]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
</div>
</div>
<p>For generating a sequence, we can implement a <code>generate</code> method that iteratively predicts the next token and appends it to the sequence. We can then use this method to generate a sequence of a given length.</p>
<div class="cell" data-execution_count="159">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BigramModel(nn.Module):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># read of the logits of the next token from table</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mapping_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mapping_table(x) <span class="co"># returns tensor of shape (batch_size, time_steps, vocab_size)</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B<span class="op">*</span>T)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, x, max_new_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we generate max_new_tokens new tokens</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        new_tokens <span class="op">=</span> []</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _t <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># we only care about the last token in Bigram, hence we bow have shape (B, C)</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># we generate probabilities for the next token</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># torch.multinomial(probs, num_samples=1) returns a tensor of shape (B, 1) </span></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># where each element is the index of the sampled token</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>            new_tokens.append(next_token)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, next_token], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To evaluate the model performance, we will use the helper function below.</p>
<p>As performance metric we will use perplexity. Perplexity is a metric that measures how well a probability model predicts a sample. It is defined as <span class="math inline">\(2^H\)</span>, where <span class="math inline">\(H\)</span> is the cross entropy loss. The lower the perplexity, the better the model.</p>
<p>To better understand it, let’s recall a few things:</p>
<p>LLMs are trained to predict the probability of a word given the previous words. For instance, in the sentence “She went to the…”, the model predicts the probability of what the next word could be (e.g., store, park, etc.).</p>
<p><em>Cross entropy</em> is a measure of the difference between two probability distributions - in this case, the distribution predicted by the model and the actual distribution of words in the language. A lower cross-entropy means the model’s predictions are closer to the actual distribution. We can calculate it as follows:</p>
<p><span class="math display">\[H(p,q) = - \sum_{x} p(x) \log q(x)\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the actual distribution and <span class="math inline">\(q\)</span> is the predicted distribution.</p>
<p><em>Perplexity</em> can be thought of as the “effective number of choices” the model feels it has when making a prediction. A lower perplexity indicates that the model is more confident (or less “perplexed”) about its predictions.</p>
<p>For example, if a model has a perplexity of 10 on a dataset, it means that, on average, each time it tries to predict the next word, it’s as uncertain as if it were choosing uniformly and randomly among 10 options. If the perplexity is 100, it’s as uncertain as if it were choosing among 100 options, and so on.</p>
<p>You can find further information about such metrics <a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">here</a>.</p>
<div class="cell" data-execution_count="160">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_perplexity(model, data_loader):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the model to evaluation mode, i.e., </span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    total_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x, y <span class="kw">in</span> data_loader:</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(device)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.to(device)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> model.loss(x, y)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        total_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp(total_loss <span class="op">/</span> total_count)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="training-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model">Training the model</h3>
<p>To train the model, we will use a simple training loop and the Adam optimizer.</p>
<p>The role of the <code>Adam</code> optimizer is to update the parameters of the model using a technique called <a href="http://d2l.ai/chapter_optimization/minibatch-sgd.html">mini-batch stochastic gradient descent</a>. The idea is that we update the weights in the direction of the gradient of the loss function, which we estimate on a small batch of data. The learning rate controls how big the steps are that we take in the direction of the gradient.</p>
<p>Setting learning rate is not trivial, you can find more background <a href="https://www.jeremyjordan.me/nn-learning-rate/">here</a>.</p>
<p>It is import to remember to use the <code>zero_grad</code> function to clear the gradients before computing the gradients for the current batch. Also, remember to call <code>loss.backward()</code> to compute the gradients for the current batch.</p>
<p>For now, we will use a very simple approach (to reuse our old dataloader) and just predict the second token given the first one.</p>
<div class="cell" data-execution_count="161">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BigramModel(<span class="bu">len</span>(tokenizer))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="162">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> CausalLanguageModelingDataset(train, tokenizer, max_length<span class="op">=</span><span class="dv">40</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="163">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(train, tokenizer, max_length<span class="op">=</span><span class="dv">40</span>), batch_size<span class="op">=</span><span class="dv">2048</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>valid_loader <span class="op">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(valid, tokenizer, max_length<span class="op">=</span><span class="dv">40</span>), batch_size<span class="op">=</span><span class="dv">2048</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(test, tokenizer, max_length<span class="op">=</span><span class="dv">40</span>), batch_size<span class="op">=</span><span class="dv">2048</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="164">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, train_loader, val_loader, epochs, lr, eval_every<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set up the optimizer</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># start training</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the model to train mode </span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># iterate over the training data</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (x,y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># move the data to the device</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.to(device)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y.to(device)</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> model.loss(x,y)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># clear the gradients</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># compute the gradients</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update the parameters</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># print the loss every eval_every iterations</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">%</span> eval_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, iter </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, train loss </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.3f}</span><span class="ss">, val perplexity </span><span class="sc">{</span>estimate_perplexity(model, val_loader)<span class="sc">:.5f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="166">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, valid_loader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">1e-3</span>, eval_every<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, iter 0, train loss 2.423, val perplexity 11.27392
Epoch 0, iter 100, train loss 2.423, val perplexity 11.24984
Epoch 0, iter 200, train loss 2.450, val perplexity 11.23007
Epoch 0, iter 300, train loss 2.451, val perplexity 11.21351
Epoch 0, iter 400, train loss 2.409, val perplexity 11.19960
Epoch 0, iter 500, train loss 2.403, val perplexity 11.18782
Epoch 0, iter 600, train loss 2.439, val perplexity 11.17740
Epoch 0, iter 700, train loss 2.435, val perplexity 11.16857
Epoch 0, iter 800, train loss 2.432, val perplexity 11.16064
Epoch 0, iter 900, train loss 2.401, val perplexity 11.15376
Epoch 0, iter 1000, train loss 2.436, val perplexity 11.14751
Epoch 0, iter 1100, train loss 2.424, val perplexity 11.14243
Epoch 0, iter 1200, train loss 2.365, val perplexity 11.13720
Epoch 0, iter 1300, train loss 2.430, val perplexity 11.13270
Epoch 0, iter 1400, train loss 2.396, val perplexity 11.12861
Epoch 0, iter 1500, train loss 2.396, val perplexity 11.12477
Epoch 0, iter 1600, train loss 2.427, val perplexity 11.12151
Epoch 0, iter 1700, train loss 2.426, val perplexity 11.11855
Epoch 0, iter 1800, train loss 2.407, val perplexity 11.11576
Epoch 0, iter 1900, train loss 2.411, val perplexity 11.11314
Epoch 0, iter 2000, train loss 2.409, val perplexity 11.11071
Epoch 0, iter 2100, train loss 2.377, val perplexity 11.10836
Epoch 0, iter 2200, train loss 2.414, val perplexity 11.10666
Epoch 0, iter 2300, train loss 2.357, val perplexity 11.10495
Epoch 1, iter 0, train loss 2.400, val perplexity 11.10374
Epoch 1, iter 100, train loss 2.430, val perplexity 11.10187
Epoch 1, iter 200, train loss 2.392, val perplexity 11.10039
Epoch 1, iter 300, train loss 2.418, val perplexity 11.09890
Epoch 1, iter 400, train loss 2.378, val perplexity 11.09767
Epoch 1, iter 500, train loss 2.423, val perplexity 11.09650
Epoch 1, iter 600, train loss 2.390, val perplexity 11.09533
Epoch 1, iter 700, train loss 2.440, val perplexity 11.09407
Epoch 1, iter 800, train loss 2.397, val perplexity 11.09331
Epoch 1, iter 900, train loss 2.408, val perplexity 11.09244
Epoch 1, iter 1000, train loss 2.397, val perplexity 11.09115
Epoch 1, iter 1100, train loss 2.422, val perplexity 11.09047
Epoch 1, iter 1200, train loss 2.382, val perplexity 11.08941
Epoch 1, iter 1300, train loss 2.411, val perplexity 11.08896
Epoch 1, iter 1400, train loss 2.390, val perplexity 11.08832
Epoch 1, iter 1500, train loss 2.410, val perplexity 11.08726
Epoch 1, iter 1600, train loss 2.423, val perplexity 11.08663
Epoch 1, iter 1700, train loss 2.394, val perplexity 11.08610
Epoch 1, iter 1800, train loss 2.411, val perplexity 11.08553
Epoch 1, iter 1900, train loss 2.418, val perplexity 11.08504
Epoch 1, iter 2000, train loss 2.438, val perplexity 11.08430
Epoch 1, iter 2100, train loss 2.403, val perplexity 11.08389
Epoch 1, iter 2200, train loss 2.378, val perplexity 11.08335
Epoch 1, iter 2300, train loss 2.429, val perplexity 11.08304
Epoch 2, iter 0, train loss 2.378, val perplexity 11.08273
Epoch 2, iter 100, train loss 2.423, val perplexity 11.08241
Epoch 2, iter 200, train loss 2.419, val perplexity 11.08202
Epoch 2, iter 300, train loss 2.398, val perplexity 11.08176
Epoch 2, iter 400, train loss 2.420, val perplexity 11.08122
Epoch 2, iter 500, train loss 2.402, val perplexity 11.08099
Epoch 2, iter 600, train loss 2.436, val perplexity 11.08070
Epoch 2, iter 700, train loss 2.400, val perplexity 11.08106
Epoch 2, iter 800, train loss 2.416, val perplexity 11.08027
Epoch 2, iter 900, train loss 2.390, val perplexity 11.07992
Epoch 2, iter 1000, train loss 2.407, val perplexity 11.07969
Epoch 2, iter 1100, train loss 2.386, val perplexity 11.07925
Epoch 2, iter 1200, train loss 2.395, val perplexity 11.07893
Epoch 2, iter 1300, train loss 2.376, val perplexity 11.07876
Epoch 2, iter 1400, train loss 2.401, val perplexity 11.07871
Epoch 2, iter 1500, train loss 2.429, val perplexity 11.07836
Epoch 2, iter 1600, train loss 2.416, val perplexity 11.07817
Epoch 2, iter 1700, train loss 2.439, val perplexity 11.07803
Epoch 2, iter 1800, train loss 2.391, val perplexity 11.07777
Epoch 2, iter 1900, train loss 2.381, val perplexity 11.07781
Epoch 2, iter 2000, train loss 2.407, val perplexity 11.07774
Epoch 2, iter 2100, train loss 2.435, val perplexity 11.07758
Epoch 2, iter 2200, train loss 2.383, val perplexity 11.07733
Epoch 2, iter 2300, train loss 2.400, val perplexity 11.07734
Epoch 3, iter 0, train loss 2.390, val perplexity 11.07707
Epoch 3, iter 100, train loss 2.398, val perplexity 11.07696
Epoch 3, iter 200, train loss 2.441, val perplexity 11.07689
Epoch 3, iter 300, train loss 2.399, val perplexity 11.07688
Epoch 3, iter 400, train loss 2.420, val perplexity 11.07665
Epoch 3, iter 500, train loss 2.395, val perplexity 11.07644
Epoch 3, iter 600, train loss 2.426, val perplexity 11.07636
Epoch 3, iter 700, train loss 2.449, val perplexity 11.07612
Epoch 3, iter 800, train loss 2.417, val perplexity 11.07598
Epoch 3, iter 900, train loss 2.389, val perplexity 11.07603
Epoch 3, iter 1000, train loss 2.426, val perplexity 11.07601
Epoch 3, iter 1100, train loss 2.423, val perplexity 11.07572
Epoch 3, iter 1200, train loss 2.434, val perplexity 11.07570
Epoch 3, iter 1300, train loss 2.377, val perplexity 11.07568
Epoch 3, iter 1400, train loss 2.378, val perplexity 11.07571
Epoch 3, iter 1500, train loss 2.435, val perplexity 11.07559
Epoch 3, iter 1600, train loss 2.398, val perplexity 11.07552
Epoch 3, iter 1700, train loss 2.414, val perplexity 11.07518
Epoch 3, iter 1800, train loss 2.446, val perplexity 11.07560
Epoch 3, iter 1900, train loss 2.428, val perplexity 11.07523
Epoch 3, iter 2000, train loss 2.366, val perplexity 11.07503
Epoch 3, iter 2100, train loss 2.383, val perplexity 11.07526
Epoch 3, iter 2200, train loss 2.390, val perplexity 11.07514
Epoch 3, iter 2300, train loss 2.402, val perplexity 11.07514
Epoch 4, iter 0, train loss 2.370, val perplexity 11.07506
Epoch 4, iter 100, train loss 2.403, val perplexity 11.07501
Epoch 4, iter 200, train loss 2.414, val perplexity 11.07492
Epoch 4, iter 300, train loss 2.422, val perplexity 11.07469
Epoch 4, iter 400, train loss 2.414, val perplexity 11.07477
Epoch 4, iter 500, train loss 2.403, val perplexity 11.07465
Epoch 4, iter 600, train loss 2.389, val perplexity 11.07464
Epoch 4, iter 700, train loss 2.437, val perplexity 11.07466
Epoch 4, iter 800, train loss 2.404, val perplexity 11.07462
Epoch 4, iter 900, train loss 2.418, val perplexity 11.07511
Epoch 4, iter 1000, train loss 2.412, val perplexity 11.07476
Epoch 4, iter 1100, train loss 2.438, val perplexity 11.07460
Epoch 4, iter 1200, train loss 2.395, val perplexity 11.07443
Epoch 4, iter 1300, train loss 2.434, val perplexity 11.07437
Epoch 4, iter 1400, train loss 2.412, val perplexity 11.07436
Epoch 4, iter 1500, train loss 2.421, val perplexity 11.07439
Epoch 4, iter 1600, train loss 2.369, val perplexity 11.07436
Epoch 4, iter 1700, train loss 2.418, val perplexity 11.07475
Epoch 4, iter 1800, train loss 2.354, val perplexity 11.07431
Epoch 4, iter 1900, train loss 2.388, val perplexity 11.07455
Epoch 4, iter 2000, train loss 2.445, val perplexity 11.07442
Epoch 4, iter 2100, train loss 2.393, val perplexity 11.07426
Epoch 4, iter 2200, train loss 2.396, val perplexity 11.07447
Epoch 4, iter 2300, train loss 2.394, val perplexity 11.07444
Epoch 5, iter 0, train loss 2.401, val perplexity 11.07427
Epoch 5, iter 100, train loss 2.397, val perplexity 11.07452
Epoch 5, iter 200, train loss 2.377, val perplexity 11.07429
Epoch 5, iter 300, train loss 2.393, val perplexity 11.07454
Epoch 5, iter 400, train loss 2.428, val perplexity 11.07433
Epoch 5, iter 500, train loss 2.438, val perplexity 11.07406
Epoch 5, iter 600, train loss 2.416, val perplexity 11.07414
Epoch 5, iter 700, train loss 2.421, val perplexity 11.07419
Epoch 5, iter 800, train loss 2.414, val perplexity 11.07401
Epoch 5, iter 900, train loss 2.415, val perplexity 11.07403
Epoch 5, iter 1000, train loss 2.387, val perplexity 11.07414
Epoch 5, iter 1100, train loss 2.404, val perplexity 11.07418
Epoch 5, iter 1200, train loss 2.386, val perplexity 11.07427
Epoch 5, iter 1300, train loss 2.408, val perplexity 11.07411
Epoch 5, iter 1400, train loss 2.402, val perplexity 11.07427
Epoch 5, iter 1500, train loss 2.388, val perplexity 11.07399
Epoch 5, iter 1600, train loss 2.361, val perplexity 11.07405
Epoch 5, iter 1700, train loss 2.415, val perplexity 11.07392
Epoch 5, iter 1800, train loss 2.406, val perplexity 11.07400
Epoch 5, iter 1900, train loss 2.363, val perplexity 11.07389
Epoch 5, iter 2000, train loss 2.409, val perplexity 11.07398
Epoch 5, iter 2100, train loss 2.419, val perplexity 11.07407
Epoch 5, iter 2200, train loss 2.401, val perplexity 11.07381
Epoch 5, iter 2300, train loss 2.443, val perplexity 11.07387
Epoch 6, iter 0, train loss 2.394, val perplexity 11.07407
Epoch 6, iter 100, train loss 2.399, val perplexity 11.07425
Epoch 6, iter 200, train loss 2.425, val perplexity 11.07410
Epoch 6, iter 300, train loss 2.404, val perplexity 11.07376
Epoch 6, iter 400, train loss 2.395, val perplexity 11.07368
Epoch 6, iter 500, train loss 2.393, val perplexity 11.07375
Epoch 6, iter 600, train loss 2.393, val perplexity 11.07386
Epoch 6, iter 700, train loss 2.401, val perplexity 11.07404
Epoch 6, iter 800, train loss 2.405, val perplexity 11.07393
Epoch 6, iter 900, train loss 2.398, val perplexity 11.07418
Epoch 6, iter 1000, train loss 2.397, val perplexity 11.07399
Epoch 6, iter 1100, train loss 2.422, val perplexity 11.07403
Epoch 6, iter 1200, train loss 2.398, val perplexity 11.07393
Epoch 6, iter 1300, train loss 2.406, val perplexity 11.07402
Epoch 6, iter 1400, train loss 2.392, val perplexity 11.07403
Epoch 6, iter 1500, train loss 2.394, val perplexity 11.07424
Epoch 6, iter 1600, train loss 2.396, val perplexity 11.07407
Epoch 6, iter 1700, train loss 2.406, val perplexity 11.07427
Epoch 6, iter 1800, train loss 2.396, val perplexity 11.07393
Epoch 6, iter 1900, train loss 2.424, val perplexity 11.07386
Epoch 6, iter 2000, train loss 2.421, val perplexity 11.07397
Epoch 6, iter 2100, train loss 2.429, val perplexity 11.07394
Epoch 6, iter 2200, train loss 2.393, val perplexity 11.07407
Epoch 6, iter 2300, train loss 2.417, val perplexity 11.07399
Epoch 7, iter 0, train loss 2.407, val perplexity 11.07377
Epoch 7, iter 100, train loss 2.394, val perplexity 11.07395
Epoch 7, iter 200, train loss 2.394, val perplexity 11.07392
Epoch 7, iter 300, train loss 2.383, val perplexity 11.07387
Epoch 7, iter 400, train loss 2.433, val perplexity 11.07381
Epoch 7, iter 500, train loss 2.418, val perplexity 11.07386
Epoch 7, iter 600, train loss 2.401, val perplexity 11.07383
Epoch 7, iter 700, train loss 2.400, val perplexity 11.07368
Epoch 7, iter 800, train loss 2.411, val perplexity 11.07384
Epoch 7, iter 900, train loss 2.410, val perplexity 11.07378
Epoch 7, iter 1000, train loss 2.439, val perplexity 11.07400
Epoch 7, iter 1100, train loss 2.371, val perplexity 11.07381
Epoch 7, iter 1200, train loss 2.412, val perplexity 11.07374
Epoch 7, iter 1300, train loss 2.441, val perplexity 11.07380
Epoch 7, iter 1400, train loss 2.392, val perplexity 11.07385
Epoch 7, iter 1500, train loss 2.405, val perplexity 11.07381
Epoch 7, iter 1600, train loss 2.407, val perplexity 11.07369
Epoch 7, iter 1700, train loss 2.383, val perplexity 11.07384
Epoch 7, iter 1800, train loss 2.426, val perplexity 11.07394
Epoch 7, iter 1900, train loss 2.443, val perplexity 11.07384
Epoch 7, iter 2000, train loss 2.376, val perplexity 11.07398
Epoch 7, iter 2100, train loss 2.409, val perplexity 11.07385
Epoch 7, iter 2200, train loss 2.367, val perplexity 11.07380
Epoch 7, iter 2300, train loss 2.399, val perplexity 11.07367
Epoch 8, iter 0, train loss 2.386, val perplexity 11.07387
Epoch 8, iter 100, train loss 2.381, val perplexity 11.07374
Epoch 8, iter 200, train loss 2.396, val perplexity 11.07371
Epoch 8, iter 300, train loss 2.392, val perplexity 11.07380
Epoch 8, iter 400, train loss 2.420, val perplexity 11.07377
Epoch 8, iter 500, train loss 2.441, val perplexity 11.07406
Epoch 8, iter 600, train loss 2.419, val perplexity 11.07404
Epoch 8, iter 700, train loss 2.389, val perplexity 11.07385
Epoch 8, iter 800, train loss 2.408, val perplexity 11.07389
Epoch 8, iter 900, train loss 2.401, val perplexity 11.07368
Epoch 8, iter 1000, train loss 2.414, val perplexity 11.07393
Epoch 8, iter 1100, train loss 2.361, val perplexity 11.07391
Epoch 8, iter 1200, train loss 2.418, val perplexity 11.07373
Epoch 8, iter 1300, train loss 2.409, val perplexity 11.07353
Epoch 8, iter 1400, train loss 2.390, val perplexity 11.07358
Epoch 8, iter 1500, train loss 2.424, val perplexity 11.07368
Epoch 8, iter 1600, train loss 2.410, val perplexity 11.07357
Epoch 8, iter 1700, train loss 2.414, val perplexity 11.07377
Epoch 8, iter 1800, train loss 2.395, val perplexity 11.07365
Epoch 8, iter 1900, train loss 2.383, val perplexity 11.07354
Epoch 8, iter 2000, train loss 2.423, val perplexity 11.07377
Epoch 8, iter 2100, train loss 2.426, val perplexity 11.07397
Epoch 8, iter 2200, train loss 2.438, val perplexity 11.07389
Epoch 8, iter 2300, train loss 2.393, val perplexity 11.07382
Epoch 9, iter 0, train loss 2.384, val perplexity 11.07388
Epoch 9, iter 100, train loss 2.395, val perplexity 11.07372
Epoch 9, iter 200, train loss 2.444, val perplexity 11.07394
Epoch 9, iter 300, train loss 2.444, val perplexity 11.07369
Epoch 9, iter 400, train loss 2.405, val perplexity 11.07371
Epoch 9, iter 500, train loss 2.368, val perplexity 11.07369
Epoch 9, iter 600, train loss 2.388, val perplexity 11.07378
Epoch 9, iter 700, train loss 2.405, val perplexity 11.07407
Epoch 9, iter 800, train loss 2.410, val perplexity 11.07403
Epoch 9, iter 900, train loss 2.427, val perplexity 11.07391
Epoch 9, iter 1000, train loss 2.395, val perplexity 11.07363
Epoch 9, iter 1100, train loss 2.413, val perplexity 11.07368
Epoch 9, iter 1200, train loss 2.388, val perplexity 11.07374
Epoch 9, iter 1300, train loss 2.412, val perplexity 11.07383
Epoch 9, iter 1400, train loss 2.438, val perplexity 11.07379
Epoch 9, iter 1500, train loss 2.410, val perplexity 11.07394
Epoch 9, iter 1600, train loss 2.397, val perplexity 11.07370
Epoch 9, iter 1700, train loss 2.442, val perplexity 11.07380
Epoch 9, iter 1800, train loss 2.396, val perplexity 11.07368
Epoch 9, iter 1900, train loss 2.428, val perplexity 11.07367
Epoch 9, iter 2000, train loss 2.430, val perplexity 11.07382
Epoch 9, iter 2100, train loss 2.363, val perplexity 11.07376
Epoch 9, iter 2200, train loss 2.391, val perplexity 11.07373
Epoch 9, iter 2300, train loss 2.406, val perplexity 11.07385</code></pre>
</div>
</div>
<p>We can now test the model by generating new SMILES strings. We will start with a random token and generate 100 new tokens.</p>
<div class="cell" data-execution_count="168">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([[<span class="dv">4</span>]])</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> a.to(device)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>generation <span class="op">=</span> model.generate(a, max_new_tokens<span class="op">=</span><span class="dv">10</span>).cpu().numpy()</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>tokenizer.decode(generation[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="168">
<pre><code>'[C@@][C@]Cl)O(BCO1'</code></pre>
</div>
</div>
<p>This does not yet look like a valid SMILES string …</p>
</section>
</section>
<section id="making-tokens-talk-using-attention" class="level2">
<h2 class="anchored" data-anchor-id="making-tokens-talk-using-attention">Making tokens talk using attention</h2>
<p>In our bigram models we made predictions based on the previous word. This is clearly not enough to make good predictions. We can improve our model by taking into more past tokens into account.</p>
<p>One naïve way to incorporate more context into our model might be to simply “pool” (features of) the preceding tokens. This kind of pooling is similar to what we do in GNNs, e.g., to combine node embeddings.</p>
<p>A very simple pooling operation is the average of the embeddings of the preceding tokens. Later, when we will implement self-attention, we will not use a simple average, but a special weighted average. The code for that will use similar ideas (in particular, the causal mask).</p>
<div class="cell" data-execution_count="167">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>B, T, C <span class="op">=</span> <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span> <span class="co"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create random data of shape (B, T, C)</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>x_bag_of_words <span class="op">=</span> torch.zeros((B,T,C))</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        x_prev <span class="op">=</span> x[b,:t<span class="op">+</span><span class="dv">1</span>] <span class="co"># shape (t, C)</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        x_bag_of_words[b, t] <span class="op">=</span> torch.mean(x_prev, dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># shape (C,)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This nested for loop is slow. However, we can implement this in an efficient way if we observe a few things:</p>
<ul>
<li><p>If we want to predict next tokens, we do not want to let the future tokens influence the prediction. Therefore, we can use a so-called causal mask to mask out the future tokens.</p></li>
<li><p>A matrix multiplication can be thought of as a weighted sum of the rows of the matrix, where the weights are given by the columns of the matrix. This is easy to see if we think of the following extremes:</p>
<ul>
<li>We can compute the sum of the rows of a matrix by multiplying the matrix with a lower-triangular matrix filled with ones.</li>
<li>We can compute the mean of the rows of a matrix by multiplying the matrix with a lower-triangular matrix filled with ones and dividing by the number of ones in the lower-triangular matrix.</li>
</ul></li>
</ul>
<p>In <code>torch</code> we can use <code>tril</code> to create a lower-triangular matrix.</p>
<div class="cell" data-execution_count="169">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>lower_triangular_mask <span class="op">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> torch.ones((T,T))</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> torch.masked_fill(weight, lower_triangular_mask<span class="op">==</span><span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> torch.softmax(weight, dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="170">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>weight  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="170">
<pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],
        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])</code></pre>
</div>
</div>
<p>We used the softmax function to normalize the weights in the rows.</p>
<div class="cell" data-execution_count="171">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>weight <span class="op">@</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="171">
<pre><code>tensor([[[-0.6594,  0.1502, -0.2865],
         [-0.5213, -0.4364,  0.4502],
         [-0.3585, -0.1425,  0.5507],
         [-0.0764,  0.1243,  0.6123],
         [-0.1630,  0.0269,  0.5489]],

        [[-0.6673,  0.6167, -0.9546],
         [-0.3099, -0.0206,  0.3154],
         [ 0.1767, -0.0318,  0.3009],
         [ 0.4085, -0.2151,  0.5611],
         [ 0.4605, -0.4176,  0.5351]]])</code></pre>
</div>
</div>
<p>In the simple average we used above, all past tokens were treated equally. However, it might be useful to <em>pay more attention</em> to certain tokens than to others. That is, we want to gather information from the past – but do this in a data-dependent way. The attention mechanism allows us to do this.</p>
<p>The attention mechanism does this by having a query vector <span class="math inline">\(q\)</span> and a key vector <span class="math inline">\(k\)</span> for each token. We then define “similarity” or “relevance” between two tokens <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> as the dot product between their query and key vectors, which we derive from the embeddings of the tokens by multiplying them with the learnable weight matrices <span class="math inline">\(W_q\)</span> and <span class="math inline">\(W_k\)</span>.</p>
<p><span class="math display">\[
\text{sim}(i, j) = a(i, h) = q_ik_j^T = \text{emb}_i W_q W_k^T \text{emb}_j^T
\]</span></p>
<p>Note that this gives us now a way to refine the <code>weight_matrix</code> we used above. Instead of weighting all tokens equally, we can now learn a weight matrix that tells us how much attention to pay to each token.</p>
<p>To start the implementation, we will first derive query and key vectors from the embeddings. We will then compute the similarity matrix and apply the softmax function to normalize the weights.</p>
<div class="cell" data-execution_count="172">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>B, T, C <span class="op">=</span> <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span> <span class="co"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>head_size <span class="op">=</span> <span class="dv">16</span> <span class="co"># hyperparameter</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># with bias = False, it only perform matrix multiplication</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>key_layer <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)  </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>query_layer <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The attention matrix defined above is now a simple matrix multiplication between the query and key vectors. The attention matrix is then normalized using a softmax function.</p>
<div class="cell" data-execution_count="174">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> query_layer(x) <span class="co"># shape (B, T, head_size)</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> key_layer(x) <span class="co"># shape (B, T, head_size)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="182">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> query <span class="op">@</span> key.transpose(<span class="dv">1</span>,<span class="dv">2</span>) <span class="co"># shape (B, T, T)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that the shape of the attention matrix is (B, T, T). The attention matrix is a matrix where each row corresponds to a query and each column corresponds to a key. The value at position (i, j) in the attention matrix is the attention score between the i-th query and the j-th key.</p>
<div class="cell" data-execution_count="183">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="183">
<pre><code>tensor([[[ 0.3521, -0.4349,  1.3019, -1.4227, -0.9973],
         [-0.3857, -0.7338,  0.5761, -1.0932, -0.8915],
         [-0.1943, -0.5278,  0.5179, -0.8923, -0.6988],
         [-1.1021, -0.8737, -0.3723, -0.4446, -0.5439],
         [-0.5155, -0.5618,  0.0866, -0.5447, -0.5076]],

        [[-1.3238,  0.7295, -0.1932, -0.3536,  0.1224],
         [ 1.2033, -0.5237, -0.4037, -0.9075, -0.6969],
         [ 0.6911, -0.5819,  0.6078,  1.0999,  0.4158],
         [ 2.9120, -1.8470,  0.5745,  0.6777, -0.2034],
         [ 1.2999, -0.8820,  0.5131,  0.8557,  0.1704]]],
       grad_fn=&lt;UnsafeViewBackward0&gt;)</code></pre>
</div>
</div>
<p>But to avoid the future tokens to influence the prediction, we will use a causal mask. We do this the same way as we did above, by using <code>torch.tril</code>.</p>
<div class="cell" data-execution_count="184">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>lower_triangular_mask <span class="op">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> torch.masked_fill(attention, lower_triangular_mask<span class="op">==</span><span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))   </span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> torch.softmax(attention, dim<span class="op">=</span><span class="dv">2</span>) <span class="co"># shape (B, T, T), softmax along the last dimension</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> attention <span class="op">@</span> x <span class="co"># shape (B, T, T) @ (B, T, C) = (B, T, C)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the attention mechanism popularized in the <a href="https://arxiv.org/abs/1706.03762">“attention is all you need” paper</a> we add even more expressive power by transforming <code>x</code> before we multiply it with the attention matrix. We call this transformed <code>x</code> the value vector (or matrix). The full implementation of the attention mechanism is then:</p>
<div class="cell" data-execution_count="186">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>B, T, C <span class="op">=</span> <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span> <span class="co"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>head_size <span class="op">=</span> <span class="dv">16</span> <span class="co"># hyperparameter</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="co"># what do I contain</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co"># with bias = False, it only perform matrix multiplication</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co"># what am I looking for</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="co"># what I will tell you</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>) <span class="co"># Output: (B, T, head_size)</span></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="co"># self-attention because k, q, v come all from the same input</span></span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> key(x) <span class="co"># shape (B, T, head_size)</span></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> query(x) <span class="co"># shape (B, T, head_size)</span></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> value(x) <span class="co"># shape (B, T, head_size)</span></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a><span class="co"># now, we want to compute the attention</span></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to compute the dot product between k and q</span></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>weight_matrix <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a><span class="co"># now we add the masking</span></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a><span class="co"># we want to mask out the future</span></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a><span class="co"># this is what is known as "decoder" block </span></span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>lower_triangular <span class="op">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>weight_matrix <span class="op">=</span> weight_matrix.masked_fill(lower_triangular<span class="op">==</span><span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a><span class="co"># use softmax to normalize</span></span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>weight_matrix <span class="op">=</span> torch.softmax(weight_matrix, dim<span class="op">=-</span><span class="dv">1</span>)<span class="op">/</span>np.sqrt(head_size) <span class="co"># shape (B, T, T)</span></span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> weight_matrix <span class="op">@</span> v <span class="co"># shape (B, T, T) @ (B, T, head_size) = (B, T, head_size)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="interlude-why-do-we-divide-by-sqrthead_size-in-the-self-attention-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="interlude-why-do-we-divide-by-sqrthead_size-in-the-self-attention-mechanism">Interlude: Why do we divide by sqrt(head_size) in the self-attention mechanism?</h4>
<p>We used one more trick to make the training more stable. We scaled the weight_matrix by the square root of the head_size. <a href="https://ai.stackexchange.com/questions/21237/why-does-this-multiplication-of-q-and-k-have-a-variance-of-d-k-in-scaled">This is because the variance of the dot product is proportional to the dimensionality of the vectors.</a>. Not scaling the weight matrix can lead to numerical instability.</p>
<p>To see this, let’s run a quick experiment</p>
<div class="cell" data-execution_count="190">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>variances <span class="op">=</span> []</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>dimensions <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">100000</span>]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> dimensions:</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> torch.randn(B, T, d)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> torch.randn(B, T, d)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute the batched matrix product between k and q</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    weight_matrix <span class="op">=</span> torch.bmm(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))   <span class="co"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    variances.append(weight_matrix.var())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="192">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>plt.plot(dimensions, variances)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimensionality'</span>)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Variance'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="192">
<pre><code>Text(0, 0.5, 'Variance')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-38-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>This has an important impact when we apply <code>softmax</code>. Positive and negative “outliers” will be “sequeezed” to 1 and 0. You can test this by creating a 1D tensor (<code>a</code>) and applying softmax on it. Then multiply the values in the tensor (<code>a</code>) and again apply softmax.</p>
<div class="cell" data-execution_count="195">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(F.softmax(torch.tensor([<span class="fl">1.</span>,<span class="fl">2.</span>,<span class="fl">3.</span>])),F.softmax(torch.tensor([<span class="fl">1.</span>,<span class="fl">2.</span>,<span class="fl">3.</span>])<span class="op">*</span><span class="dv">100</span>) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.0900, 0.2447, 0.6652]) tensor([0.0000e+00, 3.7835e-44, 1.0000e+00])</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/ipykernel_51170/1895642280.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  print(F.softmax(torch.tensor([1.,2.,3.])),F.softmax(torch.tensor([1.,2.,3.])*100) )</code></pre>
</div>
</div>
</section>
<section id="the-attention-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="the-attention-mechanism">The attention mechanism</h4>
<p>Written as a formula, the attention mechanism is:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(Q\)</span> is the query matrix, <span class="math inline">\(K\)</span> is the key matrix, and <span class="math inline">\(V\)</span> is the value matrix.</p>
</section>
<section id="refactoring-into-a-module" class="level3">
<h3 class="anchored" data-anchor-id="refactoring-into-a-module">Refactoring into a module</h3>
<div class="cell" data-execution_count="202">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embed, block_size, head_size):</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embed, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embed, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embed, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'lower_triangular'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x): </span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>        B, T, C  <span class="op">=</span> x.shape</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>        key <span class="op">=</span> <span class="va">self</span>.key(x)</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        query <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># B, T, head</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">self</span>.value(x)   <span class="co"># B, T, head</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>        weight_matrix <span class="op">=</span> query <span class="op">@</span> key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C <span class="op">**</span> (<span class="op">-</span><span class="fl">0.5</span>) <span class="co"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>        weight_matrix <span class="op">=</span> weight_matrix.masked_fill(<span class="va">self</span>.lower_triangular[:T, :T].logical_not(), <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>        weight_matrix <span class="op">=</span> F.softmax(weight_matrix, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> weight_matrix <span class="op">@</span> value <span class="co"># shape (B, T, T) @ (B, T, head_size) = (B, T, head_size)</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="revamped-bigram-model" class="level3">
<h3 class="anchored" data-anchor-id="revamped-bigram-model">Revamped Bigram Model</h3>
<p>Now, we can use it to “refine” our bigram model. We will additionally also perform two more changes:</p>
<ul>
<li>we will add positional embeddings: We will add the positional embeddings to the input embeddings. This will allow the model to take into account the position of the tokens in the sequence.</li>
<li>we will add one more indirection: One simple way of improving the expressiveness is to add one linear layer. While in the bigram model we only had one embedding layer (that mapped inputs of size <code>vocab_size</code> to <code>vocab_size</code>), we can now change the embedding layer to map inputs of size <code>vocab_size</code> to <code>embedding_size</code>. We can then add a linear layer that maps inputs of size <code>embedding_size</code> to <code>vocab_size</code>. This way, we can learn a more complex mapping from the embeddings to the next token.</li>
</ul>
<div class="cell" data-execution_count="203">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttentionModel(nn.Module):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, sequence_length<span class="op">=</span><span class="dv">100</span>, head_size<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># map the input ids to embeddings</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embedding <span class="op">=</span>  nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add positional embeddings (each position has its own learnable embedding vector)</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> nn.Embedding(sequence_length, embedding_dim)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the self-attention layer</span></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> Head(embedding_dim, sequence_length, head_size)</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the linear layer that maps the output of the self-attention layer to the vocabulary size</span></span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(head_size, vocab_size)</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># store the sequence length</span></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb55-21"><a href="#cb55-21" aria-hidden="true" tabindex="-1"></a>        B, T <span class="op">=</span> x.shape</span>
<span id="cb55-22"><a href="#cb55-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.token_embedding(x) <span class="co"># B, T, C </span></span>
<span id="cb55-23"><a href="#cb55-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">+=</span> <span class="va">self</span>.positional_embedding(torch.arange(T, device<span class="op">=</span>device)) <span class="co"># B, T, C</span></span>
<span id="cb55-24"><a href="#cb55-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.attention(x) <span class="co"># B, T, head_size</span></span>
<span id="cb55-25"><a href="#cb55-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lm_head(x) <span class="co"># B, T, vocab_size</span></span>
<span id="cb55-26"><a href="#cb55-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The prediction is for each token a probability distribution over the vocabulary</span></span>
<span id="cb55-27"><a href="#cb55-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this indicates how likely each token is the next token</span></span>
<span id="cb55-28"><a href="#cb55-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb55-29"><a href="#cb55-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-30"><a href="#cb55-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-31"><a href="#cb55-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y):</span>
<span id="cb55-32"><a href="#cb55-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb55-33"><a href="#cb55-33" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb55-34"><a href="#cb55-34" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb55-35"><a href="#cb55-35" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb55-36"><a href="#cb55-36" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B<span class="op">*</span>T)</span>
<span id="cb55-37"><a href="#cb55-37" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb55-38"><a href="#cb55-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb55-39"><a href="#cb55-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-40"><a href="#cb55-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, x, max_new_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb55-41"><a href="#cb55-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb55-42"><a href="#cb55-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we generate max_new_tokens new tokens</span></span>
<span id="cb55-43"><a href="#cb55-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _t <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb55-44"><a href="#cb55-44" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb55-45"><a href="#cb55-45" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb55-46"><a href="#cb55-46" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb55-47"><a href="#cb55-47" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb55-48"><a href="#cb55-48" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, next_token], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb55-49"><a href="#cb55-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb55-50"><a href="#cb55-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb55-51"><a href="#cb55-51" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="204">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SelfAttentionModel(<span class="bu">len</span>(tokenizer.tokens), embedding_dim<span class="op">=</span><span class="dv">128</span>, sequence_length<span class="op">=</span><span class="dv">40</span>, head_size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, valid_loader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, iter 0, train loss 4.124, val perplexity 55.82777
Epoch 0, iter 100, train loss 2.388, val perplexity 11.19858
Epoch 0, iter 200, train loss 2.387, val perplexity 11.12724
Epoch 0, iter 300, train loss 2.394, val perplexity 11.12101
Epoch 0, iter 400, train loss 2.411, val perplexity 11.12952
Epoch 0, iter 500, train loss 2.412, val perplexity 11.10590
Epoch 0, iter 600, train loss 2.409, val perplexity 11.11807
Epoch 0, iter 700, train loss 2.424, val perplexity 11.10892
Epoch 0, iter 800, train loss 2.441, val perplexity 11.11399
Epoch 0, iter 900, train loss 2.410, val perplexity 11.10943
Epoch 0, iter 1000, train loss 2.420, val perplexity 11.11094
Epoch 0, iter 1100, train loss 2.394, val perplexity 11.10506
Epoch 0, iter 1200, train loss 2.428, val perplexity 11.09540
Epoch 0, iter 1300, train loss 2.430, val perplexity 11.09374
Epoch 0, iter 1400, train loss 2.365, val perplexity 11.10669
Epoch 0, iter 1500, train loss 2.418, val perplexity 11.10741
Epoch 0, iter 1600, train loss 2.384, val perplexity 11.10166
Epoch 0, iter 1700, train loss 2.412, val perplexity 11.11698
Epoch 0, iter 1800, train loss 2.447, val perplexity 11.09141
Epoch 0, iter 1900, train loss 2.408, val perplexity 11.08764
Epoch 0, iter 2000, train loss 2.387, val perplexity 11.10106
Epoch 0, iter 2100, train loss 2.415, val perplexity 11.09020
Epoch 0, iter 2200, train loss 2.423, val perplexity 11.10499
Epoch 0, iter 2300, train loss 2.394, val perplexity 11.10698
Epoch 1, iter 0, train loss 2.416, val perplexity 11.09823
Epoch 1, iter 100, train loss 2.377, val perplexity 11.09025
Epoch 1, iter 200, train loss 2.391, val perplexity 11.08770
Epoch 1, iter 300, train loss 2.383, val perplexity 11.09473
Epoch 1, iter 400, train loss 2.411, val perplexity 11.09210
Epoch 1, iter 500, train loss 2.369, val perplexity 11.10044
Epoch 1, iter 600, train loss 2.402, val perplexity 11.11767
Epoch 1, iter 700, train loss 2.429, val perplexity 11.09801
Epoch 1, iter 800, train loss 2.400, val perplexity 11.08748
Epoch 1, iter 900, train loss 2.373, val perplexity 11.08818
Epoch 1, iter 1000, train loss 2.411, val perplexity 11.08949
Epoch 1, iter 1100, train loss 2.405, val perplexity 11.08937
Epoch 1, iter 1200, train loss 2.389, val perplexity 11.09055
Epoch 1, iter 1300, train loss 2.381, val perplexity 11.09659
Epoch 1, iter 1400, train loss 2.394, val perplexity 11.09339
Epoch 1, iter 1500, train loss 2.403, val perplexity 11.09767
Epoch 1, iter 1600, train loss 2.384, val perplexity 11.09845
Epoch 1, iter 1700, train loss 2.422, val perplexity 11.09206
Epoch 1, iter 1800, train loss 2.405, val perplexity 11.09331
Epoch 1, iter 1900, train loss 2.415, val perplexity 11.08692
Epoch 1, iter 2000, train loss 2.415, val perplexity 11.09814
Epoch 1, iter 2100, train loss 2.436, val perplexity 11.08876
Epoch 1, iter 2200, train loss 2.394, val perplexity 11.08603
Epoch 1, iter 2300, train loss 2.398, val perplexity 11.08688
Epoch 2, iter 0, train loss 2.391, val perplexity 11.09284
Epoch 2, iter 100, train loss 2.390, val perplexity 11.08920
Epoch 2, iter 200, train loss 2.408, val perplexity 11.09971
Epoch 2, iter 300, train loss 2.407, val perplexity 11.10155
Epoch 2, iter 400, train loss 2.371, val perplexity 11.08800
Epoch 2, iter 500, train loss 2.377, val perplexity 11.09237
Epoch 2, iter 600, train loss 2.397, val perplexity 11.08786
Epoch 2, iter 700, train loss 2.432, val perplexity 11.09027
Epoch 2, iter 800, train loss 2.453, val perplexity 11.09479
Epoch 2, iter 900, train loss 2.427, val perplexity 11.09070
Epoch 2, iter 1000, train loss 2.412, val perplexity 11.09146
Epoch 2, iter 1100, train loss 2.417, val perplexity 11.08907
Epoch 2, iter 1200, train loss 2.356, val perplexity 11.09570
Epoch 2, iter 1300, train loss 2.415, val perplexity 11.08749
Epoch 2, iter 1400, train loss 2.414, val perplexity 11.08112
Epoch 2, iter 1500, train loss 2.398, val perplexity 11.08555
Epoch 2, iter 1600, train loss 2.422, val perplexity 11.09179
Epoch 2, iter 1700, train loss 2.399, val perplexity 11.08997
Epoch 2, iter 1800, train loss 2.414, val perplexity 11.09650
Epoch 2, iter 1900, train loss 2.409, val perplexity 11.08493
Epoch 2, iter 2000, train loss 2.409, val perplexity 11.08983
Epoch 2, iter 2100, train loss 2.417, val perplexity 11.08645
Epoch 2, iter 2200, train loss 2.384, val perplexity 11.08418
Epoch 2, iter 2300, train loss 2.390, val perplexity 11.08305
Epoch 3, iter 0, train loss 2.380, val perplexity 11.08910
Epoch 3, iter 100, train loss 2.443, val perplexity 11.08976
Epoch 3, iter 200, train loss 2.429, val perplexity 11.08478
Epoch 3, iter 300, train loss 2.404, val perplexity 11.08251
Epoch 3, iter 400, train loss 2.411, val perplexity 11.08396
Epoch 3, iter 500, train loss 2.415, val perplexity 11.09192
Epoch 3, iter 600, train loss 2.430, val perplexity 11.08152
Epoch 3, iter 700, train loss 2.429, val perplexity 11.09316
Epoch 3, iter 800, train loss 2.448, val perplexity 11.08478
Epoch 3, iter 900, train loss 2.447, val perplexity 11.08606
Epoch 3, iter 1000, train loss 2.399, val perplexity 11.09091
Epoch 3, iter 1100, train loss 2.408, val perplexity 11.08513
Epoch 3, iter 1200, train loss 2.421, val perplexity 11.08521
Epoch 3, iter 1300, train loss 2.434, val perplexity 11.08139
Epoch 3, iter 1400, train loss 2.423, val perplexity 11.08271
Epoch 3, iter 1500, train loss 2.428, val perplexity 11.08595
Epoch 3, iter 1600, train loss 2.423, val perplexity 11.08252
Epoch 3, iter 1700, train loss 2.417, val perplexity 11.09312
Epoch 3, iter 1800, train loss 2.450, val perplexity 11.08849
Epoch 3, iter 1900, train loss 2.416, val perplexity 11.09376
Epoch 3, iter 2000, train loss 2.402, val perplexity 11.08234
Epoch 3, iter 2100, train loss 2.387, val perplexity 11.08825
Epoch 3, iter 2200, train loss 2.410, val perplexity 11.08716
Epoch 3, iter 2300, train loss 2.416, val perplexity 11.08786
Epoch 4, iter 0, train loss 2.431, val perplexity 11.08732
Epoch 4, iter 100, train loss 2.418, val perplexity 11.08994
Epoch 4, iter 200, train loss 2.375, val perplexity 11.08289
Epoch 4, iter 300, train loss 2.402, val perplexity 11.09380
Epoch 4, iter 400, train loss 2.444, val perplexity 11.08196
Epoch 4, iter 500, train loss 2.430, val perplexity 11.09146
Epoch 4, iter 600, train loss 2.414, val perplexity 11.08931
Epoch 4, iter 700, train loss 2.392, val perplexity 11.08145
Epoch 4, iter 800, train loss 2.431, val perplexity 11.08986
Epoch 4, iter 900, train loss 2.409, val perplexity 11.08562
Epoch 4, iter 1000, train loss 2.381, val perplexity 11.08575
Epoch 4, iter 1100, train loss 2.390, val perplexity 11.08247
Epoch 4, iter 1200, train loss 2.390, val perplexity 11.09000
Epoch 4, iter 1300, train loss 2.393, val perplexity 11.09160
Epoch 4, iter 1400, train loss 2.397, val perplexity 11.08732
Epoch 4, iter 1500, train loss 2.387, val perplexity 11.08443
Epoch 4, iter 1600, train loss 2.387, val perplexity 11.09168
Epoch 4, iter 1700, train loss 2.372, val perplexity 11.08432
Epoch 4, iter 1800, train loss 2.404, val perplexity 11.08689
Epoch 4, iter 1900, train loss 2.419, val perplexity 11.08175
Epoch 4, iter 2000, train loss 2.400, val perplexity 11.08565
Epoch 4, iter 2100, train loss 2.417, val perplexity 11.08862
Epoch 4, iter 2200, train loss 2.406, val perplexity 11.07848
Epoch 4, iter 2300, train loss 2.381, val perplexity 11.08443
Epoch 5, iter 0, train loss 2.433, val perplexity 11.08817
Epoch 5, iter 100, train loss 2.406, val perplexity 11.08162
Epoch 5, iter 200, train loss 2.414, val perplexity 11.08919
Epoch 5, iter 300, train loss 2.423, val perplexity 11.08315
Epoch 5, iter 400, train loss 2.418, val perplexity 11.08115
Epoch 5, iter 500, train loss 2.402, val perplexity 11.08391
Epoch 5, iter 600, train loss 2.412, val perplexity 11.09365
Epoch 5, iter 700, train loss 2.399, val perplexity 11.08553
Epoch 5, iter 800, train loss 2.385, val perplexity 11.08504
Epoch 5, iter 900, train loss 2.409, val perplexity 11.09162
Epoch 5, iter 1000, train loss 2.401, val perplexity 11.08724
Epoch 5, iter 1100, train loss 2.400, val perplexity 11.08480
Epoch 5, iter 1200, train loss 2.381, val perplexity 11.07982
Epoch 5, iter 1300, train loss 2.428, val perplexity 11.08074
Epoch 5, iter 1400, train loss 2.402, val perplexity 11.08076
Epoch 5, iter 1500, train loss 2.370, val perplexity 11.08601
Epoch 5, iter 1600, train loss 2.444, val perplexity 11.08128
Epoch 5, iter 1700, train loss 2.384, val perplexity 11.07793
Epoch 5, iter 1800, train loss 2.404, val perplexity 11.07864
Epoch 5, iter 1900, train loss 2.393, val perplexity 11.08708
Epoch 5, iter 2000, train loss 2.424, val perplexity 11.08692
Epoch 5, iter 2100, train loss 2.423, val perplexity 11.08698
Epoch 5, iter 2200, train loss 2.387, val perplexity 11.07958
Epoch 5, iter 2300, train loss 2.400, val perplexity 11.08280
Epoch 6, iter 0, train loss 2.425, val perplexity 11.08352
Epoch 6, iter 100, train loss 2.393, val perplexity 11.08390
Epoch 6, iter 200, train loss 2.404, val perplexity 11.07994
Epoch 6, iter 300, train loss 2.375, val perplexity 11.08319
Epoch 6, iter 400, train loss 2.426, val perplexity 11.08233
Epoch 6, iter 500, train loss 2.398, val perplexity 11.07754
Epoch 6, iter 600, train loss 2.389, val perplexity 11.08095
Epoch 6, iter 700, train loss 2.377, val perplexity 11.08012
Epoch 6, iter 800, train loss 2.385, val perplexity 11.08026
Epoch 6, iter 900, train loss 2.406, val perplexity 11.07890
Epoch 6, iter 1000, train loss 2.388, val perplexity 11.07917
Epoch 6, iter 1100, train loss 2.445, val perplexity 11.08292
Epoch 6, iter 1200, train loss 2.435, val perplexity 11.08142
Epoch 6, iter 1300, train loss 2.396, val perplexity 11.08304
Epoch 6, iter 1400, train loss 2.375, val perplexity 11.08752
Epoch 6, iter 1500, train loss 2.401, val perplexity 11.07907
Epoch 6, iter 1600, train loss 2.397, val perplexity 11.08185
Epoch 6, iter 1700, train loss 2.416, val perplexity 11.08626
Epoch 6, iter 1800, train loss 2.388, val perplexity 11.08229
Epoch 6, iter 1900, train loss 2.404, val perplexity 11.08109
Epoch 6, iter 2000, train loss 2.380, val perplexity 11.08226
Epoch 6, iter 2100, train loss 2.446, val perplexity 11.08410
Epoch 6, iter 2200, train loss 2.421, val perplexity 11.07880
Epoch 6, iter 2300, train loss 2.444, val perplexity 11.08569
Epoch 7, iter 0, train loss 2.407, val perplexity 11.08170
Epoch 7, iter 100, train loss 2.394, val perplexity 11.08424
Epoch 7, iter 200, train loss 2.390, val perplexity 11.08440
Epoch 7, iter 300, train loss 2.393, val perplexity 11.08069
Epoch 7, iter 400, train loss 2.419, val perplexity 11.08156
Epoch 7, iter 500, train loss 2.400, val perplexity 11.08258
Epoch 7, iter 600, train loss 2.406, val perplexity 11.08027
Epoch 7, iter 700, train loss 2.418, val perplexity 11.08211
Epoch 7, iter 800, train loss 2.415, val perplexity 11.08057
Epoch 7, iter 900, train loss 2.403, val perplexity 11.07885
Epoch 7, iter 1000, train loss 2.395, val perplexity 11.07921
Epoch 7, iter 1100, train loss 2.400, val perplexity 11.08124
Epoch 7, iter 1200, train loss 2.425, val perplexity 11.08465
Epoch 7, iter 1300, train loss 2.395, val perplexity 11.08027
Epoch 7, iter 1400, train loss 2.407, val perplexity 11.08265
Epoch 7, iter 1500, train loss 2.397, val perplexity 11.08360
Epoch 7, iter 1600, train loss 2.412, val perplexity 11.08794
Epoch 7, iter 1700, train loss 2.409, val perplexity 11.08159
Epoch 7, iter 1800, train loss 2.406, val perplexity 11.07970
Epoch 7, iter 1900, train loss 2.406, val perplexity 11.08356
Epoch 7, iter 2000, train loss 2.423, val perplexity 11.08166
Epoch 7, iter 2100, train loss 2.397, val perplexity 11.07727
Epoch 7, iter 2200, train loss 2.409, val perplexity 11.08467
Epoch 7, iter 2300, train loss 2.370, val perplexity 11.07904
Epoch 8, iter 0, train loss 2.368, val perplexity 11.08059
Epoch 8, iter 100, train loss 2.423, val perplexity 11.08200
Epoch 8, iter 200, train loss 2.410, val perplexity 11.07815
Epoch 8, iter 300, train loss 2.413, val perplexity 11.08172
Epoch 8, iter 400, train loss 2.372, val perplexity 11.07827
Epoch 8, iter 500, train loss 2.403, val perplexity 11.07991
Epoch 8, iter 600, train loss 2.408, val perplexity 11.07978
Epoch 8, iter 700, train loss 2.369, val perplexity 11.08273
Epoch 8, iter 800, train loss 2.397, val perplexity 11.08346
Epoch 8, iter 900, train loss 2.417, val perplexity 11.08189
Epoch 8, iter 1000, train loss 2.438, val perplexity 11.07828
Epoch 8, iter 1100, train loss 2.423, val perplexity 11.07997
Epoch 8, iter 1200, train loss 2.427, val perplexity 11.08032
Epoch 8, iter 1300, train loss 2.408, val perplexity 11.08005
Epoch 8, iter 1400, train loss 2.395, val perplexity 11.07992
Epoch 8, iter 1500, train loss 2.389, val perplexity 11.07970
Epoch 8, iter 1600, train loss 2.421, val perplexity 11.08282
Epoch 8, iter 1700, train loss 2.410, val perplexity 11.07893
Epoch 8, iter 1800, train loss 2.402, val perplexity 11.07853
Epoch 8, iter 1900, train loss 2.387, val perplexity 11.07859
Epoch 8, iter 2000, train loss 2.396, val perplexity 11.08859
Epoch 8, iter 2100, train loss 2.424, val perplexity 11.07993
Epoch 8, iter 2200, train loss 2.389, val perplexity 11.07753
Epoch 8, iter 2300, train loss 2.425, val perplexity 11.07979
Epoch 9, iter 0, train loss 2.393, val perplexity 11.08574
Epoch 9, iter 100, train loss 2.445, val perplexity 11.08460
Epoch 9, iter 200, train loss 2.343, val perplexity 11.08016
Epoch 9, iter 300, train loss 2.422, val perplexity 11.08048
Epoch 9, iter 400, train loss 2.400, val perplexity 11.08331
Epoch 9, iter 500, train loss 2.398, val perplexity 11.09080
Epoch 9, iter 600, train loss 2.383, val perplexity 11.08066
Epoch 9, iter 700, train loss 2.417, val perplexity 11.08187
Epoch 9, iter 800, train loss 2.441, val perplexity 11.08072
Epoch 9, iter 900, train loss 2.416, val perplexity 11.07930
Epoch 9, iter 1000, train loss 2.439, val perplexity 11.08149
Epoch 9, iter 1100, train loss 2.384, val perplexity 11.07934
Epoch 9, iter 1200, train loss 2.383, val perplexity 11.07627
Epoch 9, iter 1300, train loss 2.382, val perplexity 11.07983
Epoch 9, iter 1400, train loss 2.414, val perplexity 11.08299
Epoch 9, iter 1500, train loss 2.419, val perplexity 11.08153
Epoch 9, iter 1600, train loss 2.412, val perplexity 11.08024
Epoch 9, iter 1700, train loss 2.411, val perplexity 11.07876
Epoch 9, iter 1800, train loss 2.430, val perplexity 11.08331
Epoch 9, iter 1900, train loss 2.428, val perplexity 11.07763
Epoch 9, iter 2000, train loss 2.404, val perplexity 11.08191
Epoch 9, iter 2100, train loss 2.402, val perplexity 11.08576
Epoch 9, iter 2200, train loss 2.370, val perplexity 11.08004
Epoch 9, iter 2300, train loss 2.408, val perplexity 11.07864</code></pre>
</div>
</div>
<div class="cell" data-execution_count="207">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor([[<span class="dv">4</span>]])</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> a.to(device)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>generation <span class="op">=</span> model.generate(a, max_new_tokens<span class="op">=</span><span class="dv">10</span>).cpu().numpy()</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>tokenizer.decode(generation[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="207">
<pre><code>'[C@@])43[C][C])#[S@+]C4'</code></pre>
</div>
</div>
</section>
</section>
<section id="interlude-additional-perspectives-on-attention" class="level2">
<h2 class="anchored" data-anchor-id="interlude-additional-perspectives-on-attention">Interlude: Additional perspectives on attention</h2>
<section id="attention-as-gnn" class="level3">
<h3 class="anchored" data-anchor-id="attention-as-gnn">Attention as GNN</h3>
<ul>
<li><p>In the attention mechanism we learn how different tokens “communicate” with each other. If we think of tokens as nodes, attention corresponds to learning the edge weights of a fully connected graph.</p></li>
<li><p>The tokens per default have no notion of their position in the sequence. It is basically the communication between sets of vectors.</p></li>
</ul>
<p>In attentional GNNs, we write for the embeddings:</p>
<p><span class="math display">\[
\mathbf{h}_i=\phi\left(\mathbf{x}_i, \bigoplus_{j \in \mathcal{V}} a\left(\mathbf{x}_i, \mathbf{x}_j\right) \psi\left(\mathbf{x}_j\right)\right)
\]</span></p>
<p>where <span class="math inline">\(\bigoplus\)</span> is a permutation invariant function, e.g., sum or mean over the neighborhood <span class="math inline">\(\mathcal{V}\)</span>. <a href="https://petar-v.com/talks/GNN-EEML.pdf">Does this equation look familiar?</a></p>
<p>You can find more information <a href="https://thegradient.pub/transformers-are-graph-neural-networks/">here</a> and <a href="https://arxiv.org/pdf/2301.08210.pdf">here</a>.</p>
<p>The main difference is that in the transformer we model a fully connected graph, whereas in GNNs we model a sparse graph (which is an inductive bias).</p>
</section>
<section id="attention-as-kernel-smoothing" class="level3">
<h3 class="anchored" data-anchor-id="attention-as-kernel-smoothing">Attention as Kernel smoothing</h3>
<ul>
<li>Given that we have been introducing the attention mechanism as a way to compute a weighted average of values, the analogy to a kernel is quite natural.</li>
</ul>
<p>To understand this a bit better, let us introduce <a href="https://en.wikipedia.org/wiki/Kernel_regression">kernel smoothing</a>. Again, it is nothing else then a weighted average. In this weighted average, the weights are determined by a kernel function.</p>
<p><span class="math display">\[
\sum_{i=1}^n y_i \frac{K\left(x_i, x_o\right)}{\sum_{j=1}^n K\left(x_j, x_o\right)},
\]</span></p>
<p>where <span class="math inline">\((x_1, y_1), \dots, (x_n, y_n)\)</span> are the training points and <span class="math inline">\(x_o\)</span> is the point at which we want to make a prediction.</p>
<p>A common kernel function is the Gaussian kernel:</p>
<p><span class="math display">\[
K(x, x_o) = \exp\left(xx_o\right)
\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is a hyperparameter.</p>
<p>We are also free to add weights</p>
<p><span class="math display">\[
K(x, x_o) = \exp\left(\mathbf{w}_1 x  \mathbf{w}_2 x_o\right)
\]</span></p>
<p>where <span class="math inline">\(w\)</span> are square weight matrices. For stability, we might divide by the dimensionality of <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[
K(x, x_o) = \exp\left(\frac{\mathbf{w}_1 x  \mathbf{w}_2 x_o}{\sqrt{d}}\right)
\]</span></p>
<p>where <span class="math inline">\(d\)</span> is the dimensionality of <span class="math inline">\(x\)</span>.</p>
<p>Compare this to the attention equation:</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(d_k\)</span> is the dimension of <span class="math inline">\(K\)</span> and <span class="math inline">\(Q\)</span>.</p>
<p>You can find more information on this perspective <a href="http://bactra.org/notebooks/nn-attention-and-transformers.html">here</a>.</p>
</section>
</section>
<section id="adding-more-expressive-power-with-more-heads-and-fully-connected-layers" class="level2">
<h2 class="anchored" data-anchor-id="adding-more-expressive-power-with-more-heads-and-fully-connected-layers">Adding more expressive power with more heads and fully connected layers</h2>
<p>A very simple way to improve the attention mechanism is to use multiple attention heads. That is we apply the attention mechanism multiple times and then concatenate the results.</p>
<p>The intuition behind this is that different attention heads can learn different attention patterns.</p>
<div class="cell" data-execution_count="208">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, n_embed, block_size, head_size):</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(n_embed, block_size, head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T, C)</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we want to compute the attention for each head</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and then concatenate the results</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we will have a tensor of shape (B, T, num_heads * head_size)</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in practice, we might not concatenate but add another dimension</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to the tensors</span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat([head(x) <span class="cf">for</span> head <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we let the tokens talk to each other we currently only used one linear layer to map to the outputs. We can expect better performance if we use multiple layers.</p>
<p>One typically uses wide linear layers that can more readily be parallelized than deep linear layers.</p>
<div class="cell" data-execution_count="209">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForwardLayer(nn.Module):</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embed, hidden):</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(n_embed, hidden),</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),<span class="co"># </span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden, n_embed)</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we put it together, it looks like this:</p>
<div class="cell" data-execution_count="210">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SelfAttentionModel(nn.Module):</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embedding_dim, sequence_length<span class="op">=</span><span class="dv">100</span>, head_size<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># read of the logits of the next token from table</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embedding <span class="op">=</span> nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> nn.Embedding(sequence_length, embedding_dim)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(head_size, vocab_size)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(num_heads, embedding_dim, sequence_length, head_size)</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feed_forward <span class="op">=</span> FeedForwardLayer(embedding_dim, <span class="dv">4</span><span class="op">*</span>embedding_dim)</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>        B, T <span class="op">=</span> x.shape</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.token_embedding(x)</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">+=</span> <span class="va">self</span>.positional_embedding(torch.arange(T, device<span class="op">=</span>device))</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.attention(x)</span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y):</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B<span class="op">*</span>T)</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, x, max_new_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we generate max_new_tokens new tokens</span></span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a>        new_tokens <span class="op">=</span> []</span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _t <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb62-34"><a href="#cb62-34" aria-hidden="true" tabindex="-1"></a>            x_ <span class="op">=</span> x[:, <span class="op">-</span><span class="va">self</span>.sequence_length:]</span>
<span id="cb62-35"><a href="#cb62-35" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(x_) <span class="co"># (B, T, C)</span></span>
<span id="cb62-36"><a href="#cb62-36" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># we only care about the last token in Bigram, hence we bow have shape (B, C)</span></span>
<span id="cb62-37"><a href="#cb62-37" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># we generate probabilities for the next token</span></span>
<span id="cb62-38"><a href="#cb62-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-39"><a href="#cb62-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># torch.multinomial(probs, num_samples=1) returns a tensor of shape (B, 1) </span></span>
<span id="cb62-40"><a href="#cb62-40" aria-hidden="true" tabindex="-1"></a>            <span class="co"># where each element is the index of the sampled token</span></span>
<span id="cb62-41"><a href="#cb62-41" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb62-42"><a href="#cb62-42" aria-hidden="true" tabindex="-1"></a>            new_tokens.append(next_token)</span>
<span id="cb62-43"><a href="#cb62-43" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, next_token], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb62-44"><a href="#cb62-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb62-45"><a href="#cb62-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb62-46"><a href="#cb62-46" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="abstracting-transformers-into-blocks" class="level2">
<h2 class="anchored" data-anchor-id="abstracting-transformers-into-blocks">Abstracting transformers into blocks</h2>
<p>It turns out that we can improve the performance by performing the self-attention and feedforward multiple times. For this, it is useful to extract the reusable parts into a block.</p>
<p>However, just making the model deeper can lead to problems with training. To avoid this, we will leverage two tricks: - we will use residual connections: they allow us to “skip” over layers. During optimization, there will be a “shortcut” to between the input and the output of the block. - we will use layer normalization: it allows us to normalize the activations of a layer - we will add dropout: it allows us to randomly drop activations during training. This can be seen as a form of regularization.</p>
<p>We will apply layer norm twice: - once directly on the input - then before we pass the multihead attention output to the feedforward layer</p>
<p>Note that <a href="https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure">there is some debate</a> on where layer norm is optimally placed.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? <br><br>It places the layer normalization between the residual blocks, which doesn't match the code: <a href="https://t.co/z1oMLFpmiZ">https://t.co/z1oMLFpmiZ</a><br><br>PS: This is known as Post-LN Transformer<br><br>1/3 <a href="https://t.co/OOvp4FA8Nz">pic.twitter.com/OOvp4FA8Nz</a>
</p>
— Sebastian Raschka (<span class="citation" data-cites="rasbt">@rasbt</span>) <a href="https://twitter.com/rasbt/status/1655575611979489282?ref_src=twsrc%5Etfw">May 8, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<div class="cell" data-execution_count="211">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Transformer block: communication followed by computation """</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, block_size, n_head):</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># n_embd: embedding dimension, n_head: the number of heads we'd like</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(num_heads<span class="op">=</span>n_head, n_embed<span class="op">=</span>n_embd, block_size<span class="op">=</span>block_size, head_size<span class="op">=</span>head_size)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedForwardLayer(n_embd, n_embd<span class="op">*</span><span class="dv">4</span>)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x)) <span class="co"># residual connection</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>An important thing to realize is that a bulk of the parameters is in the <code>FeedForwardLayer</code>.</p>
<div class="cell" data-execution_count="212">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>block <span class="op">=</span> Block(<span class="dv">128</span>, <span class="dv">100</span>, <span class="dv">4</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>get_num_parameters_per_layer(block)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="212">
<pre><code>{'sa.heads.0.key.weight': 4096,
 'sa.heads.0.query.weight': 4096,
 'sa.heads.0.value.weight': 4096,
 'sa.heads.1.key.weight': 4096,
 'sa.heads.1.query.weight': 4096,
 'sa.heads.1.value.weight': 4096,
 'sa.heads.2.key.weight': 4096,
 'sa.heads.2.query.weight': 4096,
 'sa.heads.2.value.weight': 4096,
 'sa.heads.3.key.weight': 4096,
 'sa.heads.3.query.weight': 4096,
 'sa.heads.3.value.weight': 4096,
 'ffwd.net.0.weight': 65536,
 'ffwd.net.0.bias': 512,
 'ffwd.net.2.weight': 65536,
 'ffwd.net.2.bias': 128,
 'ln1.weight': 128,
 'ln1.bias': 128,
 'ln2.weight': 128,
 'ln2.bias': 128}</code></pre>
</div>
</div>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
I fixed the Transformer diagram :D <a href="https://t.co/qWnOUjZKut">pic.twitter.com/qWnOUjZKut</a>
</p>
— Andrej Karpathy (<span class="citation" data-cites="karpathy">@karpathy</span>) <a href="https://twitter.com/karpathy/status/1658161721251602432?ref_src=twsrc%5Etfw">May 15, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><img src="https://pbs.twimg.com/media/FwL5ROUagAIrsJT?format=jpg&amp;name=medium.png" class="img-fluid"></p>
<p>With all these “tricks” and enhancements of expressivity, we can now build a full GPT.</p>
<div class="cell" data-execution_count="223">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, n_embd, block_size, n_head, n_blocks):</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tok_emb <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_emb <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, block_size, n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_blocks)])</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Linear(n_embd, vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block_size <span class="op">=</span> block_size</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>        B, T <span class="op">=</span> x.shape</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.tok_emb(x) <span class="op">+</span> <span class="va">self</span>.pos_emb(torch.arange(T, device<span class="op">=</span>x.device))  <span class="co"># b,tc, batch, time - seqeuence length, embedding dimension</span></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layers(x)</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, x, y):</span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.forward(x) <span class="co"># (B, T, C)</span></span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.view(B<span class="op">*</span>T)</span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> loss</span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-28"><a href="#cb66-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, x, max_new_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb66-29"><a href="#cb66-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is a tensor of shape (B, T)</span></span>
<span id="cb66-30"><a href="#cb66-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we generate max_new_tokens new tokens</span></span>
<span id="cb66-31"><a href="#cb66-31" aria-hidden="true" tabindex="-1"></a>        new_tokens <span class="op">=</span> []</span>
<span id="cb66-32"><a href="#cb66-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _t <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb66-33"><a href="#cb66-33" aria-hidden="true" tabindex="-1"></a>            x_ <span class="op">=</span> x[:, <span class="op">-</span><span class="va">self</span>.block:]</span>
<span id="cb66-34"><a href="#cb66-34" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.forward(x_)</span>
<span id="cb66-35"><a href="#cb66-35" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb66-36"><a href="#cb66-36" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb66-37"><a href="#cb66-37" aria-hidden="true" tabindex="-1"></a>            next_token <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb66-38"><a href="#cb66-38" aria-hidden="true" tabindex="-1"></a>            new_tokens.append(next_token)</span>
<span id="cb66-39"><a href="#cb66-39" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, next_token], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb66-40"><a href="#cb66-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>gpt <span class="op">=</span> GPT(<span class="bu">len</span>(tokenizer.tokens), n_embd<span class="op">=</span><span class="dv">512</span>, block_size<span class="op">=</span><span class="dv">40</span>, n_head<span class="op">=</span><span class="dv">4</span>, n_blocks<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="227">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>get_num_parameters(gpt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="227">
<pre><code>11623424</code></pre>
</div>
</div>
<p>That is not nothing and certainly not something I can run on my MacBook while still doing other things. When I run the training code below on a GPU (Colab is good enough), I get a validation perplexity of XX and generate SMILES like XY.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>train_model(gpt, train_loader, valid_loader, epochs<span class="op">=</span><span class="dv">3</span>, lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>We saw how to build a GPT to generate new SMILES. We generalized a simple bigram model to take into account all past tokens and not just the last one. When we take the tokens into account, we do this by using self-attention, which allows the model to learn the dependencies between tokens.</p>
<p>To further improve the model, we added multiple heads to the self-attention mechanism, which allows the model to learn different dependencies between tokens. Finally, we stacked multiple blocks of self-attention and feedforward layers to create a GPT model.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Much of this discussion (and also the way it is structured, e.g., based on the bigram) is based on the outstanding material created by <a href="https://karpathy.ai/zero-to-hero.html">Andrej Karpathy</a>. In particular, the implementation here follows <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>.</p>
<p>Other useful resources are:</p>
<ul>
<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">Annotated transformer</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Illustrated transformer</a></li>
<li><a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention! Attention?</a></li>
<li><a href="https://bbycroft.net/llm">Interactive attention visualization</a></li>
<li><a href="https://udlbook.github.io/udlbook/">Simon Prince’s book</a> and <a href="https://www.borealisai.com/research-blogs/tutorial-14-transformers-i-introduction/">blog posts</a> have very nice illustrations of the attention mechanism.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="kjappelbaum/kjappelbaum.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">
        <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/kevin-maik-jablonka/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/kmjablonka">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kjappelbaum">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://mailhide.io/e/o4LeOUlq">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://scholar.google.com/citations?user=R2ntI8IAAAAJ&amp;hl=en">
      <i class="bi bi-mortarboard-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
      </div>
  </div>
</footer>



</body></html>