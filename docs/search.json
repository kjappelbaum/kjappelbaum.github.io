[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Kevin's Homepage",
    "section": "",
    "text": "I take great pleasure in teaching and mentoring. I gave a number of lectures and tutorials on data management and machine learning at EPFL and the MolSim winterschool. Starting from my undergrad at TU Munich, I have been a teaching assistant and mentoring students.\n\n\nOne of my favorite teaching experiences was developing what we call a â€œvirtual laboratory.â€\n\nFor more details, you can check out our paper in J. Chem. Educ.\n\n\n\nI have been lucky to introduce students to machine learning. For this, I have developed a series of lectures and a hands-on exercise, which you can find on GitHub. Usually, we also host a Kaggle competition as part of the hands-on exercise. You can find an example of the competition here. Typically, I like to add some live-coding examples to the lecture. You can find the notebooks for those examples on GitHub."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Kevin's Homepage",
    "section": "",
    "text": "My current and past research focuses on data-driven material design as well as the capture of (experimental) data in a machine-actionable form â€” which is pivotal to power data-driven approaches.\nThe central theme of my research is that material design is a very complex, mulitscale process that often involves questions that are not easily answered by simulations. Machine learning can help to address these questions and thereby accelerate material design and discovery and enable the development of new materials and yield to new insights.\n\n\nFor an overview of this research, see our Chem. Rev. as well as our perspective in JACS.\n\n\n\n\n\n\n\n\nAn ecosystem for digital reticular chemistry.\n\n\n\n\nA key challenge in data-driven material design, in particular for reticular chemistry, has been the lack of a comprehensive and open-source ecosystem for machine learning on reticular materials. To address this, we have been developing an open-source ecosystem for digital reticular chemistry, called mofdscribe. This ecosystem provides tools that accompany practioners along the entire data-driven design workflow: from data to publication. For this toolbox, we also developed and generalized a range of featurizers, i.e., methods to convert crystal structures into fixed-length vectors, which can be used to train machine learning models. For multiple featurizers, we could show that the generalization greatly improves the performance of the models.\n\n\n\nAnother exciting aspect of this work is that we have shown that it is straightforward to do machine-learning on crystallographic data incorrectlyâ€”and that this can lead to very misleading results.\nYou can find more details in the paper and on the open source page.\n\n\n\nGiven the tools and datasets mofdscribe provides of this, we can now use machine learning tools to address challenging material design problems.\nOver the last years, we have been doing this from the atom scale up to the pilot plant scale:\n\nFor instance, we have developed a machine learning model that can predict the oxidation state of metal cations in MOFs. This is interesting because oxidation states are a key part of chemical reasoning (they are even part of the names of the chemicals) but not quantum-mechanical observable. Using a chemically informed model, we could vastly outperform the state-of-the-art and show that the model reasons about the oxidation state in a way that is consistent with chemical intuition. For more details, see the paper.\nBeyond this, we have also been using machine learning to predict the color (see paper) as well as gas adsorption properties (see paper) of MOFs.\nRecently, we have shown how we can use machine learning to forecast the amine emission from a carbon capture pilot plant that is fed using a slipstream from a real power plant (see paper).\n\n\n\n\nThe power of machine learning in material discovery is that it can help us build a map of the chemical space. Equipped with this map, we can explore the chemical space more efficiently.\n\nDoing this for the design of materials is, however, complicated by the fact that we need to optimize multiple properties at the same time. There are often trade-offs between the properties we want to optimize, i.e., we cannot find a single optimum spot on the map, and the best we can do is to find the set of the best compromises. To avoid introducing biases in this process, we have implemented a method that allows to identify this set of best compromises (the Pareto frontier) with high confidence. You can find more information about this in our paper and an open-source implementation on GitHub.\n\n\n\n\n\n\n\n\n\n\nA digital assistant for chemists.\n\n\n\n\nHow can we generate an effective assistant for chemists to answer questions such as â€œfind all MOFs that can be made in one step in a solvothermal synthesis in water?â€ To answer questions like these, we need data in a form that cannot only be read by a machine but also understood in order to perform actions.\n\n\n\nIn order to ensure that data is an afterthought in chemistry, I got involved in the development of an open-source ELN, the cheminfo ELN (which development is led by Luc Patiny, our perspective article gives a good overview of our vision for the development of this platform). In contrast to many other ELNs, machine-actionable data is at the core of the ELN. This enables us to perform various actions on the data directly in the browser.\n\n\n\n\n\n\n\n\nIn many cases, we can only make sense of experimental data if we compare it to theoretical predictions. For instance, isotherms measure how much gas a material can adsorb as a function of pressure. Clearly, it can adsorb more if there are defects, such as missing linkers, and less if it is incompletely activated. However, to make those statements, one needs to compare to the isotherm of the â€œidealâ€ material. This is something one can do with simulations.\n\n\n\n\n\n\n\nPredict or look up XRD patterns on the fly.\n\n\n\n\nTo make such comparisons routine â€” but also to create rich datasets with data from both simulations and experiments â€” we made it very easy to â€œrequestâ€ simulations from within the ELN, by linking it to simulation platforms such as AiiDAlab or web services.\n\n\n\n\n\n\n\n\n\n\n\n\nDigital supporting information. The data is machine actionable and alive.\n\n\n\n\nOnce data is stored in a digital and machine-actionable form, we can use it to create electronic supporting information documents with just one click. In the cheminfo ELN, we make this possible by allowing users to export all their data to Zenodo. There, the data is accessible in a machine-actionable form and can interactively be explored in the browser. You can find an example of such a supporting information document here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin's Homepage",
    "section": "",
    "text": "Iâ€™m at the final touches of my Ph.D.Â in the group of Berend Smit at EPFL focused on the digitization of chemistry: Iâ€™m using computational tools and machine learning to capture the tacit dimension of chemistry. Iâ€™m also a contributor to the cheminfo electronic lab notebook (ELN) ecosystem and other open-source projects (see my GitHub)."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Kevin's Homepage",
    "section": "ðŸ’¼ Get in Touch",
    "text": "ðŸ’¼ Get in Touch\nEmail me, or DM me on  or  if youâ€™d like to chat!\nIf you are looking for a short bio or a headshot, you can find them here."
  },
  {
    "objectID": "blog/posts/plotting/index.html",
    "href": "blog/posts/plotting/index.html",
    "title": "Publication-ready plots with Python",
    "section": "",
    "text": "Over the last decade Iâ€™ve spent a lot of time plotting data. Iâ€™ve tried many different tools and workflows, and Iâ€™ve learned a lot.\nI started with gnuplot, explored pgfplots and large parts of the Python plotting ecosystem.\nMy current setup went back to basics and uses matplotlib and is heavily inspired by the work of Tufte and Jean-Luc Doumont as well as journal requirements."
  },
  {
    "objectID": "blog/posts/how-much-data/index.html#inductive-biases",
    "href": "blog/posts/how-much-data/index.html#inductive-biases",
    "title": "How much data do I need?",
    "section": "Inductive biases",
    "text": "Inductive biases"
  },
  {
    "objectID": "blog/posts/how-much-data/index.html#learning-curves",
    "href": "blog/posts/how-much-data/index.html#learning-curves",
    "title": "How much data do I need?",
    "section": "Learning curves",
    "text": "Learning curves"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Kevin's Homepage",
    "section": "",
    "text": "You can also find me on Google Scholar and ORCID."
  },
  {
    "objectID": "opensource/opensource.html",
    "href": "opensource/opensource.html",
    "title": " Open Source",
    "section": "",
    "text": "I believe that computational science relies on open code on data and subscribe to the Manifesto put out by Jonathan B. Buckheit and David L. Donoho:"
  },
  {
    "objectID": "opensource/opensource.html#digital-reticular-chemistry-mofworld",
    "href": "opensource/opensource.html#digital-reticular-chemistry-mofworld",
    "title": " Open Source",
    "section": " Digital reticular chemistry â€“ â€œMOFworldâ€",
    "text": "Digital reticular chemistry â€“ â€œMOFworldâ€\nI have been very active in developing tools for â€œdigital reticular chemistryâ€ (see also this article by Yaghi and collaborators), i.e., tools for data-driven science with materials such as metal-organic frameworks (MOFs) and covalent-organic frameworks (COFs).\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nOther References\n\n\n\n\n\n\n\n\n\nmofdscribe\n\n\nAn easy to use tool that accompanies digital reticular chemists on all stages of their work. It provides data sets, more than 40 featurizers, consistent splitting tools to avoid data leakage, as well as tools for evaluating models and comparing them on a leaderboard.\n\n\nPaper\n\n\n\n\n\n\n\nmofchecker\n\n\nThe mofchecker is a tool that allows to check the â€œsanityâ€ of a MOF structure. If implements a range of convenient check, such as checking for missing or overlapping atoms, or unreasonable coordination environment in an easy-to-use interface. Some of the checks also have automatic fixes and are implemented in a graphical interface in an AiiDAlab app.\n\n\nWeb deployment of an earlier version\n\n\n\n\n\n\n\nmoffragmentor\n\n\nThe moffragmentor is a tool that allows to fragment MOF structures into building blocks.\n\n\n\n\n\n\n\n\n\nelement-coder\n\n\nElement coder is a tool that allows to encode elements into a vector spaceâ€”but to also decode them back into the original element. This is useful for machine learning applications where one wants to encode elements into a vector space, but also wants to be able to decode them back into the original element (inverse design).\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "opensource/opensource.html#active-learning",
    "href": "opensource/opensource.html#active-learning",
    "title": " Open Source",
    "section": " Active learning",
    "text": "Active learning\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nOther References\n\n\n\n\n\n\n\n\n\nPyePAL\n\n\nPyePAL implements the e-PAL algorithm for Pareto active learning. It comes with interfaces for a range of machine learning models, including scikit-learn, GPy, and jax. It generalizes to any number of objectives and also supports batches sampling as well as various schedulers for (re)-training the model.\n\n\nPaper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "opensource/opensource.html#chemical-data-management",
    "href": "opensource/opensource.html#chemical-data-management",
    "title": " Open Source",
    "section": " Chemical Data management",
    "text": "Chemical Data management\nI have been an active contributor to the cheminfo ecosystem. For more details about it, you can give our perspective article a look.\nSome of my contributions are listed below:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nOther References\n\n\n\n\n\n\n\n\n\nisotherm-analysis\n\n\nisotherm-analysis allows to parse and analyze isotherms. It converts from multiple formats to JCAMP-DX and provides utilities for basic analysis.\n\n\nUsed in our preprint\n\n\n\n\n\n\n\npubchem\n\n\nJavaScript interface to the PubChem API. In the cheminfo ELN, we use this to display safety information.\n\n\n\n\n\n\n\n\n\nxrd-analysis\n\n\nxrd-analysis can convert output files from powder-xray diffraction into JCAMP-DX format and perform analysis (Scherrer equation, â€¦) on the diffractograms.\n\n\nDiscussed in our perspective\n\n\n\n\n\n\n\ntga-analysis\n\n\ntga-analysis provides tools to convert output files from thermogravimetric analysis (TGA) into JCAMP-DX, as well as tools to analyze the data (mass loss analysis).\n\n\n\n\n\n\n\n\n\nbaselines\n\n\nBaselines provides a collection of baseline correction methods.\n\n\n\n\n\n\n\n\n\ncheminfo.github.io\n\n\nI created the web page for the cheminfo organization, which is a collection of FAIR building blocks for chemistry and beyond.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/ml/ml_notes.html",
    "href": "teaching/ml/ml_notes.html",
    "title": "Workshop on ML for materials science",
    "section": "",
    "text": "To design new materials, we need to know their properties. There are two main routes to get the properties of a material:\n\nPerform an experiment to measure them\nPerform a simulation to â€œmeasureâ€ them in silico\n\nIn many cases, performing an experiment is time-consuming and, hence, expensive. Also high-fidelity simulations can be very costly. Fidelity expresses the exactness with which a surrogate represents the truth. In the context of ML you might also see the term multi-fidelity, which means that the approach uses multiple approximations with different levels of fidelity, e.g.Â density-functional theory and coupled cluster theory\nTherefore, there is a need for methods that can help us to predict the properties of materials with high fidelity and low cost. In this lecture, we will see that supervised machine learning (ML) is a powerful tool to achieve this goal.\nInterestingly, this tool can be used in many different ways.\n\n\nMachine learning can be used in multiple ways to make high-fidelity predictions of materials less expensive. Note that reducing the cost has been a challenge for chemists and material scientists for a long time. Dirac famously said â€œThe fundamental laws necessary for the mathematical treatment of a large part of physics and the whole of chemistry are thus completely known, and the difficulty lies only in the fact that application of these laws leads to equations that are too complex to be solved. [â€¦] approximate practical methods of applying quantum mechanics should be developed, which can lead to an explanation of the main features of complex atomic systems without too much computationâ€\n\n\n\nMachine learning (green boxes) can be used at multiple places in the material design process.\n\n\n\nReplace expensive evaluation of the potential energy surface \\(U(\\mathbf{X}, \\{\\mathbf{Z}\\})\\): Quantum chemistry as a field is concerned with the prediction of the potential energy surface \\(U(\\mathbf{X}, \\{\\mathbf{Z}\\})\\) of a system of atoms of types \\(\\mathbf{Z}\\) at positions \\(\\mathbf{X}\\). Quantum chemists have developed different approximations to this problem. However, since they are all kinds of functions that map positions of atoms (and atom types, and in some cases electron densities/coordinates) to energies, we can learn those functions with ML.\nNote that once we have done that, we generally still need to perform simulations to extract the properties of interest (e.g.Â as ensemble averages).\nThere are many good review articles about this. For example, see this one by Unke et al. as well as the ones by Deringer et al. and Behler in the same issue of Chemical Reviews.\nDirectly predict the properties of interest Instead of computing the properties of interest using a molecular simulations, we can build models that learn the \\(f(\\mathrm{structure}) \\to \\mathrm{property}\\) mapping directly. The basis for this mapping might be experimental data or high-fidelity computational data.\nAlso about this approach, there are many review articles. I also wrote one, focussing on porous materials.\n\nNote that in the context of using ML for molecular simulations, it can also be used to address sampling problems. We will not cover this in detail in this lecture. For a good introduction, see the seminal paper by Noe and a piece about it by Tuckerman."
  },
  {
    "objectID": "teaching/ml/ml_notes.html#supervised-ml-workflow",
    "href": "teaching/ml/ml_notes.html#supervised-ml-workflow",
    "title": "Workshop on ML for materials science",
    "section": "Supervised ML workflow",
    "text": "Supervised ML workflow\n\n\n\nThe supervised ML workflow.\n\n\nFor the main part of this lecture, we will assume that we use models that consume so-called tabular data, i.e.Â data that is stored in a table (feature matrix \\(\\mathbf{X}\\) and target/label vector/matrix \\(\\mathbf{Y}\\)), where each row corresponds to a material and each of the \\(p\\) columns corresponds to a so-called feature. We wil later see that this is not the only way to use ML for materials science, but it is the most common one. We will also explore in more detail how we obtain the features.\nWe will use some data \\(\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N\\) to train a model \\(f(\\mathbf{x}) \\to y\\) that can predict the target \\(y\\) for a new structure described with the feature vector \\(\\mathbf{x}^*\\)."
  },
  {
    "objectID": "teaching/ml/ml_notes.html#feeding-structures-into-models",
    "href": "teaching/ml/ml_notes.html#feeding-structures-into-models",
    "title": "Workshop on ML for materials science",
    "section": "Feeding structures into models",
    "text": "Feeding structures into models\n\nIncorporating symmetries/invariances/equivariances\n\nLearning a very simple force field\nTo understand what it takes to feed structures into ML models, let us try to build a very simple force field. To make things simple and fast, we will just attempt to predict the energies of different conformers of the same molecule.\nWe will create some data using RDkit and then use scikit-learn to train a model.\n\nGenerating data\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pymatviz.parity import density_scatter_with_hist\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem, PyMol\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport matplotx\nplt.style.use(['science', 'nature', matplotx.styles.dufte])\n\ndef gen_conformers(mol, numConfs=10_000, maxAttempts=1000, \n    pruneRmsThresh=0.2, useExpTorsionAnglePrefs=True, \n    useBasicKnowledge=True, enforceChirality=True):\n    \"\"\"Use RDkit to generate conformers for a molecule.\"\"\"\n    ids = AllChem.EmbedMultipleConfs(mol, numConfs=numConfs, maxAttempts=maxAttempts, pruneRmsThresh=pruneRmsThresh, useExpTorsionAnglePrefs=useExpTorsionAnglePrefs, useBasicKnowledge=useBasicKnowledge, enforceChirality=enforceChirality, numThreads=0)\n    return list(ids)\n\ndef calc_energy(mol, conformer_id, iterations=0):\n    \"\"\"Calculate the energy of a conformer using the Merck Molecular Force Field.\"\"\"\n    ff = AllChem.MMFFGetMoleculeForceField(mol, AllChem.MMFFGetMoleculeProperties(mol), confId=conformer_id)\n    ff.Initialize()\n    ff.CalcEnergy()\n    results = {}\n    if iterations > 0:\n        results[\"converged\"] = ff.Minimize(maxIts=iterations)\n    results[\"energy_abs\"] = ff.CalcEnergy()\n    return results\n\n# create a molecule\nmol = Chem.AddHs(Chem.MolFromSmiles('CC(CCC)CC(C)(CCCC)O'))\n\n# visualize some conformers using PyMol\nconformer_ids = gen_conformers(mol)\nv= PyMol.MolViewer()\nv.DeleteAll()\nfor cid in conformer_ids[:50]: \n    v.ShowMol(mol,confId=cid,name='Conf-%d'%cid,showOnly=False)\nv.server.do('set grid_mode, on')\nv.server.do('ray')\nv.GetPNG()\n\n\n\n\nFor those conformers, we can now retrieve the positions and energies and save them in a pandas dataframe.\n\n# make column names\ncoordinate_names = sum([[f'x_{n}',f'y_{n}', f'z_{n}'] for n in range(mol.GetNumAtoms())], []) \n\n# make a dataframe\ndata = []\nfor conformer_id in conformer_ids:\n    energy = calc_energy(mol, conformer_id)['energy_abs']\n    positions = mol.GetConformer(conformer_id).GetPositions().flatten()\n    position_dict = dict(zip(coordinate_names, positions))\n    position_dict['energy'] = energy\n    data.append(position_dict)\ndata = pd.DataFrame(data).sample(len(data))\ndata\n\n\n\n\n\n  \n    \n      \n      x_0\n      y_0\n      z_0\n      x_1\n      y_1\n      z_1\n      x_2\n      y_2\n      z_2\n      x_3\n      ...\n      x_36\n      y_36\n      z_36\n      x_37\n      y_37\n      z_37\n      x_38\n      y_38\n      z_38\n      energy\n    \n  \n  \n    \n      2081\n      -2.025968\n      -1.526113\n      0.471518\n      -1.884343\n      -0.091611\n      0.768404\n      -2.493246\n      0.904347\n      -0.150466\n      -3.968555\n      ...\n      5.829282\n      -0.540280\n      0.305073\n      5.066311\n      0.447374\n      -0.963550\n      0.706002\n      -1.593746\n      -0.671971\n      60.078391\n    \n    \n      2892\n      1.145835\n      2.157197\n      1.173924\n      1.553008\n      0.792545\n      0.679945\n      2.402984\n      1.072336\n      -0.573174\n      2.912737\n      ...\n      -4.516304\n      -1.368212\n      -1.949598\n      -3.007969\n      -0.737924\n      -2.657923\n      0.413338\n      -0.383882\n      3.032780\n      54.643342\n    \n    \n      2908\n      -2.261174\n      -1.812320\n      0.557964\n      -1.653680\n      -0.820724\n      -0.412912\n      -2.625928\n      0.064635\n      -1.091169\n      -3.479011\n      ...\n      4.395191\n      2.263609\n      -0.285119\n      4.413928\n      1.583511\n      -2.004739\n      0.673824\n      -2.767714\n      1.452331\n      58.070317\n    \n    \n      2142\n      -2.028595\n      0.630910\n      1.879652\n      -1.588651\n      0.399215\n      0.430564\n      -2.328571\n      -0.724311\n      -0.198702\n      -3.830054\n      ...\n      5.315286\n      0.309871\n      1.626134\n      4.484393\n      1.049942\n      0.185649\n      0.094166\n      -1.954269\n      -0.621633\n      48.456586\n    \n    \n      2176\n      1.400570\n      0.320934\n      -1.560453\n      1.405835\n      -0.090365\n      -0.143190\n      2.421500\n      0.653734\n      0.656290\n      3.798113\n      ...\n      -5.138237\n      1.185416\n      0.975463\n      -4.411483\n      2.139480\n      -0.360764\n      -1.501934\n      -1.534363\n      -1.896296\n      69.016981\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      161\n      -2.551243\n      -2.341812\n      -0.087507\n      -1.859942\n      -1.059513\n      -0.631920\n      -2.766438\n      0.092494\n      -0.468552\n      -3.246241\n      ...\n      5.090904\n      1.614871\n      -0.582213\n      6.214801\n      0.251601\n      -0.288913\n      0.232381\n      -0.793056\n      -2.346602\n      56.151153\n    \n    \n      2447\n      -1.728608\n      1.547571\n      -1.191712\n      -1.610555\n      0.070258\n      -1.014880\n      -2.778171\n      -0.470428\n      -0.177357\n      -2.811371\n      ...\n      2.717658\n      0.230623\n      2.968373\n      4.054951\n      -0.786646\n      2.285694\n      0.281805\n      -1.686151\n      -2.452770\n      51.514206\n    \n    \n      2485\n      -2.999042\n      -1.946034\n      -0.060973\n      -1.647301\n      -1.036666\n      -0.369187\n      -2.280691\n      0.258492\n      0.133163\n      -1.602938\n      ...\n      4.362087\n      2.483250\n      -0.200658\n      4.077290\n      0.734056\n      -0.036143\n      2.196317\n      -2.556574\n      1.441407\n      106.410259\n    \n    \n      2451\n      1.639677\n      0.645292\n      0.990599\n      1.507748\n      0.069695\n      -0.373349\n      2.763170\n      0.042261\n      -1.177419\n      3.905548\n      ...\n      -3.854125\n      -0.963152\n      -0.962775\n      -4.932492\n      -1.829539\n      0.126004\n      -0.815809\n      -1.587928\n      -0.204028\n      77.131786\n    \n    \n      377\n      -2.183812\n      -0.690793\n      -1.609484\n      -1.483993\n      -0.711148\n      -0.248412\n      -1.774222\n      0.434548\n      0.617820\n      -3.214363\n      ...\n      3.639583\n      2.781684\n      -0.895964\n      3.357913\n      1.807178\n      -2.404438\n      1.712529\n      -2.508573\n      -0.427531\n      65.279656\n    \n  \n\n3206 rows Ã— 118 columns\n\n\n\nGiven this data, we can build a model. We will use a gradient boosting regressor from scikit-learn. We will also split the data into a training and a test set. In later sections, we will see why this is important. But for now, let us us just appreciate that a test setâ€”conformers we did not train onâ€”will give us a measure of how well our model will perform on new, unseen, conformers.\n\npositions = data[coordinate_names] # X\nenergies = data['energy'] # y\n\n# split into training and test set\ntrain_points = 3000\ntrain_positions = positions[:train_points]\ntest_positions = positions[train_points:]\ntrain_energies = energies[:train_points]\ntest_energies = energies[train_points:]\n\n# train a model\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nmodel = HistGradientBoostingRegressor()\nmodel.fit(train_positions, train_energies)\n\nHistGradientBoostingRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.HistGradientBoostingRegressorHistGradientBoostingRegressor()\n\n\nOnce we have trained a model, we can use it to predict the energies of new conformers. Letâ€™s first see how well it does on the data it was trained on.\n\ntrain_predictions = model.predict(train_positions)\n\n\ndensity_scatter_with_hist(train_energies.values, train_predictions, xlabel='True energy', ylabel='Predicted energy')\n\n<AxesSubplot: xlabel='True energy', ylabel='Predicted energy'>\n\n\n\n\n\nThis looks pretty good. But how well does it do on new conformers? Letâ€™s see.\n\ntest_predictions = model.predict(test_positions)\n\ndensity_scatter_with_hist(test_energies.values, test_predictions, xlabel='True energy', ylabel='Predicted energy')\n\n<AxesSubplot: xlabel='True energy', ylabel='Predicted energy'>\n\n\n\n\n\nFrom physics we know that (without external field) the energy of a molecule does not depend on where in space it is. That is, if we translate a molecule along \\([1, 1, 1]\\), the energy should not change.\n\n# translate the molecule along [1, 1, 1]\ntranslated_positions = train_positions + 1\ntranslated_predictions = model.predict(translated_positions)\ndensity_scatter_with_hist(train_energies.values, translated_predictions)\n\n<AxesSubplot: xlabel='Actual', ylabel='Predicted'>\n\n\n\n\n\nThis is not what we expect. Our model shows completly unphysical behavior and predicts a different energy for the same conformers in different positions in space.\nTo fix this, and related problems, we need to use a more elaborate approach to building a model.\n\n\n\nMmaking predictions invariant/equivariant to transformations\nInvariance and equivariance are terms that have become very relevant in ML. It is always important to mention with respect to what operation something is invariant and equivariant; if people donâ€™t mention this they often refer to the symmetry operations of the Euclidean group which comprises all translations, rotations, and reflection. Invariant means that the property of interest does not change under those operations. Equivariant means that it changes in the same way. The energy, for example, is invariant and the forces are equivariant.\n\nWhat are symmetries we would like to respect?\nBefore we can talk about how to build a model that respects symmetries, we need to know what symmetries we would like to respect.\nIn the case of molecules, we would like to respect the following symmetries:\n\ntranslation: that is, if we move a molecule along a vector, the energy should not change (see above)\nrotation: that is, if we rotate a molecule, the energy should not change\npermutation of atoms: that is the order with which we put the atoms in the model does not matter\n\nFor crystals, we additionally need to respect periodicity. That is, for intensive properties, there should be no difference between using a unit cell or a super cell of that unit cell as input for a model.\nBroadly speaking, there are three different ways to build models that respect symmetries.\n\nData augmentation: This is the most straightforward approach. We can generate new data points by applying the symmetries to the existing data points. For example, we can generate new conformers by rotating the existing conformers. This approach is very simple to implement, but it can be very expensive. For example, if we want to generate new conformers by rotating the existing conformers, we need to generate a new conformer for every rotation. This approach is often used for computer vision pipelines in which you might want to detect a cat in an image independent of the orientation. In this case, you can generate new images by rotating the existing images.\nFeatures that are invariant/equivariant : This approach is more sophisticated. We can build features that are invariant/equivariant to the symmetries we want to respect. For example, we can build features that are invariant to rotation. In the case of force field such features are bond lengths and angles. This is approach is widely used in ML for chemistry and materials science.\nModels that are invariant/equivariant: Alternatively, one can build special models that can consume point clouds as inputs and are equivariant to the symmetries we want to respect. We will not discuss this in detail, but you can find starting points in this perspective by Tess Smidt.\n\n\n\nInvariant/equivariant features\n\nSymmetry functions\n\n\nFingerprints\n\n\nCorrelation functions\n\n\nSymmetry functions\n\n\nCheaper computations"
  },
  {
    "objectID": "teaching/ml/ml_notes.html#training-a-model",
    "href": "teaching/ml/ml_notes.html#training-a-model",
    "title": "Workshop on ML for materials science",
    "section": "Training a model",
    "text": "Training a model\n\nHow to know if a model is good?\nBefore we can proceed to building models, we need to estabilsh a way to measure how good a model is.\nInterestingly, this is not as trivial as it may sound. To realize this, it is useful to formally write down what we mean by a good model.\n\nEmpirical risk minimization\nLetâ€™s assume we have some input space \\(\\mathcal{X}\\) and some output space \\(\\mathcal{Y}\\). We can think of \\(\\mathcal{X}\\) as the space of all possible inputs and \\(\\mathcal{Y}\\) as the space of all possible outputs. For example, \\(\\mathcal{X}\\) could be the space of all possible molecules and \\(\\mathcal{Y}\\) could be the space of all possible energies. We want to learn a function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\) that maps inputs to outputs. We can think of \\(f\\) as a model that we want to train.\nTo build this models we have samples of the joint distribution \\(p(x, y)\\), where \\(x\\) is an input and \\(y\\) is the corresponding output. We can think of this as a set of data points \\(\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}\\).\nIf we now define a loss function \\(L\\) we can compute the risk, which is the expected value of the loss function:\n\\[\nR(h)={\\mathbf  {E}}[L(f(x),y)]=\\int L(f(x),y)\\,dP(x,y).\n\\]\nour goal is to find a model \\(f\\) that minimizes the risk:\n\\[\n{\\displaystyle h^{*}={\\underset {h\\in {\\mathcal {H}}}{\\operatorname {arg\\,min} }}\\,{R(h)}.}\n\\]\nIn practice we cannot compute this. The reason is that we do not have access to the joint distribution \\(p(x, y)\\), but only to a finite set of samples \\(\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}\\).\n\n\n\nLinear regression\nimport jax.numpy as jnp\n\ndef linear_regression(x, w, b):\n    return jnp.dot(x, w) + b\ndef loss(w, b):\n    prediction = linear_regression(x, w, b)\n    return jnp.mean((prediction - y) ** 2)\ndef init_params(num_feat):\n    return np.random.normal(size=(num_feat,)), 0.0\nloss_grad = jax.grad(loss, argnums=(0, 1))\nlearning_rate = 1e-6\nnum_epochs = 1000"
  },
  {
    "objectID": "teaching/ml/ml_notes.html#bias-variance-trade-off",
    "href": "teaching/ml/ml_notes.html#bias-variance-trade-off",
    "title": "Workshop on ML for materials science",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off"
  },
  {
    "objectID": "teaching/ml/ml_notes.html#hyperparameters",
    "href": "teaching/ml/ml_notes.html#hyperparameters",
    "title": "Workshop on ML for materials science",
    "section": "Hyperparameters",
    "text": "Hyperparameters"
  },
  {
    "objectID": "teaching/ml/ml_notes.html#kernel-trick",
    "href": "teaching/ml/ml_notes.html#kernel-trick",
    "title": "Workshop on ML for materials science",
    "section": "Kernel trick",
    "text": "Kernel trick\n\n\n\nKernel-based machine learning can be thought of expressing the property of interest via an expansion in a basis spanned by the structures in the training set. Figure taken from M. A. Caro,Â ArkhimedesÂ 2018,Â 3,Â 21."
  },
  {
    "objectID": "teaching/ml/ml_notes.html#feature-importance",
    "href": "teaching/ml/ml_notes.html#feature-importance",
    "title": "Workshop on ML for materials science",
    "section": "Feature importance",
    "text": "Feature importance\n\nPermutation feature importance"
  },
  {
    "objectID": "teaching/ml/ml_notes.html#feature-selection",
    "href": "teaching/ml/ml_notes.html#feature-selection",
    "title": "Workshop on ML for materials science",
    "section": "Feature selection",
    "text": "Feature selection\n\nCurse of dimensionality\nFor understanding the curse of dimensionality, it is useful to consider a very simple ML model, the \\(k\\)-nearest neighbors model. In this model, we have a set of training points \\(\\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}\\), where \\(x_i\\) is a vector of features and \\(y_i\\) is the corresponding label. To make a prediction, we compute the distance between the input and all training points and return the mode of the labels of the \\(k\\) closest training points.\nClearly, in this algorithm it is important to find the nearest neighbor. In general, this is important in many algorithms, for instance also in kernel-based learning.\nLetâ€™s now ask ourself what part of the space we need to find the nearest neighbors.\n\nFor this, letâ€™s start considering a unit cube \\([0,1]^d\\) and \\(n\\) data points \\(x_i\\) sampled uniformly from this cube.\nThe smallest hypercube that contains \\(k\\) out of the \\(n\\) points has the following edge length\n\\[\nl^d = \\frac{k}{n} \\quad \\Rightarrow \\quad l = \\left(\\frac{k}{n}\\right)^{1/d}\n\\]\nIf we plot this for different values of \\(d\\) we get the following plot:\n\nimport matplotlib.pyplot as plt\nimport numpy as np \n\ndef length(d, k=5, n=10_000):\n    return (k/n)**(1/d)\n\nd = np.arange(1, 1000)\n\nplt.plot(d, length(d))\nplt.xlabel('numbr of dimensions')\nplt.ylabel('length of hypercube that contains k neighbors')\n\nText(0, 0.5, 'length of hypercube that contains k neighbors')\n\n\n\n\n\nClearly, for large \\(d\\) the length approaches 1â€”which means that all points are now almost equally far apart and comparing distances no longer makes much sense.\nWe can also check this by performing a simulation: Generating random \\(d\\) dimensional points and computing the distance between them. We can then plot the distribution of distances.\n\nfrom scipy.spatial import distance_matrix\ndimensions = [2, 5, 10, 100, 10_000]\nnum_points = 1000\n\nfig, axes = plt.subplots(1, len(dimensions), sharey='all')\n\ndef get_distances(d, num_points):\n    points = np.random.uniform(size=(num_points, d))\n    distances = distance_matrix(points, points)\n    return np.array(distances).flatten()\n\nfor d, ax in zip(dimensions, axes):\n    distances = get_distances(d, num_points)\n    ax.hist(distances, bins=20)\n    ax.set_title(f'd={d} \\n cv={distances.std()/distances.mean():.2f}')\n\n\n\n\nClearly, for large \\(d\\) the distances are almost the same (the histograms are much more peaked). We can also see this in terms of the coefficient of variation (cv), which is the standard deviation divided by the mean. For large \\(d\\) the cv is very small, which means that the distances are very similar.\n\n\nFeature selection approaches"
  },
  {
    "objectID": "teaching/ml/ml_notes.html#feature-projection",
    "href": "teaching/ml/ml_notes.html#feature-projection",
    "title": "Workshop on ML for materials science",
    "section": "Feature projection",
    "text": "Feature projection\n\nPrincipal component analysis\n\n\nt-distributed stochastic neighbor embedding"
  },
  {
    "objectID": "teaching/ml/ml_notes.html#feature-learning",
    "href": "teaching/ml/ml_notes.html#feature-learning",
    "title": "Workshop on ML for materials science",
    "section": "Feature learning",
    "text": "Feature learning"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Kevin's Homepage",
    "section": "",
    "text": "In case you canâ€™t see the embedded document, you can download it here."
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Bio",
    "section": "",
    "text": "Kevin Jablonka obtained his bachelorâ€™s degree in chemistry at TU Munich. He joined EPFL for his masterâ€™s studies (and an extended study degree in applied machine learning), after which he joined Berend Smitâ€™s group for a Ph.D. Kevinâ€™s research interests are in the digitization of chemistry. For this, he got involved in the cheminfo ELN ecosystem. He also developed a toolbox for digital reticular chemistry. Using tools from this toolbox, he addressed questions from the atom up to the pilot-plant scale.\n\nMore\nIf you need more info, see my CV. If you need a headshot, you can use this one: portrait.jpg."
  }
]