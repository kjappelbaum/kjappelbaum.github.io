<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Kevin&#39;s Homepage</title>
<link>https://kjablonka.com/index.html</link>
<atom:link href="https://kjablonka.com/index.xml" rel="self" type="application/rss+xml"/>
<description>Kevin Maik Jablonka&#39;s personal homepage</description>
<image>
<url>https://kjablonka.com/quarto.png</url>
<title>Kevin&#39;s Homepage</title>
<link>https://kjablonka.com/index.html</link>
</image>
<generator>quarto-1.2.313</generator>
<lastBuildDate>Wed, 01 May 2024 22:00:00 GMT</lastBuildDate>
<item>
  <title>Building a GPT that can generate molecules from scratch</title>
  <link>https://kjablonka.com/blog/posts/building_an_llm/index.html</link>
  <description><![CDATA[ 



<p>Molecules can be represented in multitude of ways. One of the most widely used representations is to use text, for example in the so-called SMILES notation. In SMILES notation, a molecule is represented as a string of characters, where each character represents an atom or a bond. For example, the SMILES notation for ethanol is <code>CCO</code>. The one for benzene is <code>c1ccccc1</code>. You see that hydrogen atoms are typically omitted in SMILES notation, and that lower case letters are used for aromatic atoms. There is a <a href="http://opensmiles.org/opensmiles.html">full grammar for SMILES notation</a> and <a href="https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf">various alternative representations</a>, but we will stick to this simple version for this notebook.</p>
<p>Important problems that our final solution will need to be able to solve are:</p>
<ul>
<li>dealing with inputs of different lengths (e.g, different number of atoms in different molecules)</li>
<li>incorporating information about the semantic meaning of the atoms in the molecule (to obtain meaningful molecules, the model, e.g., should probably “know” what kind of bonds carbon can form)</li>
<li>dealing with the interaction between atoms in the molecule (not all arrangements of atoms are equally likely)</li>
</ul>
<div class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd </span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> rdkit <span class="im" style="color: #00769E;">import</span> Chem</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> nn</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> Dataset, DataLoader</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">import</span> re </span>
<span id="cb1-7"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> List</span>
<span id="cb1-8"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np </span>
<span id="cb1-9"><span class="im" style="color: #00769E;">from</span> math <span class="im" style="color: #00769E;">import</span> exp</span>
<span id="cb1-10"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb2-2"><span class="kw" style="color: #003B4F;">def</span> get_num_parameters(model):</span>
<span id="cb2-3">    <span class="co" style="color: #5E5E5E;">"""Return the number of trainable parameters in the model."""</span></span>
<span id="cb2-4">    <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">sum</span>(p.numel() <span class="cf" style="color: #003B4F;">for</span> p <span class="kw" style="color: #003B4F;">in</span> model.parameters() <span class="cf" style="color: #003B4F;">if</span> p.requires_grad)</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;">def</span> get_num_parameters_per_layer(model):</span>
<span id="cb2-7">    <span class="co" style="color: #5E5E5E;">"""Return the number of trainable parameters in the model per layer."""</span></span>
<span id="cb2-8">    layers <span class="op" style="color: #5E5E5E;">=</span> {}</span>
<span id="cb2-9">    <span class="cf" style="color: #003B4F;">for</span> name, p <span class="kw" style="color: #003B4F;">in</span> model.named_parameters():</span>
<span id="cb2-10">        <span class="cf" style="color: #003B4F;">if</span> p.requires_grad:</span>
<span id="cb2-11">            layers[name] <span class="op" style="color: #5E5E5E;">=</span> p.numel()</span>
<span id="cb2-12">    <span class="cf" style="color: #003B4F;">return</span> layers</span>
<span id="cb2-13"></span>
<span id="cb2-14"></span>
<span id="cb2-15"><span class="kw" style="color: #003B4F;">def</span> set_device():</span>
<span id="cb2-16">    <span class="cf" style="color: #003B4F;">if</span> torch.backends.mps.is_available():</span>
<span id="cb2-17">        <span class="cf" style="color: #003B4F;">if</span> torch.backends.mps.is_built():</span>
<span id="cb2-18">            device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'mps'</span></span>
<span id="cb2-19">    <span class="cf" style="color: #003B4F;">elif</span> torch.cuda.is_available():</span>
<span id="cb2-20">        device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'cuda'</span></span>
<span id="cb2-21">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb2-22">        device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'cpu'</span></span>
<span id="cb2-23">    <span class="cf" style="color: #003B4F;">return</span> device</span>
<span id="cb2-24"></span>
<span id="cb2-25">device <span class="op" style="color: #5E5E5E;">=</span> set_device()</span></code></pre></div>
</div>
<section id="dealing-with-smiles" class="level2">
<h2 class="anchored" data-anchor-id="dealing-with-smiles">Dealing with SMILES</h2>
<p>Before we can do anything, we need to obtain data. For doing so, we will need a dataset of SMILES strings. We will use the <a href="https://zinc.docking.org/">ZINC dataset</a> which is a public database of commercially-available compounds. We will use the <code>250k</code> subset of the dataset which contains 250,000 compounds.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="op" style="color: #5E5E5E;">!</span>wget <span class="st" style="color: #20794D;">'https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/zinc15_250K_2D.tar.gz'</span></span>
<span id="cb3-2"><span class="op" style="color: #5E5E5E;">!</span>tar <span class="op" style="color: #5E5E5E;">-</span>xzf zinc15_250K_2D.tar.gz</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>/Users/kevinmaikjablonka/.zshenv:.:1: no such file or directory: /Users/kevinmaikjablonka/.cargo/env
--2024-05-02 12:20:55--  https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/zinc15_250K_2D.tar.gz
Resolving deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)... 52.219.120.49, 52.219.120.145, 52.219.193.50, ...
Connecting to deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)|52.219.120.49|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 6941580 (6.6M) [application/x-gzip]
Saving to: ‘zinc15_250K_2D.tar.gz’

zinc15_250K_2D.tar. 100%[===================&gt;]   6.62M  1.25MB/s    in 14s     

2024-05-02 12:21:11 (497 KB/s) - ‘zinc15_250K_2D.tar.gz’ saved [6941580/6941580]

/Users/kevinmaikjablonka/.zshenv:.:1: no such file or directory: /Users/kevinmaikjablonka/.cargo/env</code></pre>
</div>
</div>
<p>After downloading and extracting the dataset, we can load it into memory and take a look at some molecules.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(<span class="st" style="color: #20794D;">'zinc15_250K_2D.csv'</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">Chem.MolFromSmiles(df[<span class="st" style="color: #20794D;">'smiles'</span>][<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<p><img src="https://kjablonka.com/blog/posts/building_an_llm/index_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Before we continue any further, we will also create train/valid and test sets.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">train, valid, test <span class="op" style="color: #5E5E5E;">=</span> torch.utils.data.random_split(df[<span class="st" style="color: #20794D;">'smiles'</span>], [<span class="dv" style="color: #AD0000;">200000</span>, <span class="dv" style="color: #AD0000;">25000</span>, <span class="dv" style="color: #AD0000;">25000</span>])</span></code></pre></div>
</div>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<p>For training a language model, we will need to split the SMILES into tokens. Tokens are the smallest units of text that the model will work with. The model will learn to predict a molecule token by token. There is not one correct way to do this, but one very common way is to split the SMILES into “chemical tokens”. For this, <a href="https://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc02339e">Philippe Schwaller wrote down a regular expression</a>.</p>
<p>Commonly used other tokenization methods are:</p>
<ul>
<li><a href="https://github.com/google/sentencepiece">SentencePiece</a></li>
<li><a href="https://github.com/openai/tiktoken">Byte-Pair Encoding (BPE)</a></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some try to move completely away from tokenization and <a href="https://byte-gpt.github.io/">directly</a> <a href="https://www.youtube.com/watch?v=kcd0BTKJuXk">model</a> <a href="https://arxiv.org/abs/2105.13626">bytes</a>.</p>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;">def</span> tokenize(smiles: <span class="bu" style="color: null;">str</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> List[<span class="bu" style="color: null;">str</span>]:</span>
<span id="cb8-2">    <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb8-3"><span class="co" style="color: #5E5E5E;">    Tokenize a SMILES</span></span>
<span id="cb8-4"></span>
<span id="cb8-5"><span class="co" style="color: #5E5E5E;">    Args:</span></span>
<span id="cb8-6"><span class="co" style="color: #5E5E5E;">        smiles (str): SMILES string</span></span>
<span id="cb8-7"><span class="co" style="color: #5E5E5E;">    </span></span>
<span id="cb8-8"><span class="co" style="color: #5E5E5E;">    Returns:</span></span>
<span id="cb8-9"><span class="co" style="color: #5E5E5E;">        List[str]: List of tokens</span></span>
<span id="cb8-10"><span class="co" style="color: #5E5E5E;">    """</span></span>
<span id="cb8-11">    SMI_REGEX_PATTERN <span class="op" style="color: #5E5E5E;">=</span> <span class="vs" style="color: #20794D;">r"""(\[[^\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\(|\)|\.|=|#|-|\+|\\|\/|:|~|@|\?|&gt;&gt;?|\*|\$|\%[0-9]</span><span class="sc" style="color: #5E5E5E;">{2}</span><span class="vs" style="color: #20794D;">|[0-9])"""</span></span>
<span id="cb8-12">    </span>
<span id="cb8-13">    <span class="cf" style="color: #003B4F;">return</span> re.findall(SMI_REGEX_PATTERN, smiles)</span></code></pre></div>
</div>
<p>The molecule, CCO (ethanol), is tokenized as [‘C’, ‘C’, ‘O’].</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">tokenize(<span class="st" style="color: #20794D;">'CCO'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>['C', 'C', 'O']</code></pre>
</div>
</div>
<section id="converting-tokens-into-ids" class="level4">
<h4 class="anchored" data-anchor-id="converting-tokens-into-ids">Converting tokens into IDs</h4>
<p>For inputing tokens into a model, we will need to convert them into numbers.</p>
<p>To do so, we will set up a “vocabulary” which is a dictionary that maps tokens to integers. The vocabulary also defines the tokens that are known to the model.</p>
</section>
<section id="special-tokens" class="level4">
<h4 class="anchored" data-anchor-id="special-tokens">Special tokens</h4>
<p>Our model will be fed sequences of fixed length. Our SMILES, however, are of variable length. We will have to pad them to a fixed length. We will use a padding token for this purpose. That is, we will add a specific “[PAD]” token to the vocabulary which only serves the purpose of padding.</p>
<p>Often, we also add other tokens such as <code>[EOS]</code> (end of sequence) or <code>[BOS]</code> (beginning of sequence).</p>
<p>They are typically used as follows:</p>
<ul>
<li><code>[BOS]</code> is added at the beginning of each sequence</li>
<li><code>[EOS]</code> is added at the end of each sequence</li>
<li><code>[PAD]</code> is added to the end of each sequence to pad it to a fixed length</li>
<li><code>[UNK]</code> is used to replace tokens that are not in the vocabulary</li>
</ul>
<p>We can put all of this together in a <code>Tokenizer</code> class.</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="kw" style="color: #003B4F;">class</span> Tokenizer:</span>
<span id="cb11-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, tokens: List[<span class="bu" style="color: null;">str</span>], eos: <span class="bu" style="color: null;">str</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'[EOS]'</span>, bos: <span class="bu" style="color: null;">str</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'[BOS]'</span>, pad: <span class="bu" style="color: null;">str</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'[PAD]'</span>, unk: <span class="bu" style="color: null;">str</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'[UNK]'</span>):</span>
<span id="cb11-3">        <span class="va" style="color: #111111;">self</span>.tokens <span class="op" style="color: #5E5E5E;">=</span> [pad, bos, eos, unk] <span class="op" style="color: #5E5E5E;">+</span> tokens</span>
<span id="cb11-4">        <span class="va" style="color: #111111;">self</span>._token_to_index <span class="op" style="color: #5E5E5E;">=</span> {token: index <span class="cf" style="color: #003B4F;">for</span> index, token <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(<span class="va" style="color: #111111;">self</span>.tokens)}</span>
<span id="cb11-5">        <span class="va" style="color: #111111;">self</span>.index_to_token <span class="op" style="color: #5E5E5E;">=</span> {index: token <span class="cf" style="color: #003B4F;">for</span> index, token <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(<span class="va" style="color: #111111;">self</span>.tokens)}</span>
<span id="cb11-6"></span>
<span id="cb11-7">    <span class="kw" style="color: #003B4F;">def</span> token_to_index(<span class="va" style="color: #111111;">self</span>, token: <span class="bu" style="color: null;">str</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="bu" style="color: null;">int</span>:</span>
<span id="cb11-8">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb11-9">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>._token_to_index[token]</span>
<span id="cb11-10">        <span class="cf" style="color: #003B4F;">except</span> <span class="pp" style="color: #AD0000;">KeyError</span>:</span>
<span id="cb11-11">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>._token_to_index[<span class="st" style="color: #20794D;">'[UNK]'</span>]</span>
<span id="cb11-12">        </span>
<span id="cb11-13">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__len__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb11-14">        <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.tokens)</span>
<span id="cb11-15">    </span>
<span id="cb11-16">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, item):</span>
<span id="cb11-17">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.token_to_index[item]</span>
<span id="cb11-18">    </span>
<span id="cb11-19">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__contains__</span>(<span class="va" style="color: #111111;">self</span>, item):</span>
<span id="cb11-20">        <span class="cf" style="color: #003B4F;">return</span> item <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.tokens</span>
<span id="cb11-21"></span>
<span id="cb11-22">    <span class="kw" style="color: #003B4F;">def</span> encode(<span class="va" style="color: #111111;">self</span>, smiles: <span class="bu" style="color: null;">str</span>, add_sos: <span class="bu" style="color: null;">bool</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, add_eos: <span class="bu" style="color: null;">bool</span><span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> List[<span class="bu" style="color: null;">int</span>]:</span>
<span id="cb11-23">        <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb11-24"><span class="co" style="color: #5E5E5E;">        Encode a SMILES into a list of indices</span></span>
<span id="cb11-25"></span>
<span id="cb11-26"><span class="co" style="color: #5E5E5E;">        Args:</span></span>
<span id="cb11-27"><span class="co" style="color: #5E5E5E;">            smiles (str): SMILES string</span></span>
<span id="cb11-28"><span class="co" style="color: #5E5E5E;">            add_sos (bool): Add start of sentence token</span></span>
<span id="cb11-29"><span class="co" style="color: #5E5E5E;">            add_eos (bool): Add end of sentence token</span></span>
<span id="cb11-30"></span>
<span id="cb11-31"><span class="co" style="color: #5E5E5E;">        Returns:</span></span>
<span id="cb11-32"><span class="co" style="color: #5E5E5E;">            List[int]: List of indices</span></span>
<span id="cb11-33"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb11-34">        tokens <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-35">        <span class="cf" style="color: #003B4F;">if</span> add_sos:</span>
<span id="cb11-36">            tokens.append(<span class="va" style="color: #111111;">self</span>.token_to_index(<span class="st" style="color: #20794D;">'[BOS]'</span>))</span>
<span id="cb11-37">        tokens <span class="op" style="color: #5E5E5E;">+=</span> [<span class="va" style="color: #111111;">self</span>.token_to_index(token) <span class="cf" style="color: #003B4F;">for</span> token <span class="kw" style="color: #003B4F;">in</span> tokenize(smiles)]</span>
<span id="cb11-38">        <span class="cf" style="color: #003B4F;">if</span> add_eos:</span>
<span id="cb11-39">            tokens.append(<span class="va" style="color: #111111;">self</span>.token_to_index(<span class="st" style="color: #20794D;">'[EOS]'</span>))</span>
<span id="cb11-40">        <span class="cf" style="color: #003B4F;">return</span> tokens</span>
<span id="cb11-41">    </span>
<span id="cb11-42">    <span class="kw" style="color: #003B4F;">def</span> decode(<span class="va" style="color: #111111;">self</span>, indices: List[<span class="bu" style="color: null;">int</span>], strip_special_tokens: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="bu" style="color: null;">str</span>: </span>
<span id="cb11-43">        <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb11-44"><span class="co" style="color: #5E5E5E;">        Decode a list of indices into a SMILES</span></span>
<span id="cb11-45"></span>
<span id="cb11-46"><span class="co" style="color: #5E5E5E;">        Args:</span></span>
<span id="cb11-47"><span class="co" style="color: #5E5E5E;">            indices (List[int]): List of indices</span></span>
<span id="cb11-48"><span class="co" style="color: #5E5E5E;">        </span></span>
<span id="cb11-49"><span class="co" style="color: #5E5E5E;">        Returns:</span></span>
<span id="cb11-50"><span class="co" style="color: #5E5E5E;">            str: SMILES string</span></span>
<span id="cb11-51"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb11-52">        decoded <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">''</span>.join([<span class="va" style="color: #111111;">self</span>.index_to_token[index] <span class="cf" style="color: #003B4F;">for</span> index <span class="kw" style="color: #003B4F;">in</span> indices])</span>
<span id="cb11-53">        <span class="cf" style="color: #003B4F;">if</span> strip_special_tokens:</span>
<span id="cb11-54">            <span class="cf" style="color: #003B4F;">return</span> decoded.replace(<span class="st" style="color: #20794D;">'[PAD]'</span>, <span class="st" style="color: #20794D;">''</span>).replace(<span class="st" style="color: #20794D;">'[BOS]'</span>, <span class="st" style="color: #20794D;">''</span>).replace(<span class="st" style="color: #20794D;">'[EOS]'</span>, <span class="st" style="color: #20794D;">''</span>)</span>
<span id="cb11-55">        <span class="cf" style="color: #003B4F;">return</span> decoded</span></code></pre></div>
</div>
<p>To instantiate the tokenizer, we need to pass the list of tokens that we want to use. (This is sometimes called “training” the tokenizer, but in this case, we are just defining the tokens that we want to use.) We will use the following tokens:</p>
<div class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">tokens <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>()</span>
<span id="cb12-2">lengths <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb12-3"><span class="cf" style="color: #003B4F;">for</span> smiles <span class="kw" style="color: #003B4F;">in</span> train.dataset.values:</span>
<span id="cb12-4">    tokens_ <span class="op" style="color: #5E5E5E;">=</span> tokenize(smiles)</span>
<span id="cb12-5">    tokens.update(tokens_)</span>
<span id="cb12-6">    lengths.append(<span class="bu" style="color: null;">len</span>(tokens_))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">plt.hist(lengths, bins<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">50</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>(array([3.0000e+00, 4.0000e+00, 7.0000e+00, 0.0000e+00, 2.3000e+01,
        5.6000e+01, 0.0000e+00, 7.8000e+01, 2.0100e+02, 0.0000e+00,
        3.8900e+02, 8.0200e+02, 1.4320e+03, 0.0000e+00, 2.5760e+03,
        3.9450e+03, 0.0000e+00, 5.8570e+03, 8.0820e+03, 0.0000e+00,
        1.0313e+04, 1.2675e+04, 1.4914e+04, 0.0000e+00, 1.7137e+04,
        1.8718e+04, 0.0000e+00, 2.0510e+04, 2.0796e+04, 0.0000e+00,
        2.1073e+04, 2.0330e+04, 1.8396e+04, 0.0000e+00, 1.6193e+04,
        1.2172e+04, 0.0000e+00, 9.8210e+03, 5.8470e+03, 0.0000e+00,
        3.9460e+03, 2.1220e+03, 9.6800e+02, 0.0000e+00, 4.1200e+02,
        1.4500e+02, 0.0000e+00, 4.6000e+01, 1.0000e+01, 1.0000e+00]),
 array([17. , 17.7, 18.4, 19.1, 19.8, 20.5, 21.2, 21.9, 22.6, 23.3, 24. ,
        24.7, 25.4, 26.1, 26.8, 27.5, 28.2, 28.9, 29.6, 30.3, 31. , 31.7,
        32.4, 33.1, 33.8, 34.5, 35.2, 35.9, 36.6, 37.3, 38. , 38.7, 39.4,
        40.1, 40.8, 41.5, 42.2, 42.9, 43.6, 44.3, 45. , 45.7, 46.4, 47.1,
        47.8, 48.5, 49.2, 49.9, 50.6, 51.3, 52. ]),
 &lt;BarContainer object of 50 artists&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://kjablonka.com/blog/posts/building_an_llm/index_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">tokenizer <span class="op" style="color: #5E5E5E;">=</span> Tokenizer(<span class="bu" style="color: null;">list</span>(tokens))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">tokenizer.encode(<span class="st" style="color: #20794D;">'CCO'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>[45, 45, 38]</code></pre>
</div>
</div>
</section>
</section>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">Embeddings</h3>
<p>Currently, we only encode the SMILES strings into a list of indices. There is no inherent meaning to the indices themselves, and we can improve modeling by representing each index as a vector. We call those vectors embeddings, but they are nothing more than a vector representation–like a feature vector–for each index.</p>
<p>Ideally, those vectors ensure that similar indices are close to each other in the embedding space. There are many ways to create those embeddings. But for now it is only important to know this concept.</p>
</section>
<section id="positional-encoding" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding">Positional encoding</h3>
<p>The embeddings we just created contain only information about their identity. However, they contain no information about their position in the sequence.</p>
<p>To add positional information, we can add a positional encoding to the embeddings. Again, there are many ways to do this.</p>
<p>A very simple way is called <em>absolute positional encoding</em>. For this we simply add the position index to the embedding vector.</p>
<p>For example</p>
<div class="sourceCode" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">B, T, C <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">4</span> <span class="co" style="color: #5E5E5E;"># batch size, sequence length, embedding size</span></span>
<span id="cb18-2">x <span class="op" style="color: #5E5E5E;">=</span> torch.rand(B, T, C)</span>
<span id="cb18-3">pos <span class="op" style="color: #5E5E5E;">=</span> torch.arange(T).unsqueeze(<span class="dv" style="color: #AD0000;">0</span>).repeat(B, <span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</section>
<section id="language-modeling-dataset" class="level3">
<h3 class="anchored" data-anchor-id="language-modeling-dataset">Language modeling dataset</h3>
<p>A dataset class is a class that inherits from <code>torch.utils.data.Dataset</code>. It is used to load data into a model.</p>
<p>The most important methods of a dataset class are:</p>
<ul>
<li><code>__len__</code>: This method returns the length of the dataset. It is used by the <code>DataLoader</code> to determine how many batches to load.</li>
<li><code>__getitem__</code>: This method returns a single sample from the dataset. It is used by the <code>DataLoader</code> to load a batch of samples.</li>
</ul>
<div class="cell" data-execution_count="155">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="kw" style="color: #003B4F;">class</span> CausalLanguageModelingDataset(Dataset):</span>
<span id="cb19-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, texts, tokenizer, max_length):</span>
<span id="cb19-3">        <span class="va" style="color: #111111;">self</span>.tokenizer <span class="op" style="color: #5E5E5E;">=</span> tokenizer</span>
<span id="cb19-4">        <span class="va" style="color: #111111;">self</span>.texts <span class="op" style="color: #5E5E5E;">=</span> texts</span>
<span id="cb19-5">        <span class="va" style="color: #111111;">self</span>.max_length <span class="op" style="color: #5E5E5E;">=</span> max_length</span>
<span id="cb19-6">        <span class="va" style="color: #111111;">self</span>.inputs <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb19-7">        <span class="va" style="color: #111111;">self</span>.targets <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb19-8"></span>
<span id="cb19-9">        <span class="cf" style="color: #003B4F;">for</span> text <span class="kw" style="color: #003B4F;">in</span> texts:</span>
<span id="cb19-10">            input_ids <span class="op" style="color: #5E5E5E;">=</span> np.array(tokenizer.encode(text))</span>
<span id="cb19-11">            <span class="co" style="color: #5E5E5E;"># make next token the target create datasets with sliding windows</span></span>
<span id="cb19-12">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(input_ids) <span class="op" style="color: #5E5E5E;">&lt;</span> max_length:</span>
<span id="cb19-13">                <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="bu" style="color: null;">len</span>(input_ids)):</span>
<span id="cb19-14">                    <span class="va" style="color: #111111;">self</span>.inputs.append(input_ids[:i])</span>
<span id="cb19-15">                    <span class="va" style="color: #111111;">self</span>.targets.append([input_ids[i]])</span>
<span id="cb19-16"></span>
<span id="cb19-17">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__len__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb19-18">        <span class="cf" style="color: #003B4F;">return</span> <span class="bu" style="color: null;">len</span>(<span class="va" style="color: #111111;">self</span>.inputs)</span>
<span id="cb19-19"></span>
<span id="cb19-20">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__getitem__</span>(<span class="va" style="color: #111111;">self</span>, idx):</span>
<span id="cb19-21">        input_ids <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.inputs[idx]</span>
<span id="cb19-22">        target_ids <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.targets[idx]</span>
<span id="cb19-23"></span>
<span id="cb19-24">        <span class="co" style="color: #5E5E5E;"># Padding input and target</span></span>
<span id="cb19-25">        input_ids <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._pad(input_ids, <span class="va" style="color: #111111;">self</span>.max_length)</span>
<span id="cb19-26">        target_ids <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._pad(target_ids, <span class="va" style="color: #111111;">self</span>.max_length, pad_value<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">100</span>)  <span class="co" style="color: #5E5E5E;"># -100 is often used to ignore loss calculation</span></span>
<span id="cb19-27"></span>
<span id="cb19-28">        <span class="cf" style="color: #003B4F;">return</span>  torch.tensor(input_ids, dtype<span class="op" style="color: #5E5E5E;">=</span>torch.<span class="bu" style="color: null;">long</span>), torch.tensor(target_ids, dtype<span class="op" style="color: #5E5E5E;">=</span>torch.<span class="bu" style="color: null;">long</span>)</span>
<span id="cb19-29">    </span>
<span id="cb19-30"></span>
<span id="cb19-31">    <span class="kw" style="color: #003B4F;">def</span> _pad(<span class="va" style="color: #111111;">self</span>, sequence, max_len, pad_value<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>):</span>
<span id="cb19-32">        padded_sequence <span class="op" style="color: #5E5E5E;">=</span> np.full(max_len, pad_value)</span>
<span id="cb19-33">        padded_sequence[:<span class="bu" style="color: null;">len</span>(sequence)] <span class="op" style="color: #5E5E5E;">=</span> sequence</span>
<span id="cb19-34">        <span class="cf" style="color: #003B4F;">return</span> padded_sequence</span></code></pre></div>
</div>
<p>You hopefully note something very interesting in this dataset: Based on one SMILES, we can create multiple training examples, because we can slide a window over the SMILES and predict the next token. (Note that our implementation is relatively naiive and is optimized to make this point clear. In practice, you should use dedicated methods, e.g., from the <code>transformers</code> library, to create language model datasets.)</p>
</section>
</section>
<section id="a-simple-bigram-model" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-bigram-model">A simple bigram model</h2>
<p>The simplest language model is a bigram model. In a bigram model, we predict the next token based on the previous token. A bigram model is the simplest form of <code>n-gram</code> model. In an <code>n-gram</code> model, we predict the next token based on the previous <code>n</code> tokens.</p>
<p><img src="https://latex.codecogs.com/png.latex?N">-gram models are a simple but effective way to model language. The idea is to predict the next word in a sentence given the previous <img src="https://latex.codecogs.com/png.latex?n-1"> words. For example, in a 2-gram (bigram) model, we would predict the next word given only the previous word. In a 3-gram model, we would predict the next word given the previous two words. In general, we would predict the next word given the previous <img src="https://latex.codecogs.com/png.latex?n-1"> words.</p>
<p>Formally, we can write down the bigram model as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(w_i%7Cw_%7Bi-1%7D)%20=%20%5Cfrac%7BC(w_%7Bi-1%7D,%20w_i)%7D%7BC(w_%7Bi-1%7D)%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?w_i"> is the <img src="https://latex.codecogs.com/png.latex?i">-th word in the sentence, <img src="https://latex.codecogs.com/png.latex?C(w_%7Bi-1%7D,%20w_i)"> is the number of times the bigram <img src="https://latex.codecogs.com/png.latex?w_%7Bi-1%7D,%20w_i"> occurs in the training set, and <img src="https://latex.codecogs.com/png.latex?C(w_%7Bi-1%7D)"> is the number of times the word <img src="https://latex.codecogs.com/png.latex?w_%7Bi-1%7D"> occurs in the training set.</p>
<p>Since the bigram model only considers the previous word/token, we only need a lookup table.</p>
<p>Such lookup tables are implemented in PyTorch as <code>nn.Embedding</code>. Keep in mind that an embedding layer is nothing fancy. It works like inputting a one-hot encoded vector in a linear layer:</p>
<p><img src="https://kjablonka.com/blog/posts/building_an_llm/https:/pbs.twimg.com/media/FlzZE_dWIAEKnAW?format=jpg&amp;name=medium.png" class="img-fluid"></p>
<p>Using the <code>Embedding</code> layer, we can create a simple Bigram model.</p>
<div class="cell" data-execution_count="156">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="kw" style="color: #003B4F;">class</span> BigramModel(nn.Module):</span>
<span id="cb20-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, vocab_size: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">40</span>):</span>
<span id="cb20-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb20-4">        <span class="co" style="color: #5E5E5E;"># "learnable dictionary" that maps one token to another token</span></span>
<span id="cb20-5">        <span class="va" style="color: #111111;">self</span>.mapping_layer <span class="op" style="color: #5E5E5E;">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb20-6"></span>
<span id="cb20-7">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x: torch.Tensor) <span class="op" style="color: #5E5E5E;">-&gt;</span> torch.Tensor:</span>
<span id="cb20-8">        <span class="co" style="color: #5E5E5E;"># the forward pass only consists of a lookup in the mapping layer</span></span>
<span id="cb20-9">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.mapping_layer(x)</span>
<span id="cb20-10">    </span>
<span id="cb20-11">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, x, y): </span>
<span id="cb20-12">        <span class="co" style="color: #5E5E5E;"># x has shape (batch_size, sequence_length)</span></span>
<span id="cb20-13">        predictions <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x)</span>
<span id="cb20-14">        B, T, C <span class="op" style="color: #5E5E5E;">=</span> predictions.shape</span>
<span id="cb20-15">        </span>
<span id="cb20-16">        <span class="co" style="color: #5E5E5E;"># predictions has shape (batch_size, sequence_length, vocab_size)</span></span>
<span id="cb20-17">        predictions <span class="op" style="color: #5E5E5E;">=</span> predictions.view(B<span class="op" style="color: #5E5E5E;">*</span>T, C)</span>
<span id="cb20-18">        </span>
<span id="cb20-19">        <span class="co" style="color: #5E5E5E;"># y has the shape (batch_size, sequence_length)</span></span>
<span id="cb20-20">        y <span class="op" style="color: #5E5E5E;">=</span> y.view(B<span class="op" style="color: #5E5E5E;">*</span>T)</span>
<span id="cb20-21"></span>
<span id="cb20-22">        <span class="co" style="color: #5E5E5E;"># we use cross entropy loss to train the model</span></span>
<span id="cb20-23">        <span class="cf" style="color: #003B4F;">return</span> F.cross_entropy(predictions, y)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="157">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">bigram <span class="op" style="color: #5E5E5E;">=</span> BigramModel(<span class="dv" style="color: #AD0000;">10</span>)</span></code></pre></div>
</div>
<p>Given a token ID, the model predict how likely each token of the vocabulary is to be the next. Right now, the model is not trained, so it will predict the next token randomly.</p>
<div class="cell" data-execution_count="158">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">F.softmax(bigram(torch.tensor([<span class="dv" style="color: #AD0000;">1</span>])))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/ipykernel_51170/730608109.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  F.softmax(bigram(torch.tensor([1])))</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="158">
<pre><code>tensor([[0.1001, 0.0581, 0.2631, 0.0668, 0.0498, 0.0121, 0.0129, 0.1628, 0.0420,
         0.2324]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>
</div>
</div>
<p>For generating a sequence, we can implement a <code>generate</code> method that iteratively predicts the next token and appends it to the sequence. We can then use this method to generate a sequence of a given length.</p>
<div class="cell" data-execution_count="159">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="kw" style="color: #003B4F;">class</span> BigramModel(nn.Module):</span>
<span id="cb25-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, vocab_size):</span>
<span id="cb25-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb25-4">        <span class="co" style="color: #5E5E5E;"># read of the logits of the next token from table</span></span>
<span id="cb25-5">        <span class="va" style="color: #111111;">self</span>.mapping_table <span class="op" style="color: #5E5E5E;">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb25-6"></span>
<span id="cb25-7">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb25-8">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb25-9">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.mapping_table(x) <span class="co" style="color: #5E5E5E;"># returns tensor of shape (batch_size, time_steps, vocab_size)</span></span>
<span id="cb25-10">    </span>
<span id="cb25-11">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, x, y):</span>
<span id="cb25-12">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb25-13">        logits <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x) <span class="co" style="color: #5E5E5E;"># (B, T, C)</span></span>
<span id="cb25-14">        B, T, C <span class="op" style="color: #5E5E5E;">=</span> logits.shape</span>
<span id="cb25-15">        logits <span class="op" style="color: #5E5E5E;">=</span> logits.view(B<span class="op" style="color: #5E5E5E;">*</span>T, C)</span>
<span id="cb25-16">        y <span class="op" style="color: #5E5E5E;">=</span> y.view(B<span class="op" style="color: #5E5E5E;">*</span>T)</span>
<span id="cb25-17">        loss <span class="op" style="color: #5E5E5E;">=</span> F.cross_entropy(logits, y)</span>
<span id="cb25-18">        <span class="cf" style="color: #003B4F;">return</span> loss</span>
<span id="cb25-19">    </span>
<span id="cb25-20">    <span class="kw" style="color: #003B4F;">def</span> generate(<span class="va" style="color: #111111;">self</span>, x, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>):</span>
<span id="cb25-21">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb25-22">        <span class="co" style="color: #5E5E5E;"># we generate max_new_tokens new tokens</span></span>
<span id="cb25-23">        new_tokens <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb25-24">        <span class="cf" style="color: #003B4F;">for</span> _t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(max_new_tokens):</span>
<span id="cb25-25">            logits <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x) <span class="co" style="color: #5E5E5E;"># (B, T, C)</span></span>
<span id="cb25-26">            logits <span class="op" style="color: #5E5E5E;">=</span> logits[:, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, :] <span class="co" style="color: #5E5E5E;"># we only care about the last token in Bigram, hence we bow have shape (B, C)</span></span>
<span id="cb25-27">            probs <span class="op" style="color: #5E5E5E;">=</span> F.softmax(logits, dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="co" style="color: #5E5E5E;"># we generate probabilities for the next token</span></span>
<span id="cb25-28"></span>
<span id="cb25-29">            <span class="co" style="color: #5E5E5E;"># torch.multinomial(probs, num_samples=1) returns a tensor of shape (B, 1) </span></span>
<span id="cb25-30">            <span class="co" style="color: #5E5E5E;"># where each element is the index of the sampled token</span></span>
<span id="cb25-31">            next_token <span class="op" style="color: #5E5E5E;">=</span> torch.multinomial(probs, num_samples<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb25-32">            new_tokens.append(next_token)</span>
<span id="cb25-33">            x <span class="op" style="color: #5E5E5E;">=</span> torch.cat([x, next_token], dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb25-34">        <span class="cf" style="color: #003B4F;">return</span> x</span>
<span id="cb25-35">        </span>
<span id="cb25-36">    </span></code></pre></div>
</div>
<p>To evaluate the model performance, we will use the helper function below.</p>
<p>As performance metric we will use perplexity. Perplexity is a metric that measures how well a probability model predicts a sample. It is defined as <img src="https://latex.codecogs.com/png.latex?2%5EH">, where <img src="https://latex.codecogs.com/png.latex?H"> is the cross entropy loss. The lower the perplexity, the better the model.</p>
<p>To better understand it, let’s recall a few things:</p>
<p>LLMs are trained to predict the probability of a word given the previous words. For instance, in the sentence “She went to the…”, the model predicts the probability of what the next word could be (e.g., store, park, etc.).</p>
<p><em>Cross entropy</em> is a measure of the difference between two probability distributions - in this case, the distribution predicted by the model and the actual distribution of words in the language. A lower cross-entropy means the model’s predictions are closer to the actual distribution. We can calculate it as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?H(p,q)%20=%20-%20%5Csum_%7Bx%7D%20p(x)%20%5Clog%20q(x)"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?p"> is the actual distribution and <img src="https://latex.codecogs.com/png.latex?q"> is the predicted distribution.</p>
<p><em>Perplexity</em> can be thought of as the “effective number of choices” the model feels it has when making a prediction. A lower perplexity indicates that the model is more confident (or less “perplexed”) about its predictions.</p>
<p>For example, if a model has a perplexity of 10 on a dataset, it means that, on average, each time it tries to predict the next word, it’s as uncertain as if it were choosing uniformly and randomly among 10 options. If the perplexity is 100, it’s as uncertain as if it were choosing among 100 options, and so on.</p>
<p>You can find further information about such metrics <a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">here</a>.</p>
<div class="cell" data-execution_count="160">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="at" style="color: #657422;">@torch.no_grad</span>()</span>
<span id="cb26-2"><span class="kw" style="color: #003B4F;">def</span> estimate_perplexity(model, data_loader):</span>
<span id="cb26-3">    <span class="co" style="color: #5E5E5E;"># set the model to evaluation mode, i.e., </span></span>
<span id="cb26-4">    model.<span class="bu" style="color: null;">eval</span>()</span>
<span id="cb26-5">    total_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb26-6">    total_count <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb26-7">    <span class="cf" style="color: #003B4F;">for</span> x, y <span class="kw" style="color: #003B4F;">in</span> data_loader:</span>
<span id="cb26-8">        x <span class="op" style="color: #5E5E5E;">=</span> x.to(device)</span>
<span id="cb26-9">        y <span class="op" style="color: #5E5E5E;">=</span> y.to(device)</span>
<span id="cb26-10">        loss <span class="op" style="color: #5E5E5E;">=</span> model.loss(x, y)</span>
<span id="cb26-11">        total_loss <span class="op" style="color: #5E5E5E;">+=</span> loss.item()</span>
<span id="cb26-12">        total_count <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb26-13">    <span class="cf" style="color: #003B4F;">return</span> exp(total_loss <span class="op" style="color: #5E5E5E;">/</span> total_count)</span></code></pre></div>
</div>
<section id="training-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model">Training the model</h3>
<p>To train the model, we will use a simple training loop and the Adam optimizer.</p>
<p>The role of the <code>Adam</code> optimizer is to update the parameters of the model using a technique called <a href="http://d2l.ai/chapter_optimization/minibatch-sgd.html">mini-batch stochastic gradient descent</a>. The idea is that we update the weights in the direction of the gradient of the loss function, which we estimate on a small batch of data. The learning rate controls how big the steps are that we take in the direction of the gradient.</p>
<p>Setting learning rate is not trivial, you can find more background <a href="https://www.jeremyjordan.me/nn-learning-rate/">here</a>.</p>
<p>It is import to remember to use the <code>zero_grad</code> function to clear the gradients before computing the gradients for the current batch. Also, remember to call <code>loss.backward()</code> to compute the gradients for the current batch.</p>
<p>For now, we will use a very simple approach (to reuse our old dataloader) and just predict the second token given the first one.</p>
<div class="cell" data-execution_count="161">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">model <span class="op" style="color: #5E5E5E;">=</span> BigramModel(<span class="bu" style="color: null;">len</span>(tokenizer))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="162">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">ds <span class="op" style="color: #5E5E5E;">=</span> CausalLanguageModelingDataset(train, tokenizer, max_length<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">40</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="163">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">train_loader <span class="op" style="color: #5E5E5E;">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(train, tokenizer, max_length<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">40</span>), batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2048</span>, shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb29-2">valid_loader <span class="op" style="color: #5E5E5E;">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(valid, tokenizer, max_length<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">40</span>), batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2048</span>)</span>
<span id="cb29-3">test_loader <span class="op" style="color: #5E5E5E;">=</span> torch.utils.data.DataLoader(CausalLanguageModelingDataset(test, tokenizer, max_length<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">40</span>), batch_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2048</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="164">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="kw" style="color: #003B4F;">def</span> train_model(model, train_loader, val_loader, epochs, lr, eval_every<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>):</span>
<span id="cb30-2">    <span class="co" style="color: #5E5E5E;"># set up the optimizer</span></span>
<span id="cb30-3">    optimizer <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr)</span>
<span id="cb30-4">    model.to(device)</span>
<span id="cb30-5">    <span class="co" style="color: #5E5E5E;"># start training</span></span>
<span id="cb30-6">    <span class="co" style="color: #5E5E5E;"># set the model to train mode </span></span>
<span id="cb30-7">    model.train()</span>
<span id="cb30-8">    <span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb30-9"></span>
<span id="cb30-10"></span>
<span id="cb30-11">        <span class="co" style="color: #5E5E5E;"># iterate over the training data</span></span>
<span id="cb30-12">        <span class="cf" style="color: #003B4F;">for</span> i, (x,y) <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(train_loader):</span>
<span id="cb30-13">            <span class="co" style="color: #5E5E5E;"># move the data to the device</span></span>
<span id="cb30-14">            x <span class="op" style="color: #5E5E5E;">=</span> x.to(device)</span>
<span id="cb30-15">            y <span class="op" style="color: #5E5E5E;">=</span> y.to(device)</span>
<span id="cb30-16">            loss <span class="op" style="color: #5E5E5E;">=</span> model.loss(x,y)</span>
<span id="cb30-17">            <span class="co" style="color: #5E5E5E;"># clear the gradients</span></span>
<span id="cb30-18">            optimizer.zero_grad()</span>
<span id="cb30-19">            <span class="co" style="color: #5E5E5E;"># compute the gradients</span></span>
<span id="cb30-20">            loss.backward()</span>
<span id="cb30-21">            <span class="co" style="color: #5E5E5E;"># update the parameters</span></span>
<span id="cb30-22">            optimizer.step()</span>
<span id="cb30-23"></span>
<span id="cb30-24">            <span class="co" style="color: #5E5E5E;"># print the loss every eval_every iterations</span></span>
<span id="cb30-25">            <span class="cf" style="color: #003B4F;">if</span> i <span class="op" style="color: #5E5E5E;">%</span> eval_every <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb30-26">                <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Epoch </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, iter </span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, train loss </span><span class="sc" style="color: #5E5E5E;">{</span>loss<span class="sc" style="color: #5E5E5E;">.</span>item()<span class="sc" style="color: #5E5E5E;">:.3f}</span><span class="ss" style="color: #20794D;">, val perplexity </span><span class="sc" style="color: #5E5E5E;">{</span>estimate_perplexity(model, val_loader)<span class="sc" style="color: #5E5E5E;">:.5f}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="166">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">train_model(model, train_loader, valid_loader, epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-3</span>, eval_every<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, iter 0, train loss 2.423, val perplexity 11.27392
Epoch 0, iter 100, train loss 2.423, val perplexity 11.24984
Epoch 0, iter 200, train loss 2.450, val perplexity 11.23007
Epoch 0, iter 300, train loss 2.451, val perplexity 11.21351
Epoch 0, iter 400, train loss 2.409, val perplexity 11.19960
Epoch 0, iter 500, train loss 2.403, val perplexity 11.18782
Epoch 0, iter 600, train loss 2.439, val perplexity 11.17740
Epoch 0, iter 700, train loss 2.435, val perplexity 11.16857
Epoch 0, iter 800, train loss 2.432, val perplexity 11.16064
Epoch 0, iter 900, train loss 2.401, val perplexity 11.15376
Epoch 0, iter 1000, train loss 2.436, val perplexity 11.14751
Epoch 0, iter 1100, train loss 2.424, val perplexity 11.14243
Epoch 0, iter 1200, train loss 2.365, val perplexity 11.13720
Epoch 0, iter 1300, train loss 2.430, val perplexity 11.13270
Epoch 0, iter 1400, train loss 2.396, val perplexity 11.12861
Epoch 0, iter 1500, train loss 2.396, val perplexity 11.12477
Epoch 0, iter 1600, train loss 2.427, val perplexity 11.12151
Epoch 0, iter 1700, train loss 2.426, val perplexity 11.11855
Epoch 0, iter 1800, train loss 2.407, val perplexity 11.11576
Epoch 0, iter 1900, train loss 2.411, val perplexity 11.11314
Epoch 0, iter 2000, train loss 2.409, val perplexity 11.11071
Epoch 0, iter 2100, train loss 2.377, val perplexity 11.10836
Epoch 0, iter 2200, train loss 2.414, val perplexity 11.10666
Epoch 0, iter 2300, train loss 2.357, val perplexity 11.10495
Epoch 1, iter 0, train loss 2.400, val perplexity 11.10374
Epoch 1, iter 100, train loss 2.430, val perplexity 11.10187
Epoch 1, iter 200, train loss 2.392, val perplexity 11.10039
Epoch 1, iter 300, train loss 2.418, val perplexity 11.09890
Epoch 1, iter 400, train loss 2.378, val perplexity 11.09767
Epoch 1, iter 500, train loss 2.423, val perplexity 11.09650
Epoch 1, iter 600, train loss 2.390, val perplexity 11.09533
Epoch 1, iter 700, train loss 2.440, val perplexity 11.09407
Epoch 1, iter 800, train loss 2.397, val perplexity 11.09331
Epoch 1, iter 900, train loss 2.408, val perplexity 11.09244
Epoch 1, iter 1000, train loss 2.397, val perplexity 11.09115
Epoch 1, iter 1100, train loss 2.422, val perplexity 11.09047
Epoch 1, iter 1200, train loss 2.382, val perplexity 11.08941
Epoch 1, iter 1300, train loss 2.411, val perplexity 11.08896
Epoch 1, iter 1400, train loss 2.390, val perplexity 11.08832
Epoch 1, iter 1500, train loss 2.410, val perplexity 11.08726
Epoch 1, iter 1600, train loss 2.423, val perplexity 11.08663
Epoch 1, iter 1700, train loss 2.394, val perplexity 11.08610
Epoch 1, iter 1800, train loss 2.411, val perplexity 11.08553
Epoch 1, iter 1900, train loss 2.418, val perplexity 11.08504
Epoch 1, iter 2000, train loss 2.438, val perplexity 11.08430
Epoch 1, iter 2100, train loss 2.403, val perplexity 11.08389
Epoch 1, iter 2200, train loss 2.378, val perplexity 11.08335
Epoch 1, iter 2300, train loss 2.429, val perplexity 11.08304
Epoch 2, iter 0, train loss 2.378, val perplexity 11.08273
Epoch 2, iter 100, train loss 2.423, val perplexity 11.08241
Epoch 2, iter 200, train loss 2.419, val perplexity 11.08202
Epoch 2, iter 300, train loss 2.398, val perplexity 11.08176
Epoch 2, iter 400, train loss 2.420, val perplexity 11.08122
Epoch 2, iter 500, train loss 2.402, val perplexity 11.08099
Epoch 2, iter 600, train loss 2.436, val perplexity 11.08070
Epoch 2, iter 700, train loss 2.400, val perplexity 11.08106
Epoch 2, iter 800, train loss 2.416, val perplexity 11.08027
Epoch 2, iter 900, train loss 2.390, val perplexity 11.07992
Epoch 2, iter 1000, train loss 2.407, val perplexity 11.07969
Epoch 2, iter 1100, train loss 2.386, val perplexity 11.07925
Epoch 2, iter 1200, train loss 2.395, val perplexity 11.07893
Epoch 2, iter 1300, train loss 2.376, val perplexity 11.07876
Epoch 2, iter 1400, train loss 2.401, val perplexity 11.07871
Epoch 2, iter 1500, train loss 2.429, val perplexity 11.07836
Epoch 2, iter 1600, train loss 2.416, val perplexity 11.07817
Epoch 2, iter 1700, train loss 2.439, val perplexity 11.07803
Epoch 2, iter 1800, train loss 2.391, val perplexity 11.07777
Epoch 2, iter 1900, train loss 2.381, val perplexity 11.07781
Epoch 2, iter 2000, train loss 2.407, val perplexity 11.07774
Epoch 2, iter 2100, train loss 2.435, val perplexity 11.07758
Epoch 2, iter 2200, train loss 2.383, val perplexity 11.07733
Epoch 2, iter 2300, train loss 2.400, val perplexity 11.07734
Epoch 3, iter 0, train loss 2.390, val perplexity 11.07707
Epoch 3, iter 100, train loss 2.398, val perplexity 11.07696
Epoch 3, iter 200, train loss 2.441, val perplexity 11.07689
Epoch 3, iter 300, train loss 2.399, val perplexity 11.07688
Epoch 3, iter 400, train loss 2.420, val perplexity 11.07665
Epoch 3, iter 500, train loss 2.395, val perplexity 11.07644
Epoch 3, iter 600, train loss 2.426, val perplexity 11.07636
Epoch 3, iter 700, train loss 2.449, val perplexity 11.07612
Epoch 3, iter 800, train loss 2.417, val perplexity 11.07598
Epoch 3, iter 900, train loss 2.389, val perplexity 11.07603
Epoch 3, iter 1000, train loss 2.426, val perplexity 11.07601
Epoch 3, iter 1100, train loss 2.423, val perplexity 11.07572
Epoch 3, iter 1200, train loss 2.434, val perplexity 11.07570
Epoch 3, iter 1300, train loss 2.377, val perplexity 11.07568
Epoch 3, iter 1400, train loss 2.378, val perplexity 11.07571
Epoch 3, iter 1500, train loss 2.435, val perplexity 11.07559
Epoch 3, iter 1600, train loss 2.398, val perplexity 11.07552
Epoch 3, iter 1700, train loss 2.414, val perplexity 11.07518
Epoch 3, iter 1800, train loss 2.446, val perplexity 11.07560
Epoch 3, iter 1900, train loss 2.428, val perplexity 11.07523
Epoch 3, iter 2000, train loss 2.366, val perplexity 11.07503
Epoch 3, iter 2100, train loss 2.383, val perplexity 11.07526
Epoch 3, iter 2200, train loss 2.390, val perplexity 11.07514
Epoch 3, iter 2300, train loss 2.402, val perplexity 11.07514
Epoch 4, iter 0, train loss 2.370, val perplexity 11.07506
Epoch 4, iter 100, train loss 2.403, val perplexity 11.07501
Epoch 4, iter 200, train loss 2.414, val perplexity 11.07492
Epoch 4, iter 300, train loss 2.422, val perplexity 11.07469
Epoch 4, iter 400, train loss 2.414, val perplexity 11.07477
Epoch 4, iter 500, train loss 2.403, val perplexity 11.07465
Epoch 4, iter 600, train loss 2.389, val perplexity 11.07464
Epoch 4, iter 700, train loss 2.437, val perplexity 11.07466
Epoch 4, iter 800, train loss 2.404, val perplexity 11.07462
Epoch 4, iter 900, train loss 2.418, val perplexity 11.07511
Epoch 4, iter 1000, train loss 2.412, val perplexity 11.07476
Epoch 4, iter 1100, train loss 2.438, val perplexity 11.07460
Epoch 4, iter 1200, train loss 2.395, val perplexity 11.07443
Epoch 4, iter 1300, train loss 2.434, val perplexity 11.07437
Epoch 4, iter 1400, train loss 2.412, val perplexity 11.07436
Epoch 4, iter 1500, train loss 2.421, val perplexity 11.07439
Epoch 4, iter 1600, train loss 2.369, val perplexity 11.07436
Epoch 4, iter 1700, train loss 2.418, val perplexity 11.07475
Epoch 4, iter 1800, train loss 2.354, val perplexity 11.07431
Epoch 4, iter 1900, train loss 2.388, val perplexity 11.07455
Epoch 4, iter 2000, train loss 2.445, val perplexity 11.07442
Epoch 4, iter 2100, train loss 2.393, val perplexity 11.07426
Epoch 4, iter 2200, train loss 2.396, val perplexity 11.07447
Epoch 4, iter 2300, train loss 2.394, val perplexity 11.07444
Epoch 5, iter 0, train loss 2.401, val perplexity 11.07427
Epoch 5, iter 100, train loss 2.397, val perplexity 11.07452
Epoch 5, iter 200, train loss 2.377, val perplexity 11.07429
Epoch 5, iter 300, train loss 2.393, val perplexity 11.07454
Epoch 5, iter 400, train loss 2.428, val perplexity 11.07433
Epoch 5, iter 500, train loss 2.438, val perplexity 11.07406
Epoch 5, iter 600, train loss 2.416, val perplexity 11.07414
Epoch 5, iter 700, train loss 2.421, val perplexity 11.07419
Epoch 5, iter 800, train loss 2.414, val perplexity 11.07401
Epoch 5, iter 900, train loss 2.415, val perplexity 11.07403
Epoch 5, iter 1000, train loss 2.387, val perplexity 11.07414
Epoch 5, iter 1100, train loss 2.404, val perplexity 11.07418
Epoch 5, iter 1200, train loss 2.386, val perplexity 11.07427
Epoch 5, iter 1300, train loss 2.408, val perplexity 11.07411
Epoch 5, iter 1400, train loss 2.402, val perplexity 11.07427
Epoch 5, iter 1500, train loss 2.388, val perplexity 11.07399
Epoch 5, iter 1600, train loss 2.361, val perplexity 11.07405
Epoch 5, iter 1700, train loss 2.415, val perplexity 11.07392
Epoch 5, iter 1800, train loss 2.406, val perplexity 11.07400
Epoch 5, iter 1900, train loss 2.363, val perplexity 11.07389
Epoch 5, iter 2000, train loss 2.409, val perplexity 11.07398
Epoch 5, iter 2100, train loss 2.419, val perplexity 11.07407
Epoch 5, iter 2200, train loss 2.401, val perplexity 11.07381
Epoch 5, iter 2300, train loss 2.443, val perplexity 11.07387
Epoch 6, iter 0, train loss 2.394, val perplexity 11.07407
Epoch 6, iter 100, train loss 2.399, val perplexity 11.07425
Epoch 6, iter 200, train loss 2.425, val perplexity 11.07410
Epoch 6, iter 300, train loss 2.404, val perplexity 11.07376
Epoch 6, iter 400, train loss 2.395, val perplexity 11.07368
Epoch 6, iter 500, train loss 2.393, val perplexity 11.07375
Epoch 6, iter 600, train loss 2.393, val perplexity 11.07386
Epoch 6, iter 700, train loss 2.401, val perplexity 11.07404
Epoch 6, iter 800, train loss 2.405, val perplexity 11.07393
Epoch 6, iter 900, train loss 2.398, val perplexity 11.07418
Epoch 6, iter 1000, train loss 2.397, val perplexity 11.07399
Epoch 6, iter 1100, train loss 2.422, val perplexity 11.07403
Epoch 6, iter 1200, train loss 2.398, val perplexity 11.07393
Epoch 6, iter 1300, train loss 2.406, val perplexity 11.07402
Epoch 6, iter 1400, train loss 2.392, val perplexity 11.07403
Epoch 6, iter 1500, train loss 2.394, val perplexity 11.07424
Epoch 6, iter 1600, train loss 2.396, val perplexity 11.07407
Epoch 6, iter 1700, train loss 2.406, val perplexity 11.07427
Epoch 6, iter 1800, train loss 2.396, val perplexity 11.07393
Epoch 6, iter 1900, train loss 2.424, val perplexity 11.07386
Epoch 6, iter 2000, train loss 2.421, val perplexity 11.07397
Epoch 6, iter 2100, train loss 2.429, val perplexity 11.07394
Epoch 6, iter 2200, train loss 2.393, val perplexity 11.07407
Epoch 6, iter 2300, train loss 2.417, val perplexity 11.07399
Epoch 7, iter 0, train loss 2.407, val perplexity 11.07377
Epoch 7, iter 100, train loss 2.394, val perplexity 11.07395
Epoch 7, iter 200, train loss 2.394, val perplexity 11.07392
Epoch 7, iter 300, train loss 2.383, val perplexity 11.07387
Epoch 7, iter 400, train loss 2.433, val perplexity 11.07381
Epoch 7, iter 500, train loss 2.418, val perplexity 11.07386
Epoch 7, iter 600, train loss 2.401, val perplexity 11.07383
Epoch 7, iter 700, train loss 2.400, val perplexity 11.07368
Epoch 7, iter 800, train loss 2.411, val perplexity 11.07384
Epoch 7, iter 900, train loss 2.410, val perplexity 11.07378
Epoch 7, iter 1000, train loss 2.439, val perplexity 11.07400
Epoch 7, iter 1100, train loss 2.371, val perplexity 11.07381
Epoch 7, iter 1200, train loss 2.412, val perplexity 11.07374
Epoch 7, iter 1300, train loss 2.441, val perplexity 11.07380
Epoch 7, iter 1400, train loss 2.392, val perplexity 11.07385
Epoch 7, iter 1500, train loss 2.405, val perplexity 11.07381
Epoch 7, iter 1600, train loss 2.407, val perplexity 11.07369
Epoch 7, iter 1700, train loss 2.383, val perplexity 11.07384
Epoch 7, iter 1800, train loss 2.426, val perplexity 11.07394
Epoch 7, iter 1900, train loss 2.443, val perplexity 11.07384
Epoch 7, iter 2000, train loss 2.376, val perplexity 11.07398
Epoch 7, iter 2100, train loss 2.409, val perplexity 11.07385
Epoch 7, iter 2200, train loss 2.367, val perplexity 11.07380
Epoch 7, iter 2300, train loss 2.399, val perplexity 11.07367
Epoch 8, iter 0, train loss 2.386, val perplexity 11.07387
Epoch 8, iter 100, train loss 2.381, val perplexity 11.07374
Epoch 8, iter 200, train loss 2.396, val perplexity 11.07371
Epoch 8, iter 300, train loss 2.392, val perplexity 11.07380
Epoch 8, iter 400, train loss 2.420, val perplexity 11.07377
Epoch 8, iter 500, train loss 2.441, val perplexity 11.07406
Epoch 8, iter 600, train loss 2.419, val perplexity 11.07404
Epoch 8, iter 700, train loss 2.389, val perplexity 11.07385
Epoch 8, iter 800, train loss 2.408, val perplexity 11.07389
Epoch 8, iter 900, train loss 2.401, val perplexity 11.07368
Epoch 8, iter 1000, train loss 2.414, val perplexity 11.07393
Epoch 8, iter 1100, train loss 2.361, val perplexity 11.07391
Epoch 8, iter 1200, train loss 2.418, val perplexity 11.07373
Epoch 8, iter 1300, train loss 2.409, val perplexity 11.07353
Epoch 8, iter 1400, train loss 2.390, val perplexity 11.07358
Epoch 8, iter 1500, train loss 2.424, val perplexity 11.07368
Epoch 8, iter 1600, train loss 2.410, val perplexity 11.07357
Epoch 8, iter 1700, train loss 2.414, val perplexity 11.07377
Epoch 8, iter 1800, train loss 2.395, val perplexity 11.07365
Epoch 8, iter 1900, train loss 2.383, val perplexity 11.07354
Epoch 8, iter 2000, train loss 2.423, val perplexity 11.07377
Epoch 8, iter 2100, train loss 2.426, val perplexity 11.07397
Epoch 8, iter 2200, train loss 2.438, val perplexity 11.07389
Epoch 8, iter 2300, train loss 2.393, val perplexity 11.07382
Epoch 9, iter 0, train loss 2.384, val perplexity 11.07388
Epoch 9, iter 100, train loss 2.395, val perplexity 11.07372
Epoch 9, iter 200, train loss 2.444, val perplexity 11.07394
Epoch 9, iter 300, train loss 2.444, val perplexity 11.07369
Epoch 9, iter 400, train loss 2.405, val perplexity 11.07371
Epoch 9, iter 500, train loss 2.368, val perplexity 11.07369
Epoch 9, iter 600, train loss 2.388, val perplexity 11.07378
Epoch 9, iter 700, train loss 2.405, val perplexity 11.07407
Epoch 9, iter 800, train loss 2.410, val perplexity 11.07403
Epoch 9, iter 900, train loss 2.427, val perplexity 11.07391
Epoch 9, iter 1000, train loss 2.395, val perplexity 11.07363
Epoch 9, iter 1100, train loss 2.413, val perplexity 11.07368
Epoch 9, iter 1200, train loss 2.388, val perplexity 11.07374
Epoch 9, iter 1300, train loss 2.412, val perplexity 11.07383
Epoch 9, iter 1400, train loss 2.438, val perplexity 11.07379
Epoch 9, iter 1500, train loss 2.410, val perplexity 11.07394
Epoch 9, iter 1600, train loss 2.397, val perplexity 11.07370
Epoch 9, iter 1700, train loss 2.442, val perplexity 11.07380
Epoch 9, iter 1800, train loss 2.396, val perplexity 11.07368
Epoch 9, iter 1900, train loss 2.428, val perplexity 11.07367
Epoch 9, iter 2000, train loss 2.430, val perplexity 11.07382
Epoch 9, iter 2100, train loss 2.363, val perplexity 11.07376
Epoch 9, iter 2200, train loss 2.391, val perplexity 11.07373
Epoch 9, iter 2300, train loss 2.406, val perplexity 11.07385</code></pre>
</div>
</div>
<p>We can now test the model by generating new SMILES strings. We will start with a random token and generate 100 new tokens.</p>
<div class="cell" data-execution_count="168">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">a <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([[<span class="dv" style="color: #AD0000;">4</span>]])</span>
<span id="cb33-2">a <span class="op" style="color: #5E5E5E;">=</span> a.to(device)</span>
<span id="cb33-3">generation <span class="op" style="color: #5E5E5E;">=</span> model.generate(a, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>).cpu().numpy()</span>
<span id="cb33-4">tokenizer.decode(generation[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="168">
<pre><code>'[C@@][C@]Cl)O(BCO1'</code></pre>
</div>
</div>
<p>This does not yet look like a valid SMILES string …</p>
</section>
</section>
<section id="making-tokens-talk-using-attention" class="level2">
<h2 class="anchored" data-anchor-id="making-tokens-talk-using-attention">Making tokens talk using attention</h2>
<p>In our bigram models we made predictions based on the previous word. This is clearly not enough to make good predictions. We can improve our model by taking into more past tokens into account.</p>
<p>One naïve way to incorporate more context into our model might be to simply “pool” (features of) the preceding tokens. This kind of pooling is similar to what we do in GNNs, e.g., to combine node embeddings.</p>
<p>A very simple pooling operation is the average of the embeddings of the preceding tokens. Later, when we will implement self-attention, we will not use a simple average, but a special weighted average. The code for that will use similar ideas (in particular, the causal mask).</p>
<div class="cell" data-execution_count="167">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">B, T, C <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">3</span> <span class="co" style="color: #5E5E5E;"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb35-2"></span>
<span id="cb35-3"><span class="co" style="color: #5E5E5E;"># create random data of shape (B, T, C)</span></span>
<span id="cb35-4">x <span class="op" style="color: #5E5E5E;">=</span> torch.randn(B,T,C)</span>
<span id="cb35-5"></span>
<span id="cb35-6">x_bag_of_words <span class="op" style="color: #5E5E5E;">=</span> torch.zeros((B,T,C))</span>
<span id="cb35-7"><span class="cf" style="color: #003B4F;">for</span> b <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(B):</span>
<span id="cb35-8">    <span class="cf" style="color: #003B4F;">for</span> t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(T):</span>
<span id="cb35-9">        x_prev <span class="op" style="color: #5E5E5E;">=</span> x[b,:t<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>] <span class="co" style="color: #5E5E5E;"># shape (t, C)</span></span>
<span id="cb35-10">        </span>
<span id="cb35-11">        x_bag_of_words[b, t] <span class="op" style="color: #5E5E5E;">=</span> torch.mean(x_prev, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>) <span class="co" style="color: #5E5E5E;"># shape (C,)</span></span></code></pre></div>
</div>
<p>This nested for loop is slow. However, we can implement this in an efficient way if we observe a few things:</p>
<ul>
<li><p>If we want to predict next tokens, we do not want to let the future tokens influence the prediction. Therefore, we can use a so-called causal mask to mask out the future tokens.</p></li>
<li><p>A matrix multiplication can be thought of as a weighted sum of the rows of the matrix, where the weights are given by the columns of the matrix. This is easy to see if we think of the following extremes:</p>
<ul>
<li>We can compute the sum of the rows of a matrix by multiplying the matrix with a lower-triangular matrix filled with ones.</li>
<li>We can compute the mean of the rows of a matrix by multiplying the matrix with a lower-triangular matrix filled with ones and dividing by the number of ones in the lower-triangular matrix.</li>
</ul></li>
</ul>
<p>In <code>torch</code> we can use <code>tril</code> to create a lower-triangular matrix.</p>
<div class="cell" data-execution_count="169">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">lower_triangular_mask <span class="op" style="color: #5E5E5E;">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb36-2"></span>
<span id="cb36-3">weight <span class="op" style="color: #5E5E5E;">=</span> torch.ones((T,T))</span>
<span id="cb36-4">weight <span class="op" style="color: #5E5E5E;">=</span> torch.masked_fill(weight, lower_triangular_mask<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'-inf'</span>))</span>
<span id="cb36-5"></span>
<span id="cb36-6">weight <span class="op" style="color: #5E5E5E;">=</span> torch.softmax(weight, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="170">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">weight  </span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="170">
<pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000],
        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])</code></pre>
</div>
</div>
<p>We used the softmax function to normalize the weights in the rows.</p>
<div class="cell" data-execution_count="171">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">weight <span class="op" style="color: #5E5E5E;">@</span> x</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="171">
<pre><code>tensor([[[-0.6594,  0.1502, -0.2865],
         [-0.5213, -0.4364,  0.4502],
         [-0.3585, -0.1425,  0.5507],
         [-0.0764,  0.1243,  0.6123],
         [-0.1630,  0.0269,  0.5489]],

        [[-0.6673,  0.6167, -0.9546],
         [-0.3099, -0.0206,  0.3154],
         [ 0.1767, -0.0318,  0.3009],
         [ 0.4085, -0.2151,  0.5611],
         [ 0.4605, -0.4176,  0.5351]]])</code></pre>
</div>
</div>
<p>In the simple average we used above, all past tokens were treated equally. However, it might be useful to <em>pay more attention</em> to certain tokens than to others. That is, we want to gather information from the past – but do this in a data-dependent way. The attention mechanism allows us to do this.</p>
<p>The attention mechanism does this by having a query vector <img src="https://latex.codecogs.com/png.latex?q"> and a key vector <img src="https://latex.codecogs.com/png.latex?k"> for each token. We then define “similarity” or “relevance” between two tokens <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j"> as the dot product between their query and key vectors, which we derive from the embeddings of the tokens by multiplying them with the learnable weight matrices <img src="https://latex.codecogs.com/png.latex?W_q"> and <img src="https://latex.codecogs.com/png.latex?W_k">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bsim%7D(i,%20j)%20=%20a(i,%20h)%20=%20q_ik_j%5ET%20=%20%5Ctext%7Bemb%7D_i%20W_q%20W_k%5ET%20%5Ctext%7Bemb%7D_j%5ET%0A"></p>
<p>Note that this gives us now a way to refine the <code>weight_matrix</code> we used above. Instead of weighting all tokens equally, we can now learn a weight matrix that tells us how much attention to pay to each token.</p>
<p>To start the implementation, we will first derive query and key vectors from the embeddings. We will then compute the similarity matrix and apply the softmax function to normalize the weights.</p>
<div class="cell" data-execution_count="172">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">B, T, C <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">3</span> <span class="co" style="color: #5E5E5E;"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb41-2">x <span class="op" style="color: #5E5E5E;">=</span> torch.randn(B,T,C)</span>
<span id="cb41-3"></span>
<span id="cb41-4">head_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">16</span> <span class="co" style="color: #5E5E5E;"># hyperparameter</span></span>
<span id="cb41-5"></span>
<span id="cb41-6"><span class="co" style="color: #5E5E5E;"># with bias = False, it only perform matrix multiplication</span></span>
<span id="cb41-7">key_layer <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(C, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)  </span>
<span id="cb41-8">query_layer <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(C, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
</div>
<p>The attention matrix defined above is now a simple matrix multiplication between the query and key vectors. The attention matrix is then normalized using a softmax function.</p>
<div class="cell" data-execution_count="174">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">query <span class="op" style="color: #5E5E5E;">=</span> query_layer(x) <span class="co" style="color: #5E5E5E;"># shape (B, T, head_size)</span></span>
<span id="cb42-2">key <span class="op" style="color: #5E5E5E;">=</span> key_layer(x) <span class="co" style="color: #5E5E5E;"># shape (B, T, head_size)</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="182">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">attention <span class="op" style="color: #5E5E5E;">=</span> query <span class="op" style="color: #5E5E5E;">@</span> key.transpose(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>) <span class="co" style="color: #5E5E5E;"># shape (B, T, T)</span></span></code></pre></div>
</div>
<p>Note that the shape of the attention matrix is (B, T, T). The attention matrix is a matrix where each row corresponds to a query and each column corresponds to a key. The value at position (i, j) in the attention matrix is the attention score between the i-th query and the j-th key.</p>
<div class="cell" data-execution_count="183">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">attention</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="183">
<pre><code>tensor([[[ 0.3521, -0.4349,  1.3019, -1.4227, -0.9973],
         [-0.3857, -0.7338,  0.5761, -1.0932, -0.8915],
         [-0.1943, -0.5278,  0.5179, -0.8923, -0.6988],
         [-1.1021, -0.8737, -0.3723, -0.4446, -0.5439],
         [-0.5155, -0.5618,  0.0866, -0.5447, -0.5076]],

        [[-1.3238,  0.7295, -0.1932, -0.3536,  0.1224],
         [ 1.2033, -0.5237, -0.4037, -0.9075, -0.6969],
         [ 0.6911, -0.5819,  0.6078,  1.0999,  0.4158],
         [ 2.9120, -1.8470,  0.5745,  0.6777, -0.2034],
         [ 1.2999, -0.8820,  0.5131,  0.8557,  0.1704]]],
       grad_fn=&lt;UnsafeViewBackward0&gt;)</code></pre>
</div>
</div>
<p>But to avoid the future tokens to influence the prediction, we will use a causal mask. We do this the same way as we did above, by using <code>torch.tril</code>.</p>
<div class="cell" data-execution_count="184">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">lower_triangular_mask <span class="op" style="color: #5E5E5E;">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb46-2"></span>
<span id="cb46-3">attention <span class="op" style="color: #5E5E5E;">=</span> torch.masked_fill(attention, lower_triangular_mask<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'-inf'</span>))   </span>
<span id="cb46-4"></span>
<span id="cb46-5">attention <span class="op" style="color: #5E5E5E;">=</span> torch.softmax(attention, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>) <span class="co" style="color: #5E5E5E;"># shape (B, T, T), softmax along the last dimension</span></span>
<span id="cb46-6"></span>
<span id="cb46-7">out <span class="op" style="color: #5E5E5E;">=</span> attention <span class="op" style="color: #5E5E5E;">@</span> x <span class="co" style="color: #5E5E5E;"># shape (B, T, T) @ (B, T, C) = (B, T, C)</span></span></code></pre></div>
</div>
<p>In the attention mechanism popularized in the <a href="https://arxiv.org/abs/1706.03762">“attention is all you need” paper</a> we add even more expressive power by transforming <code>x</code> before we multiply it with the attention matrix. We call this transformed <code>x</code> the value vector (or matrix). The full implementation of the attention mechanism is then:</p>
<div class="cell" data-execution_count="186">
<div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">B, T, C <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">3</span> <span class="co" style="color: #5E5E5E;"># batch size, time (sequence length), channels (features)</span></span>
<span id="cb47-2">x <span class="op" style="color: #5E5E5E;">=</span> torch.randn(B,T,C)</span>
<span id="cb47-3"></span>
<span id="cb47-4">head_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">16</span> <span class="co" style="color: #5E5E5E;"># hyperparameter</span></span>
<span id="cb47-5"></span>
<span id="cb47-6"><span class="co" style="color: #5E5E5E;"># what do I contain</span></span>
<span id="cb47-7"><span class="co" style="color: #5E5E5E;"># with bias = False, it only perform matrix multiplication</span></span>
<span id="cb47-8">key <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(C, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb47-9"></span>
<span id="cb47-10"><span class="co" style="color: #5E5E5E;"># what am I looking for</span></span>
<span id="cb47-11">query <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(C, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb47-12"></span>
<span id="cb47-13"><span class="co" style="color: #5E5E5E;"># what I will tell you</span></span>
<span id="cb47-14">value <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(C, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>) <span class="co" style="color: #5E5E5E;"># Output: (B, T, head_size)</span></span>
<span id="cb47-15"><span class="co" style="color: #5E5E5E;"># self-attention because k, q, v come all from the same input</span></span>
<span id="cb47-16">k <span class="op" style="color: #5E5E5E;">=</span> key(x) <span class="co" style="color: #5E5E5E;"># shape (B, T, head_size)</span></span>
<span id="cb47-17">q <span class="op" style="color: #5E5E5E;">=</span> query(x) <span class="co" style="color: #5E5E5E;"># shape (B, T, head_size)</span></span>
<span id="cb47-18">v <span class="op" style="color: #5E5E5E;">=</span> value(x) <span class="co" style="color: #5E5E5E;"># shape (B, T, head_size)</span></span>
<span id="cb47-19"></span>
<span id="cb47-20"><span class="co" style="color: #5E5E5E;"># now, we want to compute the attention</span></span>
<span id="cb47-21"><span class="co" style="color: #5E5E5E;"># we need to compute the dot product between k and q</span></span>
<span id="cb47-22">weight_matrix <span class="op" style="color: #5E5E5E;">=</span> q <span class="op" style="color: #5E5E5E;">@</span> k.transpose(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="co" style="color: #5E5E5E;"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb47-23"></span>
<span id="cb47-24"><span class="co" style="color: #5E5E5E;"># now we add the masking</span></span>
<span id="cb47-25"><span class="co" style="color: #5E5E5E;"># we want to mask out the future</span></span>
<span id="cb47-26"><span class="co" style="color: #5E5E5E;"># this is what is known as "decoder" block </span></span>
<span id="cb47-27">lower_triangular <span class="op" style="color: #5E5E5E;">=</span> torch.tril(torch.ones((T,T)))</span>
<span id="cb47-28">weight_matrix <span class="op" style="color: #5E5E5E;">=</span> weight_matrix.masked_fill(lower_triangular<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'-inf'</span>))</span>
<span id="cb47-29"></span>
<span id="cb47-30"><span class="co" style="color: #5E5E5E;"># use softmax to normalize</span></span>
<span id="cb47-31">weight_matrix <span class="op" style="color: #5E5E5E;">=</span> torch.softmax(weight_matrix, dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)<span class="op" style="color: #5E5E5E;">/</span>np.sqrt(head_size) <span class="co" style="color: #5E5E5E;"># shape (B, T, T)</span></span>
<span id="cb47-32"></span>
<span id="cb47-33">out <span class="op" style="color: #5E5E5E;">=</span> weight_matrix <span class="op" style="color: #5E5E5E;">@</span> v <span class="co" style="color: #5E5E5E;"># shape (B, T, T) @ (B, T, head_size) = (B, T, head_size)</span></span></code></pre></div>
</div>
<section id="interlude-why-do-we-divide-by-sqrthead_size-in-the-self-attention-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="interlude-why-do-we-divide-by-sqrthead_size-in-the-self-attention-mechanism">Interlude: Why do we divide by sqrt(head_size) in the self-attention mechanism?</h4>
<p>We used one more trick to make the training more stable. We scaled the weight_matrix by the square root of the head_size. <a href="https://ai.stackexchange.com/questions/21237/why-does-this-multiplication-of-q-and-k-have-a-variance-of-d-k-in-scaled">This is because the variance of the dot product is proportional to the dimensionality of the vectors.</a>. Not scaling the weight matrix can lead to numerical instability.</p>
<p>To see this, let’s run a quick experiment</p>
<div class="cell" data-execution_count="190">
<div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1">variances <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb48-2">dimensions <span class="op" style="color: #5E5E5E;">=</span> [<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">1000</span>, <span class="dv" style="color: #AD0000;">10000</span>, <span class="dv" style="color: #AD0000;">100000</span>]</span>
<span id="cb48-3"></span>
<span id="cb48-4"><span class="cf" style="color: #003B4F;">for</span> d <span class="kw" style="color: #003B4F;">in</span> dimensions:</span>
<span id="cb48-5"></span>
<span id="cb48-6">    k <span class="op" style="color: #5E5E5E;">=</span> torch.randn(B, T, d)</span>
<span id="cb48-7">    q <span class="op" style="color: #5E5E5E;">=</span> torch.randn(B, T, d)</span>
<span id="cb48-8"></span>
<span id="cb48-9">    <span class="co" style="color: #5E5E5E;"># compute the batched matrix product between k and q</span></span>
<span id="cb48-10">    weight_matrix <span class="op" style="color: #5E5E5E;">=</span> torch.bmm(q, k.transpose(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>))   <span class="co" style="color: #5E5E5E;"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb48-11">    variances.append(weight_matrix.var())</span></code></pre></div>
</div>
<div class="cell" data-execution_count="192">
<div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">plt.plot(dimensions, variances)</span>
<span id="cb49-2">plt.xscale(<span class="st" style="color: #20794D;">'log'</span>)</span>
<span id="cb49-3">plt.yscale(<span class="st" style="color: #20794D;">'log'</span>)</span>
<span id="cb49-4">plt.xlabel(<span class="st" style="color: #20794D;">'Dimensionality'</span>)</span>
<span id="cb49-5">plt.ylabel(<span class="st" style="color: #20794D;">'Variance'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="192">
<pre><code>Text(0, 0.5, 'Variance')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://kjablonka.com/blog/posts/building_an_llm/index_files/figure-html/cell-38-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>This has an important impact when we apply <code>softmax</code>. Positive and negative “outliers” will be “sequeezed” to 1 and 0. You can test this by creating a 1D tensor (<code>a</code>) and applying softmax on it. Then multiply the values in the tensor (<code>a</code>) and again apply softmax.</p>
<div class="cell" data-execution_count="195">
<div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><span class="bu" style="color: null;">print</span>(F.softmax(torch.tensor([<span class="fl" style="color: #AD0000;">1.</span>,<span class="fl" style="color: #AD0000;">2.</span>,<span class="fl" style="color: #AD0000;">3.</span>])),F.softmax(torch.tensor([<span class="fl" style="color: #AD0000;">1.</span>,<span class="fl" style="color: #AD0000;">2.</span>,<span class="fl" style="color: #AD0000;">3.</span>])<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">100</span>) )</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.0900, 0.2447, 0.6652]) tensor([0.0000e+00, 3.7835e-44, 1.0000e+00])</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/ipykernel_51170/1895642280.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  print(F.softmax(torch.tensor([1.,2.,3.])),F.softmax(torch.tensor([1.,2.,3.])*100) )</code></pre>
</div>
</div>
</section>
<section id="the-attention-mechanism" class="level4">
<h4 class="anchored" data-anchor-id="the-attention-mechanism">The attention mechanism</h4>
<p>Written as a formula, the attention mechanism is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAttention%7D(Q,%20K,%20V)%20=%20%5Ctext%7Bsoftmax%7D%5Cleft(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%5Cright)V%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Q"> is the query matrix, <img src="https://latex.codecogs.com/png.latex?K"> is the key matrix, and <img src="https://latex.codecogs.com/png.latex?V"> is the value matrix.</p>
</section>
<section id="refactoring-into-a-module" class="level3">
<h3 class="anchored" data-anchor-id="refactoring-into-a-module">Refactoring into a module</h3>
<div class="cell" data-execution_count="202">
<div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><span class="kw" style="color: #003B4F;">class</span> Head(nn.Module):</span>
<span id="cb54-2"></span>
<span id="cb54-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_embed, block_size, head_size):</span>
<span id="cb54-4">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb54-5">        <span class="va" style="color: #111111;">self</span>.key <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(n_embed, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb54-6">        <span class="va" style="color: #111111;">self</span>.query <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(n_embed, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb54-7">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(n_embed, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb54-8">        </span>
<span id="cb54-9">        <span class="va" style="color: #111111;">self</span>.register_buffer(<span class="st" style="color: #20794D;">'lower_triangular'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb54-10"></span>
<span id="cb54-11">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x): </span>
<span id="cb54-12">        B, T, C  <span class="op" style="color: #5E5E5E;">=</span> x.shape</span>
<span id="cb54-13">        key <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.key(x)</span>
<span id="cb54-14">        query <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.query(x) <span class="co" style="color: #5E5E5E;"># B, T, head</span></span>
<span id="cb54-15">        value <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.value(x)   <span class="co" style="color: #5E5E5E;"># B, T, head</span></span>
<span id="cb54-16"></span>
<span id="cb54-17">        weight_matrix <span class="op" style="color: #5E5E5E;">=</span> query <span class="op" style="color: #5E5E5E;">@</span> key.transpose(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="op" style="color: #5E5E5E;">*</span> C <span class="op" style="color: #5E5E5E;">**</span> (<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">0.5</span>) <span class="co" style="color: #5E5E5E;"># shape (B, T, head_size) @ (B, head_size, T) = (B, T, T)</span></span>
<span id="cb54-18">        weight_matrix <span class="op" style="color: #5E5E5E;">=</span> weight_matrix.masked_fill(<span class="va" style="color: #111111;">self</span>.lower_triangular[:T, :T].logical_not(), <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'-inf'</span>))</span>
<span id="cb54-19">        weight_matrix <span class="op" style="color: #5E5E5E;">=</span> F.softmax(weight_matrix, dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb54-20"></span>
<span id="cb54-21">        out <span class="op" style="color: #5E5E5E;">=</span> weight_matrix <span class="op" style="color: #5E5E5E;">@</span> value <span class="co" style="color: #5E5E5E;"># shape (B, T, T) @ (B, T, head_size) = (B, T, head_size)</span></span>
<span id="cb54-22">        <span class="cf" style="color: #003B4F;">return</span> out</span></code></pre></div>
</div>
</section>
<section id="revamped-bigram-model" class="level3">
<h3 class="anchored" data-anchor-id="revamped-bigram-model">Revamped Bigram Model</h3>
<p>Now, we can use it to “refine” our bigram model. We will additionally also perform two more changes:</p>
<ul>
<li>we will add positional embeddings: We will add the positional embeddings to the input embeddings. This will allow the model to take into account the position of the tokens in the sequence.</li>
<li>we will add one more indirection: One simple way of improving the expressiveness is to add one linear layer. While in the bigram model we only had one embedding layer (that mapped inputs of size <code>vocab_size</code> to <code>vocab_size</code>), we can now change the embedding layer to map inputs of size <code>vocab_size</code> to <code>embedding_size</code>. We can then add a linear layer that maps inputs of size <code>embedding_size</code> to <code>vocab_size</code>. This way, we can learn a more complex mapping from the embeddings to the next token.</li>
</ul>
<div class="cell" data-execution_count="203">
<div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><span class="kw" style="color: #003B4F;">class</span> SelfAttentionModel(nn.Module):</span>
<span id="cb55-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, vocab_size, embedding_dim, sequence_length<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, head_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>):</span>
<span id="cb55-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb55-4"></span>
<span id="cb55-5">        <span class="co" style="color: #5E5E5E;"># map the input ids to embeddings</span></span>
<span id="cb55-6">        <span class="va" style="color: #111111;">self</span>.token_embedding <span class="op" style="color: #5E5E5E;">=</span>  nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb55-7"></span>
<span id="cb55-8">        <span class="co" style="color: #5E5E5E;"># add positional embeddings (each position has its own learnable embedding vector)</span></span>
<span id="cb55-9">        <span class="va" style="color: #111111;">self</span>.positional_embedding <span class="op" style="color: #5E5E5E;">=</span> nn.Embedding(sequence_length, embedding_dim)</span>
<span id="cb55-10"></span>
<span id="cb55-11">        <span class="co" style="color: #5E5E5E;"># the self-attention layer</span></span>
<span id="cb55-12">        <span class="va" style="color: #111111;">self</span>.attention <span class="op" style="color: #5E5E5E;">=</span> Head(embedding_dim, sequence_length, head_size)</span>
<span id="cb55-13"></span>
<span id="cb55-14">        <span class="co" style="color: #5E5E5E;"># the linear layer that maps the output of the self-attention layer to the vocabulary size</span></span>
<span id="cb55-15">        <span class="va" style="color: #111111;">self</span>.lm_head <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(head_size, vocab_size)</span>
<span id="cb55-16"></span>
<span id="cb55-17">        <span class="co" style="color: #5E5E5E;"># store the sequence length</span></span>
<span id="cb55-18">        <span class="va" style="color: #111111;">self</span>.sequence_length <span class="op" style="color: #5E5E5E;">=</span> sequence_length</span>
<span id="cb55-19"></span>
<span id="cb55-20">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb55-21">        B, T <span class="op" style="color: #5E5E5E;">=</span> x.shape</span>
<span id="cb55-22">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.token_embedding(x) <span class="co" style="color: #5E5E5E;"># B, T, C </span></span>
<span id="cb55-23">        x <span class="op" style="color: #5E5E5E;">+=</span> <span class="va" style="color: #111111;">self</span>.positional_embedding(torch.arange(T, device<span class="op" style="color: #5E5E5E;">=</span>device)) <span class="co" style="color: #5E5E5E;"># B, T, C</span></span>
<span id="cb55-24">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.attention(x) <span class="co" style="color: #5E5E5E;"># B, T, head_size</span></span>
<span id="cb55-25">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.lm_head(x) <span class="co" style="color: #5E5E5E;"># B, T, vocab_size</span></span>
<span id="cb55-26">        <span class="co" style="color: #5E5E5E;"># The prediction is for each token a probability distribution over the vocabulary</span></span>
<span id="cb55-27">        <span class="co" style="color: #5E5E5E;"># this indicates how likely each token is the next token</span></span>
<span id="cb55-28">        <span class="cf" style="color: #003B4F;">return</span> x</span>
<span id="cb55-29"></span>
<span id="cb55-30">        </span>
<span id="cb55-31">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, x, y):</span>
<span id="cb55-32">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb55-33">        logits <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x) <span class="co" style="color: #5E5E5E;"># (B, T, C)</span></span>
<span id="cb55-34">        B, T, C <span class="op" style="color: #5E5E5E;">=</span> logits.shape</span>
<span id="cb55-35">        logits <span class="op" style="color: #5E5E5E;">=</span> logits.view(B<span class="op" style="color: #5E5E5E;">*</span>T, C)</span>
<span id="cb55-36">        y <span class="op" style="color: #5E5E5E;">=</span> y.view(B<span class="op" style="color: #5E5E5E;">*</span>T)</span>
<span id="cb55-37">        loss <span class="op" style="color: #5E5E5E;">=</span> F.cross_entropy(logits, y)</span>
<span id="cb55-38">        <span class="cf" style="color: #003B4F;">return</span> loss</span>
<span id="cb55-39">    </span>
<span id="cb55-40">    <span class="kw" style="color: #003B4F;">def</span> generate(<span class="va" style="color: #111111;">self</span>, x, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>):</span>
<span id="cb55-41">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb55-42">        <span class="co" style="color: #5E5E5E;"># we generate max_new_tokens new tokens</span></span>
<span id="cb55-43">        <span class="cf" style="color: #003B4F;">for</span> _t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(max_new_tokens):</span>
<span id="cb55-44">            logits <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x)</span>
<span id="cb55-45">            logits <span class="op" style="color: #5E5E5E;">=</span> logits[:, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, :]</span>
<span id="cb55-46">            probs <span class="op" style="color: #5E5E5E;">=</span> F.softmax(logits, dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb55-47">            next_token <span class="op" style="color: #5E5E5E;">=</span> torch.multinomial(probs, num_samples<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb55-48">            x <span class="op" style="color: #5E5E5E;">=</span> torch.cat([x, next_token], dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb55-49">        <span class="cf" style="color: #003B4F;">return</span> x</span>
<span id="cb55-50">        </span>
<span id="cb55-51">    </span></code></pre></div>
</div>
<div class="cell" data-execution_count="204">
<div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">model <span class="op" style="color: #5E5E5E;">=</span> SelfAttentionModel(<span class="bu" style="color: null;">len</span>(tokenizer.tokens), embedding_dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">128</span>, sequence_length<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">40</span>, head_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb56-2"></span>
<span id="cb56-3">train_model(model, train_loader, valid_loader, epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, iter 0, train loss 4.124, val perplexity 55.82777
Epoch 0, iter 100, train loss 2.388, val perplexity 11.19858
Epoch 0, iter 200, train loss 2.387, val perplexity 11.12724
Epoch 0, iter 300, train loss 2.394, val perplexity 11.12101
Epoch 0, iter 400, train loss 2.411, val perplexity 11.12952
Epoch 0, iter 500, train loss 2.412, val perplexity 11.10590
Epoch 0, iter 600, train loss 2.409, val perplexity 11.11807
Epoch 0, iter 700, train loss 2.424, val perplexity 11.10892
Epoch 0, iter 800, train loss 2.441, val perplexity 11.11399
Epoch 0, iter 900, train loss 2.410, val perplexity 11.10943
Epoch 0, iter 1000, train loss 2.420, val perplexity 11.11094
Epoch 0, iter 1100, train loss 2.394, val perplexity 11.10506
Epoch 0, iter 1200, train loss 2.428, val perplexity 11.09540
Epoch 0, iter 1300, train loss 2.430, val perplexity 11.09374
Epoch 0, iter 1400, train loss 2.365, val perplexity 11.10669
Epoch 0, iter 1500, train loss 2.418, val perplexity 11.10741
Epoch 0, iter 1600, train loss 2.384, val perplexity 11.10166
Epoch 0, iter 1700, train loss 2.412, val perplexity 11.11698
Epoch 0, iter 1800, train loss 2.447, val perplexity 11.09141
Epoch 0, iter 1900, train loss 2.408, val perplexity 11.08764
Epoch 0, iter 2000, train loss 2.387, val perplexity 11.10106
Epoch 0, iter 2100, train loss 2.415, val perplexity 11.09020
Epoch 0, iter 2200, train loss 2.423, val perplexity 11.10499
Epoch 0, iter 2300, train loss 2.394, val perplexity 11.10698
Epoch 1, iter 0, train loss 2.416, val perplexity 11.09823
Epoch 1, iter 100, train loss 2.377, val perplexity 11.09025
Epoch 1, iter 200, train loss 2.391, val perplexity 11.08770
Epoch 1, iter 300, train loss 2.383, val perplexity 11.09473
Epoch 1, iter 400, train loss 2.411, val perplexity 11.09210
Epoch 1, iter 500, train loss 2.369, val perplexity 11.10044
Epoch 1, iter 600, train loss 2.402, val perplexity 11.11767
Epoch 1, iter 700, train loss 2.429, val perplexity 11.09801
Epoch 1, iter 800, train loss 2.400, val perplexity 11.08748
Epoch 1, iter 900, train loss 2.373, val perplexity 11.08818
Epoch 1, iter 1000, train loss 2.411, val perplexity 11.08949
Epoch 1, iter 1100, train loss 2.405, val perplexity 11.08937
Epoch 1, iter 1200, train loss 2.389, val perplexity 11.09055
Epoch 1, iter 1300, train loss 2.381, val perplexity 11.09659
Epoch 1, iter 1400, train loss 2.394, val perplexity 11.09339
Epoch 1, iter 1500, train loss 2.403, val perplexity 11.09767
Epoch 1, iter 1600, train loss 2.384, val perplexity 11.09845
Epoch 1, iter 1700, train loss 2.422, val perplexity 11.09206
Epoch 1, iter 1800, train loss 2.405, val perplexity 11.09331
Epoch 1, iter 1900, train loss 2.415, val perplexity 11.08692
Epoch 1, iter 2000, train loss 2.415, val perplexity 11.09814
Epoch 1, iter 2100, train loss 2.436, val perplexity 11.08876
Epoch 1, iter 2200, train loss 2.394, val perplexity 11.08603
Epoch 1, iter 2300, train loss 2.398, val perplexity 11.08688
Epoch 2, iter 0, train loss 2.391, val perplexity 11.09284
Epoch 2, iter 100, train loss 2.390, val perplexity 11.08920
Epoch 2, iter 200, train loss 2.408, val perplexity 11.09971
Epoch 2, iter 300, train loss 2.407, val perplexity 11.10155
Epoch 2, iter 400, train loss 2.371, val perplexity 11.08800
Epoch 2, iter 500, train loss 2.377, val perplexity 11.09237
Epoch 2, iter 600, train loss 2.397, val perplexity 11.08786
Epoch 2, iter 700, train loss 2.432, val perplexity 11.09027
Epoch 2, iter 800, train loss 2.453, val perplexity 11.09479
Epoch 2, iter 900, train loss 2.427, val perplexity 11.09070
Epoch 2, iter 1000, train loss 2.412, val perplexity 11.09146
Epoch 2, iter 1100, train loss 2.417, val perplexity 11.08907
Epoch 2, iter 1200, train loss 2.356, val perplexity 11.09570
Epoch 2, iter 1300, train loss 2.415, val perplexity 11.08749
Epoch 2, iter 1400, train loss 2.414, val perplexity 11.08112
Epoch 2, iter 1500, train loss 2.398, val perplexity 11.08555
Epoch 2, iter 1600, train loss 2.422, val perplexity 11.09179
Epoch 2, iter 1700, train loss 2.399, val perplexity 11.08997
Epoch 2, iter 1800, train loss 2.414, val perplexity 11.09650
Epoch 2, iter 1900, train loss 2.409, val perplexity 11.08493
Epoch 2, iter 2000, train loss 2.409, val perplexity 11.08983
Epoch 2, iter 2100, train loss 2.417, val perplexity 11.08645
Epoch 2, iter 2200, train loss 2.384, val perplexity 11.08418
Epoch 2, iter 2300, train loss 2.390, val perplexity 11.08305
Epoch 3, iter 0, train loss 2.380, val perplexity 11.08910
Epoch 3, iter 100, train loss 2.443, val perplexity 11.08976
Epoch 3, iter 200, train loss 2.429, val perplexity 11.08478
Epoch 3, iter 300, train loss 2.404, val perplexity 11.08251
Epoch 3, iter 400, train loss 2.411, val perplexity 11.08396
Epoch 3, iter 500, train loss 2.415, val perplexity 11.09192
Epoch 3, iter 600, train loss 2.430, val perplexity 11.08152
Epoch 3, iter 700, train loss 2.429, val perplexity 11.09316
Epoch 3, iter 800, train loss 2.448, val perplexity 11.08478
Epoch 3, iter 900, train loss 2.447, val perplexity 11.08606
Epoch 3, iter 1000, train loss 2.399, val perplexity 11.09091
Epoch 3, iter 1100, train loss 2.408, val perplexity 11.08513
Epoch 3, iter 1200, train loss 2.421, val perplexity 11.08521
Epoch 3, iter 1300, train loss 2.434, val perplexity 11.08139
Epoch 3, iter 1400, train loss 2.423, val perplexity 11.08271
Epoch 3, iter 1500, train loss 2.428, val perplexity 11.08595
Epoch 3, iter 1600, train loss 2.423, val perplexity 11.08252
Epoch 3, iter 1700, train loss 2.417, val perplexity 11.09312
Epoch 3, iter 1800, train loss 2.450, val perplexity 11.08849
Epoch 3, iter 1900, train loss 2.416, val perplexity 11.09376
Epoch 3, iter 2000, train loss 2.402, val perplexity 11.08234
Epoch 3, iter 2100, train loss 2.387, val perplexity 11.08825
Epoch 3, iter 2200, train loss 2.410, val perplexity 11.08716
Epoch 3, iter 2300, train loss 2.416, val perplexity 11.08786
Epoch 4, iter 0, train loss 2.431, val perplexity 11.08732
Epoch 4, iter 100, train loss 2.418, val perplexity 11.08994
Epoch 4, iter 200, train loss 2.375, val perplexity 11.08289
Epoch 4, iter 300, train loss 2.402, val perplexity 11.09380
Epoch 4, iter 400, train loss 2.444, val perplexity 11.08196
Epoch 4, iter 500, train loss 2.430, val perplexity 11.09146
Epoch 4, iter 600, train loss 2.414, val perplexity 11.08931
Epoch 4, iter 700, train loss 2.392, val perplexity 11.08145
Epoch 4, iter 800, train loss 2.431, val perplexity 11.08986
Epoch 4, iter 900, train loss 2.409, val perplexity 11.08562
Epoch 4, iter 1000, train loss 2.381, val perplexity 11.08575
Epoch 4, iter 1100, train loss 2.390, val perplexity 11.08247
Epoch 4, iter 1200, train loss 2.390, val perplexity 11.09000
Epoch 4, iter 1300, train loss 2.393, val perplexity 11.09160
Epoch 4, iter 1400, train loss 2.397, val perplexity 11.08732
Epoch 4, iter 1500, train loss 2.387, val perplexity 11.08443
Epoch 4, iter 1600, train loss 2.387, val perplexity 11.09168
Epoch 4, iter 1700, train loss 2.372, val perplexity 11.08432
Epoch 4, iter 1800, train loss 2.404, val perplexity 11.08689
Epoch 4, iter 1900, train loss 2.419, val perplexity 11.08175
Epoch 4, iter 2000, train loss 2.400, val perplexity 11.08565
Epoch 4, iter 2100, train loss 2.417, val perplexity 11.08862
Epoch 4, iter 2200, train loss 2.406, val perplexity 11.07848
Epoch 4, iter 2300, train loss 2.381, val perplexity 11.08443
Epoch 5, iter 0, train loss 2.433, val perplexity 11.08817
Epoch 5, iter 100, train loss 2.406, val perplexity 11.08162
Epoch 5, iter 200, train loss 2.414, val perplexity 11.08919
Epoch 5, iter 300, train loss 2.423, val perplexity 11.08315
Epoch 5, iter 400, train loss 2.418, val perplexity 11.08115
Epoch 5, iter 500, train loss 2.402, val perplexity 11.08391
Epoch 5, iter 600, train loss 2.412, val perplexity 11.09365
Epoch 5, iter 700, train loss 2.399, val perplexity 11.08553
Epoch 5, iter 800, train loss 2.385, val perplexity 11.08504
Epoch 5, iter 900, train loss 2.409, val perplexity 11.09162
Epoch 5, iter 1000, train loss 2.401, val perplexity 11.08724
Epoch 5, iter 1100, train loss 2.400, val perplexity 11.08480
Epoch 5, iter 1200, train loss 2.381, val perplexity 11.07982
Epoch 5, iter 1300, train loss 2.428, val perplexity 11.08074
Epoch 5, iter 1400, train loss 2.402, val perplexity 11.08076
Epoch 5, iter 1500, train loss 2.370, val perplexity 11.08601
Epoch 5, iter 1600, train loss 2.444, val perplexity 11.08128
Epoch 5, iter 1700, train loss 2.384, val perplexity 11.07793
Epoch 5, iter 1800, train loss 2.404, val perplexity 11.07864
Epoch 5, iter 1900, train loss 2.393, val perplexity 11.08708
Epoch 5, iter 2000, train loss 2.424, val perplexity 11.08692
Epoch 5, iter 2100, train loss 2.423, val perplexity 11.08698
Epoch 5, iter 2200, train loss 2.387, val perplexity 11.07958
Epoch 5, iter 2300, train loss 2.400, val perplexity 11.08280
Epoch 6, iter 0, train loss 2.425, val perplexity 11.08352
Epoch 6, iter 100, train loss 2.393, val perplexity 11.08390
Epoch 6, iter 200, train loss 2.404, val perplexity 11.07994
Epoch 6, iter 300, train loss 2.375, val perplexity 11.08319
Epoch 6, iter 400, train loss 2.426, val perplexity 11.08233
Epoch 6, iter 500, train loss 2.398, val perplexity 11.07754
Epoch 6, iter 600, train loss 2.389, val perplexity 11.08095
Epoch 6, iter 700, train loss 2.377, val perplexity 11.08012
Epoch 6, iter 800, train loss 2.385, val perplexity 11.08026
Epoch 6, iter 900, train loss 2.406, val perplexity 11.07890
Epoch 6, iter 1000, train loss 2.388, val perplexity 11.07917
Epoch 6, iter 1100, train loss 2.445, val perplexity 11.08292
Epoch 6, iter 1200, train loss 2.435, val perplexity 11.08142
Epoch 6, iter 1300, train loss 2.396, val perplexity 11.08304
Epoch 6, iter 1400, train loss 2.375, val perplexity 11.08752
Epoch 6, iter 1500, train loss 2.401, val perplexity 11.07907
Epoch 6, iter 1600, train loss 2.397, val perplexity 11.08185
Epoch 6, iter 1700, train loss 2.416, val perplexity 11.08626
Epoch 6, iter 1800, train loss 2.388, val perplexity 11.08229
Epoch 6, iter 1900, train loss 2.404, val perplexity 11.08109
Epoch 6, iter 2000, train loss 2.380, val perplexity 11.08226
Epoch 6, iter 2100, train loss 2.446, val perplexity 11.08410
Epoch 6, iter 2200, train loss 2.421, val perplexity 11.07880
Epoch 6, iter 2300, train loss 2.444, val perplexity 11.08569
Epoch 7, iter 0, train loss 2.407, val perplexity 11.08170
Epoch 7, iter 100, train loss 2.394, val perplexity 11.08424
Epoch 7, iter 200, train loss 2.390, val perplexity 11.08440
Epoch 7, iter 300, train loss 2.393, val perplexity 11.08069
Epoch 7, iter 400, train loss 2.419, val perplexity 11.08156
Epoch 7, iter 500, train loss 2.400, val perplexity 11.08258
Epoch 7, iter 600, train loss 2.406, val perplexity 11.08027
Epoch 7, iter 700, train loss 2.418, val perplexity 11.08211
Epoch 7, iter 800, train loss 2.415, val perplexity 11.08057
Epoch 7, iter 900, train loss 2.403, val perplexity 11.07885
Epoch 7, iter 1000, train loss 2.395, val perplexity 11.07921
Epoch 7, iter 1100, train loss 2.400, val perplexity 11.08124
Epoch 7, iter 1200, train loss 2.425, val perplexity 11.08465
Epoch 7, iter 1300, train loss 2.395, val perplexity 11.08027
Epoch 7, iter 1400, train loss 2.407, val perplexity 11.08265
Epoch 7, iter 1500, train loss 2.397, val perplexity 11.08360
Epoch 7, iter 1600, train loss 2.412, val perplexity 11.08794
Epoch 7, iter 1700, train loss 2.409, val perplexity 11.08159
Epoch 7, iter 1800, train loss 2.406, val perplexity 11.07970
Epoch 7, iter 1900, train loss 2.406, val perplexity 11.08356
Epoch 7, iter 2000, train loss 2.423, val perplexity 11.08166
Epoch 7, iter 2100, train loss 2.397, val perplexity 11.07727
Epoch 7, iter 2200, train loss 2.409, val perplexity 11.08467
Epoch 7, iter 2300, train loss 2.370, val perplexity 11.07904
Epoch 8, iter 0, train loss 2.368, val perplexity 11.08059
Epoch 8, iter 100, train loss 2.423, val perplexity 11.08200
Epoch 8, iter 200, train loss 2.410, val perplexity 11.07815
Epoch 8, iter 300, train loss 2.413, val perplexity 11.08172
Epoch 8, iter 400, train loss 2.372, val perplexity 11.07827
Epoch 8, iter 500, train loss 2.403, val perplexity 11.07991
Epoch 8, iter 600, train loss 2.408, val perplexity 11.07978
Epoch 8, iter 700, train loss 2.369, val perplexity 11.08273
Epoch 8, iter 800, train loss 2.397, val perplexity 11.08346
Epoch 8, iter 900, train loss 2.417, val perplexity 11.08189
Epoch 8, iter 1000, train loss 2.438, val perplexity 11.07828
Epoch 8, iter 1100, train loss 2.423, val perplexity 11.07997
Epoch 8, iter 1200, train loss 2.427, val perplexity 11.08032
Epoch 8, iter 1300, train loss 2.408, val perplexity 11.08005
Epoch 8, iter 1400, train loss 2.395, val perplexity 11.07992
Epoch 8, iter 1500, train loss 2.389, val perplexity 11.07970
Epoch 8, iter 1600, train loss 2.421, val perplexity 11.08282
Epoch 8, iter 1700, train loss 2.410, val perplexity 11.07893
Epoch 8, iter 1800, train loss 2.402, val perplexity 11.07853
Epoch 8, iter 1900, train loss 2.387, val perplexity 11.07859
Epoch 8, iter 2000, train loss 2.396, val perplexity 11.08859
Epoch 8, iter 2100, train loss 2.424, val perplexity 11.07993
Epoch 8, iter 2200, train loss 2.389, val perplexity 11.07753
Epoch 8, iter 2300, train loss 2.425, val perplexity 11.07979
Epoch 9, iter 0, train loss 2.393, val perplexity 11.08574
Epoch 9, iter 100, train loss 2.445, val perplexity 11.08460
Epoch 9, iter 200, train loss 2.343, val perplexity 11.08016
Epoch 9, iter 300, train loss 2.422, val perplexity 11.08048
Epoch 9, iter 400, train loss 2.400, val perplexity 11.08331
Epoch 9, iter 500, train loss 2.398, val perplexity 11.09080
Epoch 9, iter 600, train loss 2.383, val perplexity 11.08066
Epoch 9, iter 700, train loss 2.417, val perplexity 11.08187
Epoch 9, iter 800, train loss 2.441, val perplexity 11.08072
Epoch 9, iter 900, train loss 2.416, val perplexity 11.07930
Epoch 9, iter 1000, train loss 2.439, val perplexity 11.08149
Epoch 9, iter 1100, train loss 2.384, val perplexity 11.07934
Epoch 9, iter 1200, train loss 2.383, val perplexity 11.07627
Epoch 9, iter 1300, train loss 2.382, val perplexity 11.07983
Epoch 9, iter 1400, train loss 2.414, val perplexity 11.08299
Epoch 9, iter 1500, train loss 2.419, val perplexity 11.08153
Epoch 9, iter 1600, train loss 2.412, val perplexity 11.08024
Epoch 9, iter 1700, train loss 2.411, val perplexity 11.07876
Epoch 9, iter 1800, train loss 2.430, val perplexity 11.08331
Epoch 9, iter 1900, train loss 2.428, val perplexity 11.07763
Epoch 9, iter 2000, train loss 2.404, val perplexity 11.08191
Epoch 9, iter 2100, train loss 2.402, val perplexity 11.08576
Epoch 9, iter 2200, train loss 2.370, val perplexity 11.08004
Epoch 9, iter 2300, train loss 2.408, val perplexity 11.07864</code></pre>
</div>
</div>
<div class="cell" data-execution_count="207">
<div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">a <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([[<span class="dv" style="color: #AD0000;">4</span>]])</span>
<span id="cb58-2">a <span class="op" style="color: #5E5E5E;">=</span> a.to(device)</span>
<span id="cb58-3">generation <span class="op" style="color: #5E5E5E;">=</span> model.generate(a, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>).cpu().numpy()</span>
<span id="cb58-4">tokenizer.decode(generation[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="207">
<pre><code>'[C@@])43[C][C])#[S@+]C4'</code></pre>
</div>
</div>
</section>
</section>
<section id="interlude-additional-perspectives-on-attention" class="level2">
<h2 class="anchored" data-anchor-id="interlude-additional-perspectives-on-attention">Interlude: Additional perspectives on attention</h2>
<section id="attention-as-gnn" class="level3">
<h3 class="anchored" data-anchor-id="attention-as-gnn">Attention as GNN</h3>
<ul>
<li><p>In the attention mechanism we learn how different tokens “communicate” with each other. If we think of tokens as nodes, attention corresponds to learning the edge weights of a fully connected graph.</p></li>
<li><p>The tokens per default have no notion of their position in the sequence. It is basically the communication between sets of vectors.</p></li>
</ul>
<p>In attentional GNNs, we write for the embeddings:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bh%7D_i=%5Cphi%5Cleft(%5Cmathbf%7Bx%7D_i,%20%5Cbigoplus_%7Bj%20%5Cin%20%5Cmathcal%7BV%7D%7D%20a%5Cleft(%5Cmathbf%7Bx%7D_i,%20%5Cmathbf%7Bx%7D_j%5Cright)%20%5Cpsi%5Cleft(%5Cmathbf%7Bx%7D_j%5Cright)%5Cright)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbigoplus"> is a permutation invariant function, e.g., sum or mean over the neighborhood <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BV%7D">. <a href="https://petar-v.com/talks/GNN-EEML.pdf">Does this equation look familiar?</a></p>
<p>You can find more information <a href="https://thegradient.pub/transformers-are-graph-neural-networks/">here</a> and <a href="https://arxiv.org/pdf/2301.08210.pdf">here</a>.</p>
<p>The main difference is that in the transformer we model a fully connected graph, whereas in GNNs we model a sparse graph (which is an inductive bias).</p>
</section>
<section id="attention-as-kernel-smoothing" class="level3">
<h3 class="anchored" data-anchor-id="attention-as-kernel-smoothing">Attention as Kernel smoothing</h3>
<ul>
<li>Given that we have been introducing the attention mechanism as a way to compute a weighted average of values, the analogy to a kernel is quite natural.</li>
</ul>
<p>To understand this a bit better, let us introduce <a href="https://en.wikipedia.org/wiki/Kernel_regression">kernel smoothing</a>. Again, it is nothing else then a weighted average. In this weighted average, the weights are determined by a kernel function.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5En%20y_i%20%5Cfrac%7BK%5Cleft(x_i,%20x_o%5Cright)%7D%7B%5Csum_%7Bj=1%7D%5En%20K%5Cleft(x_j,%20x_o%5Cright)%7D,%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?(x_1,%20y_1),%20%5Cdots,%20(x_n,%20y_n)"> are the training points and <img src="https://latex.codecogs.com/png.latex?x_o"> is the point at which we want to make a prediction.</p>
<p>A common kernel function is the Gaussian kernel:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AK(x,%20x_o)%20=%20%5Cexp%5Cleft(xx_o%5Cright)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is a hyperparameter.</p>
<p>We are also free to add weights</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AK(x,%20x_o)%20=%20%5Cexp%5Cleft(%5Cmathbf%7Bw%7D_1%20x%20%20%5Cmathbf%7Bw%7D_2%20x_o%5Cright)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?w"> are square weight matrices. For stability, we might divide by the dimensionality of <img src="https://latex.codecogs.com/png.latex?x">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AK(x,%20x_o)%20=%20%5Cexp%5Cleft(%5Cfrac%7B%5Cmathbf%7Bw%7D_1%20x%20%20%5Cmathbf%7Bw%7D_2%20x_o%7D%7B%5Csqrt%7Bd%7D%7D%5Cright)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?d"> is the dimensionality of <img src="https://latex.codecogs.com/png.latex?x">.</p>
<p>Compare this to the attention equation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAttention%7D(Q,%20K,%20V)%20=%20%5Ctext%7Bsoftmax%7D%5Cleft(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%5Cright)V%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?d_k"> is the dimension of <img src="https://latex.codecogs.com/png.latex?K"> and <img src="https://latex.codecogs.com/png.latex?Q">.</p>
<p>You can find more information on this perspective <a href="http://bactra.org/notebooks/nn-attention-and-transformers.html">here</a>.</p>
</section>
</section>
<section id="adding-more-expressive-power-with-more-heads-and-fully-connected-layers" class="level2">
<h2 class="anchored" data-anchor-id="adding-more-expressive-power-with-more-heads-and-fully-connected-layers">Adding more expressive power with more heads and fully connected layers</h2>
<p>A very simple way to improve the attention mechanism is to use multiple attention heads. That is we apply the attention mechanism multiple times and then concatenate the results.</p>
<p>The intuition behind this is that different attention heads can learn different attention patterns.</p>
<div class="cell" data-execution_count="208">
<div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><span class="kw" style="color: #003B4F;">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb60-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, num_heads, n_embed, block_size, head_size):</span>
<span id="cb60-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb60-4">        <span class="va" style="color: #111111;">self</span>.heads <span class="op" style="color: #5E5E5E;">=</span> nn.ModuleList([Head(n_embed, block_size, head_size) <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_heads)])</span>
<span id="cb60-5"></span>
<span id="cb60-6">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb60-7">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T, C)</span></span>
<span id="cb60-8">        <span class="co" style="color: #5E5E5E;"># we want to compute the attention for each head</span></span>
<span id="cb60-9">        <span class="co" style="color: #5E5E5E;"># and then concatenate the results</span></span>
<span id="cb60-10">        <span class="co" style="color: #5E5E5E;"># we will have a tensor of shape (B, T, num_heads * head_size)</span></span>
<span id="cb60-11">        <span class="co" style="color: #5E5E5E;"># in practice, we might not concatenate but add another dimension</span></span>
<span id="cb60-12">        <span class="co" style="color: #5E5E5E;"># to the tensors</span></span>
<span id="cb60-13">        <span class="cf" style="color: #003B4F;">return</span> torch.cat([head(x) <span class="cf" style="color: #003B4F;">for</span> head <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.heads], dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
<p>Once we let the tokens talk to each other we currently only used one linear layer to map to the outputs. We can expect better performance if we use multiple layers.</p>
<p>One typically uses wide linear layers that can more readily be parallelized than deep linear layers.</p>
<div class="cell" data-execution_count="209">
<div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><span class="kw" style="color: #003B4F;">class</span> FeedForwardLayer(nn.Module):</span>
<span id="cb61-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_embed, hidden):</span>
<span id="cb61-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb61-4">        <span class="va" style="color: #111111;">self</span>.net <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb61-5">            nn.Linear(n_embed, hidden),</span>
<span id="cb61-6">            nn.ReLU(),<span class="co" style="color: #5E5E5E;"># </span></span>
<span id="cb61-7">            nn.Linear(hidden, n_embed)</span>
<span id="cb61-8">        )</span>
<span id="cb61-9">        </span>
<span id="cb61-10">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb61-11">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.net(x)</span></code></pre></div>
</div>
<p>If we put it together, it looks like this:</p>
<div class="cell" data-execution_count="210">
<div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><span class="kw" style="color: #003B4F;">class</span> SelfAttentionModel(nn.Module):</span>
<span id="cb62-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, vocab_size, embedding_dim, sequence_length<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>, head_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">12</span>, num_heads<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>):</span>
<span id="cb62-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb62-4">        <span class="co" style="color: #5E5E5E;"># read of the logits of the next token from table</span></span>
<span id="cb62-5">        <span class="va" style="color: #111111;">self</span>.token_embedding <span class="op" style="color: #5E5E5E;">=</span> nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb62-6">        <span class="va" style="color: #111111;">self</span>.positional_embedding <span class="op" style="color: #5E5E5E;">=</span> nn.Embedding(sequence_length, embedding_dim)</span>
<span id="cb62-7">        <span class="va" style="color: #111111;">self</span>.lm_head <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(head_size, vocab_size)</span>
<span id="cb62-8">        <span class="va" style="color: #111111;">self</span>.sequence_length <span class="op" style="color: #5E5E5E;">=</span> sequence_length</span>
<span id="cb62-9">        <span class="va" style="color: #111111;">self</span>.attention <span class="op" style="color: #5E5E5E;">=</span> MultiHeadAttention(num_heads, embedding_dim, sequence_length, head_size)</span>
<span id="cb62-10">        <span class="va" style="color: #111111;">self</span>.feed_forward <span class="op" style="color: #5E5E5E;">=</span> FeedForwardLayer(embedding_dim, <span class="dv" style="color: #AD0000;">4</span><span class="op" style="color: #5E5E5E;">*</span>embedding_dim)</span>
<span id="cb62-11"></span>
<span id="cb62-12">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb62-13">        B, T <span class="op" style="color: #5E5E5E;">=</span> x.shape</span>
<span id="cb62-14">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.token_embedding(x)</span>
<span id="cb62-15">        x <span class="op" style="color: #5E5E5E;">+=</span> <span class="va" style="color: #111111;">self</span>.positional_embedding(torch.arange(T, device<span class="op" style="color: #5E5E5E;">=</span>device))</span>
<span id="cb62-16">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.attention(x)</span>
<span id="cb62-17">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.lm_head(x)</span>
<span id="cb62-18">        <span class="cf" style="color: #003B4F;">return</span> x</span>
<span id="cb62-19">    </span>
<span id="cb62-20">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, x, y):</span>
<span id="cb62-21">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb62-22">        logits <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x) <span class="co" style="color: #5E5E5E;"># (B, T, C)</span></span>
<span id="cb62-23">        B, T, C <span class="op" style="color: #5E5E5E;">=</span> logits.shape</span>
<span id="cb62-24">        logits <span class="op" style="color: #5E5E5E;">=</span> logits.view(B<span class="op" style="color: #5E5E5E;">*</span>T, C)</span>
<span id="cb62-25">        y <span class="op" style="color: #5E5E5E;">=</span> y.view(B<span class="op" style="color: #5E5E5E;">*</span>T)</span>
<span id="cb62-26">        loss <span class="op" style="color: #5E5E5E;">=</span> F.cross_entropy(logits, y)</span>
<span id="cb62-27">        <span class="cf" style="color: #003B4F;">return</span> loss</span>
<span id="cb62-28">    </span>
<span id="cb62-29">    <span class="kw" style="color: #003B4F;">def</span> generate(<span class="va" style="color: #111111;">self</span>, x, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>):</span>
<span id="cb62-30">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb62-31">        <span class="co" style="color: #5E5E5E;"># we generate max_new_tokens new tokens</span></span>
<span id="cb62-32">        new_tokens <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb62-33">        <span class="cf" style="color: #003B4F;">for</span> _t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(max_new_tokens):</span>
<span id="cb62-34">            x_ <span class="op" style="color: #5E5E5E;">=</span> x[:, <span class="op" style="color: #5E5E5E;">-</span><span class="va" style="color: #111111;">self</span>.sequence_length:]</span>
<span id="cb62-35">            logits <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x_) <span class="co" style="color: #5E5E5E;"># (B, T, C)</span></span>
<span id="cb62-36">            logits <span class="op" style="color: #5E5E5E;">=</span> logits[:, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, :] <span class="co" style="color: #5E5E5E;"># we only care about the last token in Bigram, hence we bow have shape (B, C)</span></span>
<span id="cb62-37">            probs <span class="op" style="color: #5E5E5E;">=</span> F.softmax(logits, dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="co" style="color: #5E5E5E;"># we generate probabilities for the next token</span></span>
<span id="cb62-38"></span>
<span id="cb62-39">            <span class="co" style="color: #5E5E5E;"># torch.multinomial(probs, num_samples=1) returns a tensor of shape (B, 1) </span></span>
<span id="cb62-40">            <span class="co" style="color: #5E5E5E;"># where each element is the index of the sampled token</span></span>
<span id="cb62-41">            next_token <span class="op" style="color: #5E5E5E;">=</span> torch.multinomial(probs, num_samples<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb62-42">            new_tokens.append(next_token)</span>
<span id="cb62-43">            x <span class="op" style="color: #5E5E5E;">=</span> torch.cat([x, next_token], dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb62-44">        <span class="cf" style="color: #003B4F;">return</span> x</span>
<span id="cb62-45">        </span>
<span id="cb62-46">    </span></code></pre></div>
</div>
</section>
<section id="abstracting-transformers-into-blocks" class="level2">
<h2 class="anchored" data-anchor-id="abstracting-transformers-into-blocks">Abstracting transformers into blocks</h2>
<p>It turns out that we can improve the performance by performing the self-attention and feedforward multiple times. For this, it is useful to extract the reusable parts into a block.</p>
<p>However, just making the model deeper can lead to problems with training. To avoid this, we will leverage two tricks: - we will use residual connections: they allow us to “skip” over layers. During optimization, there will be a “shortcut” to between the input and the output of the block. - we will use layer normalization: it allows us to normalize the activations of a layer - we will add dropout: it allows us to randomly drop activations during training. This can be seen as a form of regularization.</p>
<p>We will apply layer norm twice: - once directly on the input - then before we pass the multihead attention output to the feedforward layer</p>
<p>Note that <a href="https://magazine.sebastianraschka.com/p/why-the-original-transformer-figure">there is some debate</a> on where layer norm is optimally placed.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
Fun fact: did you know that the original Attention Is All Your Need transformer figure is wrong? <br><br>It places the layer normalization between the residual blocks, which doesn't match the code: <a href="https://t.co/z1oMLFpmiZ">https://t.co/z1oMLFpmiZ</a><br><br>PS: This is known as Post-LN Transformer<br><br>1/3 <a href="https://t.co/OOvp4FA8Nz">pic.twitter.com/OOvp4FA8Nz</a>
</p>
— Sebastian Raschka (<span class="citation" data-cites="rasbt">@rasbt</span>) <a href="https://twitter.com/rasbt/status/1655575611979489282?ref_src=twsrc%5Etfw">May 8, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<div class="cell" data-execution_count="211">
<div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><span class="kw" style="color: #003B4F;">class</span> Block(nn.Module):</span>
<span id="cb63-2">    <span class="co" style="color: #5E5E5E;">""" Transformer block: communication followed by computation """</span></span>
<span id="cb63-3"></span>
<span id="cb63-4">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_embd, block_size, n_head):</span>
<span id="cb63-5">        <span class="co" style="color: #5E5E5E;"># n_embd: embedding dimension, n_head: the number of heads we'd like</span></span>
<span id="cb63-6">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb63-7">        head_size <span class="op" style="color: #5E5E5E;">=</span> n_embd <span class="op" style="color: #5E5E5E;">//</span> n_head</span>
<span id="cb63-8">        <span class="va" style="color: #111111;">self</span>.sa <span class="op" style="color: #5E5E5E;">=</span> MultiHeadAttention(num_heads<span class="op" style="color: #5E5E5E;">=</span>n_head, n_embed<span class="op" style="color: #5E5E5E;">=</span>n_embd, block_size<span class="op" style="color: #5E5E5E;">=</span>block_size, head_size<span class="op" style="color: #5E5E5E;">=</span>head_size)</span>
<span id="cb63-9">        <span class="va" style="color: #111111;">self</span>.ffwd <span class="op" style="color: #5E5E5E;">=</span> FeedForwardLayer(n_embd, n_embd<span class="op" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb63-10">        <span class="va" style="color: #111111;">self</span>.ln1 <span class="op" style="color: #5E5E5E;">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb63-11">        <span class="va" style="color: #111111;">self</span>.ln2 <span class="op" style="color: #5E5E5E;">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb63-12"></span>
<span id="cb63-13">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb63-14">        x <span class="op" style="color: #5E5E5E;">=</span> x <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.sa(<span class="va" style="color: #111111;">self</span>.ln1(x)) <span class="co" style="color: #5E5E5E;"># residual connection</span></span>
<span id="cb63-15">        x <span class="op" style="color: #5E5E5E;">=</span> x <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.ffwd(<span class="va" style="color: #111111;">self</span>.ln2(x))</span>
<span id="cb63-16">        <span class="cf" style="color: #003B4F;">return</span> x</span></code></pre></div>
</div>
<p>An important thing to realize is that a bulk of the parameters is in the <code>FeedForwardLayer</code>.</p>
<div class="cell" data-execution_count="212">
<div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1">block <span class="op" style="color: #5E5E5E;">=</span> Block(<span class="dv" style="color: #AD0000;">128</span>, <span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">4</span>)</span>
<span id="cb64-2">get_num_parameters_per_layer(block)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="212">
<pre><code>{'sa.heads.0.key.weight': 4096,
 'sa.heads.0.query.weight': 4096,
 'sa.heads.0.value.weight': 4096,
 'sa.heads.1.key.weight': 4096,
 'sa.heads.1.query.weight': 4096,
 'sa.heads.1.value.weight': 4096,
 'sa.heads.2.key.weight': 4096,
 'sa.heads.2.query.weight': 4096,
 'sa.heads.2.value.weight': 4096,
 'sa.heads.3.key.weight': 4096,
 'sa.heads.3.query.weight': 4096,
 'sa.heads.3.value.weight': 4096,
 'ffwd.net.0.weight': 65536,
 'ffwd.net.0.bias': 512,
 'ffwd.net.2.weight': 65536,
 'ffwd.net.2.bias': 128,
 'ln1.weight': 128,
 'ln1.bias': 128,
 'ln2.weight': 128,
 'ln2.bias': 128}</code></pre>
</div>
</div>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
I fixed the Transformer diagram :D <a href="https://t.co/qWnOUjZKut">pic.twitter.com/qWnOUjZKut</a>
</p>
— Andrej Karpathy (<span class="citation" data-cites="karpathy">@karpathy</span>) <a href="https://twitter.com/karpathy/status/1658161721251602432?ref_src=twsrc%5Etfw">May 15, 2023</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><img src="https://kjablonka.com/blog/posts/building_an_llm/https:/pbs.twimg.com/media/FwL5ROUagAIrsJT?format=jpg&amp;name=medium.png" class="img-fluid"></p>
<p>With all these “tricks” and enhancements of expressivity, we can now build a full GPT.</p>
<div class="cell" data-execution_count="223">
<div class="sourceCode cell-code" id="cb66" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><span class="kw" style="color: #003B4F;">class</span> GPT(nn.Module):</span>
<span id="cb66-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, vocab_size, n_embd, block_size, n_head, n_blocks):</span>
<span id="cb66-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb66-4">        <span class="va" style="color: #111111;">self</span>.tok_emb <span class="op" style="color: #5E5E5E;">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb66-5">        <span class="va" style="color: #111111;">self</span>.pos_emb <span class="op" style="color: #5E5E5E;">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb66-6">        <span class="va" style="color: #111111;">self</span>.layers <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(<span class="op" style="color: #5E5E5E;">*</span>[Block(n_embd, block_size, n_head) <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n_blocks)])</span>
<span id="cb66-7">        <span class="va" style="color: #111111;">self</span>.head <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(n_embd, vocab_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb66-8">        <span class="va" style="color: #111111;">self</span>.block_size <span class="op" style="color: #5E5E5E;">=</span> block_size</span>
<span id="cb66-9"></span>
<span id="cb66-10">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb66-11">        B, T <span class="op" style="color: #5E5E5E;">=</span> x.shape</span>
<span id="cb66-12">        </span>
<span id="cb66-13">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.tok_emb(x) <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.pos_emb(torch.arange(T, device<span class="op" style="color: #5E5E5E;">=</span>x.device))  <span class="co" style="color: #5E5E5E;"># b,tc, batch, time - seqeuence length, embedding dimension</span></span>
<span id="cb66-14">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.layers(x)</span>
<span id="cb66-15">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.head(x)</span>
<span id="cb66-16">        <span class="cf" style="color: #003B4F;">return</span> x</span>
<span id="cb66-17"></span>
<span id="cb66-18">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, x, y):</span>
<span id="cb66-19">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb66-20">        logits <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x) <span class="co" style="color: #5E5E5E;"># (B, T, C)</span></span>
<span id="cb66-21">        B, T, C <span class="op" style="color: #5E5E5E;">=</span> logits.shape</span>
<span id="cb66-22">        logits <span class="op" style="color: #5E5E5E;">=</span> logits.view(B<span class="op" style="color: #5E5E5E;">*</span>T, C)</span>
<span id="cb66-23">        y <span class="op" style="color: #5E5E5E;">=</span> y.view(B<span class="op" style="color: #5E5E5E;">*</span>T)</span>
<span id="cb66-24">        loss <span class="op" style="color: #5E5E5E;">=</span> F.cross_entropy(logits, y)</span>
<span id="cb66-25">        <span class="cf" style="color: #003B4F;">return</span> loss</span>
<span id="cb66-26">    </span>
<span id="cb66-27"></span>
<span id="cb66-28">    <span class="kw" style="color: #003B4F;">def</span> generate(<span class="va" style="color: #111111;">self</span>, x, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>):</span>
<span id="cb66-29">        <span class="co" style="color: #5E5E5E;"># x is a tensor of shape (B, T)</span></span>
<span id="cb66-30">        <span class="co" style="color: #5E5E5E;"># we generate max_new_tokens new tokens</span></span>
<span id="cb66-31">        new_tokens <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb66-32">        <span class="cf" style="color: #003B4F;">for</span> _t <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(max_new_tokens):</span>
<span id="cb66-33">            x_ <span class="op" style="color: #5E5E5E;">=</span> x[:, <span class="op" style="color: #5E5E5E;">-</span><span class="va" style="color: #111111;">self</span>.block:]</span>
<span id="cb66-34">            logits <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.forward(x_)</span>
<span id="cb66-35">            logits <span class="op" style="color: #5E5E5E;">=</span> logits[:, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, :]</span>
<span id="cb66-36">            probs <span class="op" style="color: #5E5E5E;">=</span> F.softmax(logits, dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb66-37">            next_token <span class="op" style="color: #5E5E5E;">=</span> torch.multinomial(probs, num_samples<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb66-38">            new_tokens.append(next_token)</span>
<span id="cb66-39">            x <span class="op" style="color: #5E5E5E;">=</span> torch.cat([x, next_token], dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb66-40">        <span class="cf" style="color: #003B4F;">return</span> x</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1">gpt <span class="op" style="color: #5E5E5E;">=</span> GPT(<span class="bu" style="color: null;">len</span>(tokenizer.tokens), n_embd<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">512</span>, block_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">40</span>, n_head<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, n_blocks<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="227">
<div class="sourceCode cell-code" id="cb68" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1">get_num_parameters(gpt)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="227">
<pre><code>11623424</code></pre>
</div>
</div>
<p>That is not nothing and certainly not something I can run on my MacBook while still doing other things. When I run the training code below on a GPU (Colab is good enough), I get a validation perplexity of XX and generate SMILES like XY.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1">train_model(gpt, train_loader, valid_loader, epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1e-3</span>)</span></code></pre></div>
</div>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>We saw how to build a GPT to generate new SMILES. We generalized a simple bigram model to take into account all past tokens and not just the last one. When we take the tokens into account, we do this by using self-attention, which allows the model to learn the dependencies between tokens.</p>
<p>To further improve the model, we added multiple heads to the self-attention mechanism, which allows the model to learn different dependencies between tokens. Finally, we stacked multiple blocks of self-attention and feedforward layers to create a GPT model.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Much of this discussion (and also the way it is structured, e.g., based on the bigram) is based on the outstanding material created by <a href="https://karpathy.ai/zero-to-hero.html">Andrej Karpathy</a>. In particular, the implementation here follows <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>.</p>
<p>Other useful resources are:</p>
<ul>
<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">Annotated transformer</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Illustrated transformer</a></li>
<li><a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention! Attention?</a></li>
<li><a href="https://bbycroft.net/llm">Interactive attention visualization</a></li>
<li><a href="https://udlbook.github.io/udlbook/">Simon Prince’s book</a> and <a href="https://www.borealisai.com/research-blogs/tutorial-14-transformers-i-introduction/">blog posts</a> have very nice illustrations of the attention mechanism.</li>
</ul>


</section>

 ]]></description>
  <category>machine-learning</category>
  <category>llm</category>
  <guid>https://kjablonka.com/blog/posts/building_an_llm/index.html</guid>
  <pubDate>Wed, 01 May 2024 22:00:00 GMT</pubDate>
  <media:content url="https://pbs.twimg.com/media/FlzZE_dWIAEKnAW?format=jpg&amp;name=medium" medium="image"/>
</item>
<item>
  <title>The ‘researcher’s stuff’</title>
  <link>https://kjablonka.com/blog/posts/researchers_stuff/index.html</link>
  <description><![CDATA[ 



<p>In the hope of trying to better understand the thing I pretend to do for a living, I have been reading <a href="https://en.wikipedia.org/wiki/Isabelle_Stengers">Isabelle Stenger</a>’s <a href="https://www.wiley.com/en-gb/Another+Science+is+Possible:+A+Manifesto+for+Slow+Science+-p-9781509521814">“Another Science is Possible: A Manifesto for Slow Science”</a>. My goal is to better understand why I feel that <a href="https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/">science is seemingly less efficient</a> and why academia is a, perhaps, an increasingly “special” place to work in.</p>
<p>Early in the book, she compares the “right stuff” NASA test pilots needed to have with what she calls the “researcher’s stuff”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/researchers_stuff/dalle_right_stuff.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">DALL E’s illustration of the “right stuff” and “researcher’s stuff”</figcaption><p></p>
</figure>
</div>
<p><a href="https://en.wikipedia.org/wiki/The_Right_Stuff_(book)">In Tom Wolfe’s book the “right stuff”</a> was the “stuff” the NASA pilots who survived had — and those who died didn’t have</p>
<blockquote class="blockquote">
<p>In this fraternity, even though it was military, men were not rated by their outward rank as ensigns, lieutenants, commanders, or whatever. No, herein the world was divided into those who had it and those who did not. This quality, this it, was never named, however, nor was it talked about in any way.</p>
</blockquote>
<p>Stenger rephrases this as</p>
<blockquote class="blockquote">
<p>It is precisely this unacceptable degree of dependency that the expression hides: whatever flying coffin they were given to test, those who were killed didn’t have the right stuff.</p>
</blockquote>
<p>and links this to working conditions in academia</p>
<blockquote class="blockquote">
<p>Far from being treated as a primary resource that is now under threat, young researchers of either gender, doctoral students or postdocs, have to accept the realities of onerous working conditions and fierce competition. They are supposed to grin and bear it: the great adventure of human curiosity presented to them as children is replaced by the theme of a vocation that demands body-and-soul commitment. And this is what we accuse today’s young people of no longer accepting: compliance with the sacrifices that service to science demands.</p>
</blockquote>
<p>While there <a href="https://milan.cvitkovic.net/writing/market_failures_in_science/">is</a> <a href="https://x.com/ashleyruba_phd/status/1720105966165762269?s=20">a</a> <a href="https://rowanzellers.com/blog/rowan-job-search2/">lot</a> <a href="https://forbetterscience.com/2020/09/06/new-jacs-eic-erick-carreira-correct-your-work-ethic-immediately/">to</a> <a href="https://www.republik.ch/2021/04/15/eidgenoessische-toxische-hochschule">say</a> <a href="https://www.nature.com/articles/s41562-021-01178-6">about</a> (<a href="https://www.nature.com/articles/d41586-021-03567-3">working</a>) <a href="https://www.theatlantic.com/business/archive/2010/05/why-does-academia-treat-its-workforce-so-badly/56829/">conditions</a> <a href="https://www.theatlantic.com/science/archive/2016/11/why-would-a-poor-kid-want-to-work-in-academia/622698/">in</a> <a href="https://www.theatlantic.com/education/archive/2019/04/adjunct-professors-higher-education-thea-hunter/586168/">academia</a> and how the system in many parts failed to evolve, the link to “over objectivization”, which is perhaps very natural to many scientists, was more interesting to me. In an attempt to increase transparency and objectivity, “objective metrics” are being used to quantify how much “researcher stuff” a researcher has. However, those metrics do, of course, not work for every type of science (Stenger’s attempts to show that they stem from what she calls “fast sciences”). More importantly, however, we know from works such as the one from <a href="https://link.springer.com/book/10.1007/978-3-319-15524-1">Kenneth Stanley and Joel Lehman that “greatness cannot be planned”</a> as paths to great discoveries ofteen go via “stepping stones” we cannot anticipate and which optimization of “naiive” metrics would us not lead to.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/researchers_stuff/https:/pbs.twimg.com/media/FW3pxEkaMAAb5K4?format=jpg&amp;name=large.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">There is <a href="https://wiki.santafe.edu/images/3/34/Stanley_innovation_workshop14.pdf">empirical research that some things can be found more easily when not looking for it</a>. This could, for example, be seen in the PicBreeder experiment where participants were asked to “breed” images.</figcaption><p></p>
</figure>
</div>
<p><a href="https://www.lesswrong.com/posts/wd7qxFBF2swRscBiS/academia-as-company-hierarchy">From this point of view, viewing academia via the lense of the comic Company Hierarchy by Hugh MacLeod makes some sense.</a> In many layers of academia we have the tendency to optimize for metrics (h index, citations, …) which is in this perspective the definition of the “clueless”. [Stenger also has an interesting tangent how this might be tight to current science education. In a Kuhnian perspective of paradigms and “normal science”, we are not really taught to question different ways of thinking, but rather focus in methodological details. Questioning different schools of thinking is perhaps more natural to the social sciences.]{:.aside}</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/researchers_stuff/https:/39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/18d38a134d5c8109364d169f34a464d4d8b7ca91aaf600de.jpg/w_400.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Hugh MacLeod’s Company Hierarchy.</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>The Clueless cannot process anything that is not finite, countable and external. They can only process the legible.” Certainly this describes the behavior of faculty, literally counting lines on their CV, grubbing for citations, breathlessly calculating their h-index.</p>
</blockquote>
<p>To help science, Stenger argues that scientists should start caring more about the broader relevance of their work and not forget, what relevance means in the end: Not bibliometric metrics but rather evaluation by the community</p>
<blockquote class="blockquote">
<p>“if a scientific claim can be trusted as reliable, it is not because scientists are objective, but because the claim has been exposed to the demanding objections of competent colleagues concerned about its reliability”</p>
</blockquote>
<p>Latter might sometimes correlate with bibliometric metrics but will not always do so. Simply because we rely on many different things (software, databases, …) that are created on <a href="https://www.nature.com/articles/495300a">very different timescales</a>.</p>
<p>To me, Stenger really urges us to step out of the “ivory tower” and “appreciate the originality or the relevance of an idea but also pay attention to questions or possibilities that were not taken into account in its production, but that might become important in other circumstances”. This is also very important when we think about all the ways technologies can be misused. Stepping out of the ivory tower and taking society serious, however, probably also has to prompt us to rethink working conditions in academia.</p>
<p>In any case, I am very happy to see that <a href="https://www.convergentresearch.org/">new forms of doing science</a> <a href="https://arcinstitute.org/">are</a> <a href="https://arenabio.works/">being</a> explored, <a href="https://www.sam-rodriques.com/blog">because academia certainly is not the only and best way to do science</a>.</p>



 ]]></description>
  <category>academia</category>
  <category>metascience</category>
  <guid>https://kjablonka.com/blog/posts/researchers_stuff/index.html</guid>
  <pubDate>Sat, 23 Mar 2024 23:00:00 GMT</pubDate>
  <media:content url="https://kjablonka.com/blog/posts/researchers_stuff/dalle_right_stuff.png" medium="image" type="image/png" height="82" width="144"/>
</item>
<item>
  <title>Language I want to be more mindful of</title>
  <link>https://kjablonka.com/blog/posts/bad_language/index.html</link>
  <description><![CDATA[ 



<div class="page-columns page-full"><p>While an <a href="https://www.bridgewater.com/culture/bridgewaters-idea-meritocracy">idea meritocracy</a> might be an ideal way to run science. <a href="https://www.washingtonpost.com/news/monkey-cage/wp/2015/02/12/academia-is-not-a-meritocracy/">Academia</a> <a href="https://www.nature.com/articles/s41562-019-0735-y">is</a> <a href="https://www.pnas.org/doi/10.1073/pnas.1211286109">not</a> <a href="https://www.science.org/doi/full/10.1126/science.1196783">a</a> <a href="https://www.aaup.org/article/why-graduate-students-reject-fast-track">meritocracy</a><a href="https://www.nature.com/articles/s41550-017-0141">.</a>  Even worse, some of the language we (including myself) use might make some with great ideas feel unsafe and not welcome.</p><div class="no-row-height column-margin column-container"><span class="">Some of the metascience works of <a href="https://aaronclauset.github.io/">Aaron Clauset</a> give great evidence for that. For example, <a href="https://www.youtube.com/watch?v=gyhZ645Vh14">this talk</a>.</span></div></div>
<section id="junior-group-leader" class="level2">
<h2 class="anchored" data-anchor-id="junior-group-leader">Junior group leader</h2>
<p>In some communities, the term “junior group leader” is quite common. Why is this suboptimal? The term “junior” might suggest to some colleagues or students that the group leader has significantly less expertise or authority compared to “senior” colleagues and reinforces hierarchical structures within academia.</p>
<p>A simple title such as “Research Group Leader” without the “junior” prefix can emphasize the role rather than the perceived hierarchy or experience level.</p>
<p><em>Before:</em> “We need a junior group leader to handle the initial phase.”</p>
<p><em>After:</em> “We’re looking for an independent research leader to spearhead the initial phase.”</p>
<p>This is a special case of seniority and age being more important in some societies than skill and accomplishment.</p>
</section>
<section id="gender" class="level2">
<h2 class="anchored" data-anchor-id="gender">Gender</h2>
<p>Gender is diverse and nothing we can assume based on names, roles, or societal expectations. If we can be more proactive in communicating in a way that makes people more respected, we can do so.</p>
<p><em>Before:</em> “Each student must submit his or her proposal by next week.”</p>
<p><em>After:</em> “All students must submit their proposals by next week.”</p>
<p>In academia we can also be more inclusive by being mindful of how we address people. Instead of using Mr or Ms we can simply address them using gender-neutral <em>earned</em> titles.</p>
<p><em>Before:</em> “Dear Ms.&nbsp;Curie”</p>
<p><em>After:</em> “Dear Dr.&nbsp;Curie”</p>
</section>
<section id="speaking-of-students-as-commodities" class="level2">
<h2 class="anchored" data-anchor-id="speaking-of-students-as-commodities">Speaking of students as commodities</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/bad_language/http:/www.phdcomics.com/comics/archive/phd012609s.gif" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Cartoon illustrating the commoditization of students.</figcaption><p></p>
</figure>
</div>
<p>As team leader, one easily slips into language that strips students of their human nature and makes them seem like a commodity for the production of papers. However, it is important to realize that we all have been a <a href="https://www.lesswrong.com/posts/wd7qxFBF2swRscBiS/academia-as-company-hierarchy">“productive student”</a> (or a less productive one) at points of our career.</p>
<p><em>Before:</em> “We need to put more students on this to increase our output.”</p>
<p><em>After:</em> “Let’s involve more team members to bring diverse perspectives and enrich our project.”</p>
</section>
<section id="authorship-lists" class="level2">
<h2 class="anchored" data-anchor-id="authorship-lists">Authorship lists</h2>
<p>Authorship is still the currency of academia. We currently indicate the “relevance” of each other by their position on the list of others on a paper. However, contributions are very diverse and cannot be easily rank-ordered (there are many dimensions and introducing a <a href="https://mathworld.wolfram.com/TotallyOrderedSet.html">total order</a> would require us to introduce some weighting of the different dimensions).</p>
<p><em>Before:</em> Listing authors strictly by seniority, regardless of contributions.</p>
<p><em>After:</em> Using contributorship statements that detail each author’s role, such as “A.B. designed the study and wrote the manuscript. C.D. conducted the experiments and analyzed the data.”</p>


</section>

 ]]></description>
  <category>academia</category>
  <category>communication</category>
  <guid>https://kjablonka.com/blog/posts/bad_language/index.html</guid>
  <pubDate>Fri, 01 Mar 2024 23:00:00 GMT</pubDate>
  <media:content url="http://www.phdcomics.com/comics/archive/phd012609s.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Multiple instances learning</title>
  <link>https://kjablonka.com/blog/posts/mil/index.html</link>
  <description><![CDATA[ 



<p>Molecules or materials are dynamic. At realistic temperatures, there will always be an ensemble of different conformers. In addition, we typically do not deal with pure materials but more commonly with blends for which the exact structure is not known.</p>
<p>Multiple instances learning (MIL) is a framework that allows us to make predictions for such systems. For example, by thinking of molecules as <em>bags</em> of conformers or materials as <em>bags</em> of components of a blend.</p>
<p>Often, practioners already use without explicitly naming it. An overview over applications in chemistry can be found in <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/wcms.1698">Zankov et al.</a></p>
<section id="the-idea-behind-multiple-instances-learning" class="level2">
<h2 class="anchored" data-anchor-id="the-idea-behind-multiple-instances-learning">The idea behind multiple instances learning</h2>
<p>At its core, MIL is a variant of supervised learning that handles data grouped into bags, each containing multiple instances. In the context of chemical prediction, a “bag” might represent a single chemical compound, and the “instances” within could be different conformations, representations, or features of that compound. The distinctive aspect of MIL is that it assigns labels to bags, not to the individual instances they contain, making it particularly suited to scenarios where precise instance-level labels are hard to obtain or define.</p>
<p>It was formalized 1997 by a team around <a href="https://scholar.google.com/citations?hl=en&amp;user=09kJn28AAAAJ">Thomas G. Dietterich</a> <a href="https://www.sciencedirect.com/science/article/pii/S0004370296000343">with the goal of better drug-activity predictions</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/mil/mil_overview.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Overview of multiple instances learning. A bag (e.g.&nbsp;molecule) consists of multiple instances (e.g.&nbsp;conformers or tautomers). The goal is to make predictions for each bag.</figcaption><p></p>
</figure>
</div>
</section>
<section id="approaches-to-mil" class="level2">
<h2 class="anchored" data-anchor-id="approaches-to-mil">Approaches to MIL</h2>
<p>There are different ways to perform MIL: At the instance-level or the bag-level</p>
<section id="instance-level-mil" class="level3">
<h3 class="anchored" data-anchor-id="instance-level-mil">Instance-level MIL</h3>
<p>The perhaps conceptually simplest way to perform MIL is to make a prediction for each instance and then aggregate the predictions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/mil/mil_instance.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">One approach to MIL is to make a prediction for each instance and to then aggregate those predictions.</figcaption><p></p>
</figure>
</div>
<p>Conceptually, this is quite similar to Behler-Parinello Neural Networks. Here, we decompose a target, such as the energy, into atomic contributions and then make predictions for atomic energies and then add those up.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/mil/behler_parinello.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Behler-Parinello style models can be thought of instance-level MIL. We predict energies for each atom (instance) and then sum them up (aggregation) to obtain energies for the entire molecule (bag).</figcaption><p></p>
</figure>
</div>
</section>
<section id="bag-level-mil" class="level3">
<h3 class="anchored" data-anchor-id="bag-level-mil">Bag-level MIL</h3>
<p>Alternatively, one might obtain a representation for each instance and then make predictions based on aggregated representations. Note that this is not different from what we typically do in a graph-neural network: We obtain a representation for each atom using, for example, graph convolutions, then aggregate those (e.g.&nbsp;by taking the mean) abnd then perform the prediction over the full molecule (the bag). Also the fingerprint averaging methods for copolymers or polymer blends proposed by <a href="https://arxiv.org/pdf/2303.12938.pdf">Shukla et al.</a> can be seen as special case of MIL.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/mil/mil_bag.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">One can perform MIL by using representations for each instance in a learning algorithm. The simplest approach might be to average representations and to then feed them into a feedforward neural network.</figcaption><p></p>
</figure>
</div>
<p>If we use a more learnable pooling mechanism (e.g.&nbsp;attention-based), we can also attempt to find out what the most important instances are. This is known as key-instance detection.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/mil/attention_weighted.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Attention weighted aggregation might be used to identify key instances by identifying the largest attention weights</figcaption><p></p>
</figure>
</div>
<section id="specialized-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="specialized-algorithms">Specialized algorithms</h4>
<section id="set-comparisons-based" class="level5">
<h5 class="anchored" data-anchor-id="set-comparisons-based">Set comparisons based</h5>
<p>Solving the MIL problem boils down to comparing sets. And there are various similarity measures for comparing set, which can then be implemented in distance-based algorithms such as SVM or kNN.</p>
<p>A common metric is the Haussdorff distance. In this metric</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ad_%7B%5Ctext%20%7BHausdorff%20%7D%7D%5Cleft(B_1,%20B_2%5Cright)=%5Cmax%20%5Cleft(%5Coperatorname%20%7Bmax%20%7D%20_%20%7B%20b%20_%20%7B%20i%20%7D%20%5Cin%20B%20_%20%7B%201%20%7D%20%7D%20%5Cleft(%5Cmin%20_%7Bb_j%20%5Cin%20B_2%7D%5Cleft(d%5Cleft(b_i,%20b_j%5Cright)%5Cright),%20%5Cmax%20_%7Bb_i%20%5Cin%20B_2%7D%5Cleft(%5Cmin%20_%7Bb_j%20%5Cin%20B_1%7D%5Cleft(d%5Cleft(b_i,%20b_j%5Cright)%5Cright)%5Cright)%5Cright.%5Cright.%0A"> where <img src="https://latex.codecogs.com/png.latex?d"> is a distancve over the feature space of an instance <img src="https://latex.codecogs.com/png.latex?b"> in a bag <img src="https://latex.codecogs.com/png.latex?B">. Essentially, the Haussdorff distance is the distance of the point from one set that is furthest away from any point in the other set, considering both directions. This ensures that the Hausdorff Distance captures the worst-case scenario — the greatest of all the distances from a point in one set to the closest point in the other set.</p>
</section>
</section>
<section id="diettrichs-original-algorithm-axis-parallel-rectangles-aprs" class="level4">
<h4 class="anchored" data-anchor-id="diettrichs-original-algorithm-axis-parallel-rectangles-aprs">Diettrich’s original algorithm: Axis Parallel Rectangles (APRS)</h4>
<p>The idea is to learn a “concept” in feature space as axis-parallel rectangle $$$ in which there is - at least one instance from each positive example - exclude all instances from negative examples</p>
<p>the prediction is then positive if a new <img src="https://latex.codecogs.com/png.latex?x"> is in the rectangle</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x,%20R)%20=%20%5Cbegin%7Bcases%7D%0A1%20&amp;%20x%20%5Cin%20R%20%5C%5C%0A0%20&amp;%20%5Ctext%7Belse%7D%0A%5Cend%7Bcases%7D%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://kjablonka.com/blog/posts/mil/APR.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Illustration of the axis-parallel rectangle approach. The filled shapes represent instances, the grey ellipses bags. The organe rectangle is the APR. Blue indicates negative instances, red ones postive ones. Each bag with at least one positive instance is labled as positive.</figcaption><p></p>
</figure>
</div>
<p>In the original article there are different algorithms for growing those rectangles. One rough implementation might look as follows:</p>
<ol type="1">
<li><em>Initialization</em>: Choose a seed positive instance to start constructing the APR.</li>
<li><em>Grow APR</em>: find the smallest APR that covers at least one instance of every positive molecule (i.e.&nbsp;bag). One can implement it greedly to add until there is at least one instance from every positive molecule. For addition, we choose the molecule that would lead to the smallest growth of the APR. This is run over a set of possible features.</li>
<li><em>Select Discriminating Features</em>
<ul>
<li>Evaluate each feature for its ability to exclude negative instances while including positive ones.</li>
<li>Select features that provide the best discrimination between positive and negative instances.</li>
</ul></li>
<li><em>Expand APR</em>: The APR with the steps above is often too tight: “It is typically so tight that it excludes most positive instances in the test set”. Those, one can
<ul>
<li>Apply kernel density estimation on each selected feature to determine the optimal expansion of the APR bounds.</li>
<li>Adjust bounds to ensure a high probability of covering new positive instances and excluding negatives.</li>
</ul></li>
<li><em>Iterate</em>: Alternate between selecting discriminating features and expanding the APR until the process converges on a stable set of features and APR bounds.</li>
</ol>
</section>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><a href="Ihttps://www.uco.es/users/sventura/talk/slides/2015-multiple-instance-learning.pdf">Lecture notes on MIL by Sebastián Ventura</a></li>
<li><a href="https://www.dbs.ifi.lmu.de/Lehre/KDD_II/WS1415/skript/KDD2-4-VarietyData2.pdf">Lecture notes by the Database Systems Group at LMU</a></li>
</ol>


<!-- -->

</section>

 ]]></description>
  <category>machine-learning</category>
  <guid>https://kjablonka.com/blog/posts/mil/index.html</guid>
  <pubDate>Fri, 01 Mar 2024 23:00:00 GMT</pubDate>
  <media:content url="https://kjablonka.com/blog/posts/mil/mil_overview.png" medium="image" type="image/png" height="120" width="144"/>
</item>
<item>
  <title>Developing an intuition for backpropagation</title>
  <link>https://kjablonka.com/blog/posts/backprop/index.html</link>
  <description><![CDATA[ 



<section id="setting-weights-in-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="setting-weights-in-neural-networks">Setting weights in neural networks</h2>
<p>When we build neural networks, we tune weights to ensure that the outputs are close to what we want them to be.</p>
<p>The power of deep learning is that having many layers of weights allows us to learn very complex functions (i.e.&nbsp;mappings from input to output).</p>
<p>Here, we want to understand how to systematically tune the weights to achieve this.</p>
 <style>
        .flex-container {
            display: flex;
            justify-content: center;
            align-items: start; /* Adjust this as needed */
        }
        .slider-container {
            flex: 2;
            padding: 1px;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }
        .visualization-container {
            flex: 2; /* Gives the visualization more room */
            padding: 1px;
        }
        .slider-label {
            margin-bottom: 10px;
            color: white;
        }
    </style>
  
    <meta charset="UTF-8">
    <title>Neural Network Visualization</title>
    <script src="https://d3js.org/d3.v6.min.js"></script>
    <style>
      .slider-label {
        display: block;
        margin-top: 10px;
      }
      #outputLabel {
        margin-top: 10px;
      }

    </style>
  
  
    
    <div class="flex-container">
        <div class="slider-container">
    <div class="slider-label">
      Input:
      <input type="range" min="0" max="1" step="0.01" value="0.5" id="inputSlider">
    </div>
    <div class="slider-label">
      Weight 1-1:
      <input type="range" min="-1" max="1" step="0.01" value="0.5" id="weight1_1Slider">
    </div>
    <div class="slider-label">
      Weight 1-2:
      <input type="range" min="-1" max="1" step="0.01" value="0.5" id="weight1_2Slider">
    </div>
    <div class="slider-label">
      Weight 2-1:
      <input type="range" min="-1" max="1" step="0.01" value="0.5" id="weight2_1Slider">
    </div>
    <div class="slider-label">
      Weight 2-2:
      <input type="range" min="-1" max="1" step="0.01" value="0.5" id="weight2_2Slider">
    </div>
    <div class="slider-label">
      Target Output:
      <input type="range" min="0" max="1" step="0.01" value="0.5" id="targetOutputSlider">
    </div>
    <bf><div id="outputLabel">Loss: 0.0000</div></bf>
    </div>
    <div class="visualization-container"></div>
    <svg id="networkVisualization" width="600" height="400"></svg>
    </div>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        function relu(x) {
          return Math.max(0, x);
        }

        function forwardPass(inputs, weights1, weights2) {
          let hiddenLayerInput = [inputs * weights1[0], inputs * weights1[1]];
          let hiddenLayerOutput = hiddenLayerInput.map(relu);
          let outputLayerInput =
            hiddenLayerOutput[0] * weights2[0] +
            hiddenLayerOutput[1] * weights2[1];
          return outputLayerInput;
        }

        function computeMSELoss(predicted, target) {
          return Math.pow(predicted - target, 2);
        }
        const colorScale = d3.scaleLinear()
            .domain([-1, 0, 1])
            .range(["blue","red"]);


         function drawNetwork(selector, weights1, weights2, inputs, hiddenActivations, outputActivation) {
        const svg = d3.select(selector);
        svg.selectAll("*").remove(); // Clear previous drawing

        const width = +svg.attr("width");
        const height = +svg.attr("height");

        // Define neuron positions
        const positions = {
            input: [{x: width * 0.2, y: height / 2, value: inputs}],
            hidden: [
                {x: width * 0.5, y: height * 0.3, value: hiddenActivations[0]},
                {x: width * 0.5, y: height * 0.7, value: hiddenActivations[1]}
            ],
            output: [{x: width * 0.8, y: height / 2, value: outputActivation[0]}]
        };

        // Draw connections and labels for weights
        positions.input.forEach((inputPos, i) => {
            positions.hidden.forEach((hiddenPos, j) => {
                svg.append("line")
                    .attr("x1", inputPos.x)
                    .attr("y1", inputPos.y)
                    .attr("x2", hiddenPos.x)
                    .attr("y2", hiddenPos.y)
                    .attr("stroke", colorScale(weights1[j]))
                    .attr("stroke-width", Math.abs(weights1[j]) * 2 + 1);

                // Label for weight
                svg.append("text")
                    .attr("x", (inputPos.x + hiddenPos.x) / 2 -10)
                    .attr("y", (inputPos.y + hiddenPos.y) / 2 - (j === 0 ? 20 : -40))
                    .attr("dy", "-5")
                    .attr("text-anchor", "middle")
                    .attr("fill", "white") 
                    .text(`weight 1-${j+1}: ${weights1[j].toFixed(2)}`);
            });
        });

        positions.hidden.forEach((hiddenPos, i) => {
            svg.append("line")
                .attr("x1", hiddenPos.x)
                .attr("y1", hiddenPos.y)
                .attr("x2", positions.output[0].x)
                .attr("y2", positions.output[0].y)
                .attr("stroke", colorScale(weights2[i]))
                .attr("stroke-width", Math.abs(weights2[i]) * 2 + 1);

            // Label for weight
            svg.append("text")
                .attr("x", (hiddenPos.x + positions.output[0].x) / 2 + 10)
                .attr("y", (hiddenPos.y + positions.output[0].y) / 2 - (i === 0 ? 20 : -40))
                .attr("dy", "-5")
                .attr("text-anchor", "middle")
                .attr("fill", "white")
                .text(`weight 2-${i+1}: ${weights2[i].toFixed(2)}`);
        });

        // Draw neurons and labels for activations
        [...positions.input, ...positions.hidden, ...positions.output].forEach(pos => {
            svg.append("circle")
                .attr("cx", pos.x)
                .attr("cy", pos.y)
                .attr("r", 20)
                .attr("fill", colorScale(pos.value))
                .attr("stroke", "black");

            // Label for neuron value
            svg.append("text")
                .attr("x", pos.x)
                .attr("y", pos.y)
                .attr("dy", "5")
                .attr("text-anchor", "middle")
                .attr("fill", "white")
                .text(pos.value.toFixed(2));
        });
    }

        function updateVisualization() {
          let inputs = parseFloat(document.getElementById("inputSlider").value);
          let weights1 = [
            parseFloat(document.getElementById("weight1_1Slider").value),
            parseFloat(document.getElementById("weight1_2Slider").value),
          ];
          let weights2 = [
            parseFloat(document.getElementById("weight2_1Slider").value),
            parseFloat(document.getElementById("weight2_2Slider").value),
          ];
          let targetOutput = parseFloat(
            document.getElementById("targetOutputSlider").value
          );

          let output = forwardPass(inputs, weights1, weights2);
          let loss = computeMSELoss(output, targetOutput);

          document.getElementById(
            "outputLabel"
          ).innerText = `Loss: ${loss.toFixed(
            4
          )}`;

          drawNetwork(
            "#networkVisualization",
            weights1,
            weights2,
            inputs,
            weights1.map(relu),
            [output]
          );
        }

        document.querySelectorAll("input[type=range]").forEach((slider) => {
          slider.addEventListener("input", updateVisualization);
        });

        updateVisualization(); // Initial visualization
      });
    </script>
  
<p>When we think of the tiny neural network in the widget above one might think of many different ways for optimizing the weights (line strenghts) of this model.</p>
<section id="option-1-randomly-choose-weights" class="level3">
<h3 class="anchored" data-anchor-id="option-1-randomly-choose-weights">Option 1: Randomly choose weights</h3>
<p>One option you might try is to randomly try different weight values to then find one that minimizes the difference between ground truth and prediction (i.e., minimizes the loss). While we might be lucky for this toy example, we can imagine that it might take a long time until we guessed all the weights in a billion-parameter model (e.g.&nbsp;GPT-3) correctly.</p>
<p>Using a strategy like a grid search (in which you loop over a range of possible weight values for all weights) will also only work for small models (think of the <img src="https://latex.codecogs.com/png.latex?100%5E4"> combinations you would have to just try of 100 trial values for 4 weights).</p>
</section>
<section id="option-2-using-numerical-gradients" class="level3">
<h3 class="anchored" data-anchor-id="option-2-using-numerical-gradients">Option 2: Using numerical gradients</h3>
<p>When we think of our neural network, the loss forms a landscape, that can be very complex. In our simple example below, it looks as follows:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="kw" style="color: #003B4F;">def</span> relu(x):</span>
<span id="cb1-5">    <span class="cf" style="color: #003B4F;">return</span> np.maximum(<span class="dv" style="color: #AD0000;">0</span>, x)</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;">def</span> linear(x):</span>
<span id="cb1-8">    <span class="cf" style="color: #003B4F;">return</span> x</span>
<span id="cb1-9"></span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="kw" style="color: #003B4F;">def</span> forward_pass(inputs, weights1, weights2, record_activation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb1-12">    hidden_layer_input <span class="op" style="color: #5E5E5E;">=</span> np.dot(inputs, weights1)</span>
<span id="cb1-13">    hidden_layer_output <span class="op" style="color: #5E5E5E;">=</span> relu(hidden_layer_input)</span>
<span id="cb1-14">    output_layer_input <span class="op" style="color: #5E5E5E;">=</span> np.dot(hidden_layer_output, weights2)</span>
<span id="cb1-15">    output <span class="op" style="color: #5E5E5E;">=</span> linear(output_layer_input)</span>
<span id="cb1-16">    <span class="cf" style="color: #003B4F;">if</span> record_activation:</span>
<span id="cb1-17">        <span class="cf" style="color: #003B4F;">return</span> output, hidden_layer_output</span>
<span id="cb1-18">    <span class="cf" style="color: #003B4F;">return</span> output</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="kw" style="color: #003B4F;">def</span> compute_mse_loss(predicted, target):</span>
<span id="cb1-21">    loss <span class="op" style="color: #5E5E5E;">=</span>  np.mean(np.square(predicted <span class="op" style="color: #5E5E5E;">-</span> target))</span>
<span id="cb1-22">    <span class="cf" style="color: #003B4F;">return</span> loss</span>
<span id="cb1-23"></span>
<span id="cb1-24"><span class="co" style="color: #5E5E5E;"># Simplify the scenario for clear visualization</span></span>
<span id="cb1-25"><span class="co" style="color: #5E5E5E;"># Set the target output and input</span></span>
<span id="cb1-26">target <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.9</span></span>
<span id="cb1-27">input_val <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.8</span>  <span class="co" style="color: #5E5E5E;"># A simple input value to keep the forward pass straightforward</span></span>
<span id="cb1-28"></span>
<span id="cb1-29"><span class="co" style="color: #5E5E5E;"># Define a range for weight updates that centers around an expected minimum</span></span>
<span id="cb1-30">weight_range <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">3.5</span>  <span class="co" style="color: #5E5E5E;"># Explore weights within [-2, 2] for both weights</span></span>
<span id="cb1-31">num_steps <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100</span>  <span class="co" style="color: #5E5E5E;"># Increase the number of steps for finer resolution</span></span>
<span id="cb1-32">step_size <span class="op" style="color: #5E5E5E;">=</span> weight_range <span class="op" style="color: #5E5E5E;">/</span> num_steps</span>
<span id="cb1-33"></span>
<span id="cb1-34">weight1_1_range <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="dv" style="color: #AD0000;">0</span>, weight_range, <span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">*</span> num_steps <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>)  <span class="co" style="color: #5E5E5E;"># Start from 0 to weight_range</span></span>
<span id="cb1-35">weight2_1_range <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="op" style="color: #5E5E5E;">-</span>weight_range, weight_range, <span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">*</span> num_steps <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>)  <span class="co" style="color: #5E5E5E;"># Keep full range for weight2_1</span></span>
<span id="cb1-36">weight1_1_vals, weight2_1_vals <span class="op" style="color: #5E5E5E;">=</span> np.meshgrid(weight1_1_range, weight2_1_range)</span>
<span id="cb1-37"></span>
<span id="cb1-38">fixed_weight1_2 <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.2</span></span>
<span id="cb1-39">fixed_weight2_2 <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.8</span></span>
<span id="cb1-40">losses <span class="op" style="color: #5E5E5E;">=</span> np.zeros((<span class="bu" style="color: null;">len</span>(weight1_1_range), <span class="bu" style="color: null;">len</span>(weight2_1_range)))</span>
<span id="cb1-41"><span class="co" style="color: #5E5E5E;"># Recalculate the losses with the updated range</span></span>
<span id="cb1-42"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(weight1_1_range)):</span>
<span id="cb1-43">    <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(weight2_1_range)):</span>
<span id="cb1-44">        current_weights1 <span class="op" style="color: #5E5E5E;">=</span> np.array([weight1_1_vals[i, j], fixed_weight1_2])</span>
<span id="cb1-45">        current_weights2 <span class="op" style="color: #5E5E5E;">=</span> np.array([weight2_1_vals[i, j], fixed_weight2_2])</span>
<span id="cb1-46">        output <span class="op" style="color: #5E5E5E;">=</span> forward_pass(np.array([[input_val]]), current_weights1.reshape(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>), current_weights2.reshape(<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb1-47">        losses[i, j] <span class="op" style="color: #5E5E5E;">=</span> compute_mse_loss(output, np.array([[target]]))</span>
<span id="cb1-48"></span>
<span id="cb1-49"><span class="co" style="color: #5E5E5E;"># Create a 2D contour plot to visualize the loss landscape</span></span>
<span id="cb1-50">plt.figure()</span>
<span id="cb1-51">heatmap <span class="op" style="color: #5E5E5E;">=</span> plt.contourf(weight1_1_vals, weight2_1_vals, losses, levels<span class="op" style="color: #5E5E5E;">=</span>np.linspace(losses.<span class="bu" style="color: null;">min</span>(), losses.<span class="bu" style="color: null;">max</span>(), <span class="dv" style="color: #AD0000;">50</span>), cmap<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'viridis'</span>)</span>
<span id="cb1-52">plt.colorbar()</span>
<span id="cb1-53">plt.title(<span class="st" style="color: #20794D;">'Loss Landscape'</span>)</span>
<span id="cb1-54">plt.xlabel(<span class="st" style="color: #20794D;">'$w_1^1$ values'</span>)</span>
<span id="cb1-55">plt.ylabel(<span class="st" style="color: #20794D;">'$w_2^1$ values'</span>)</span>
<span id="cb1-56">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://kjablonka.com/blog/posts/backprop/index_files/figure-html/cell-2-output-1.png" width="580" height="454"></p>
</div>
</div>
<p>To create this plot, we keep two weights fixed, vary two others and then analyze how the loss looks like. We see that there is a clear structure that might remind us of a hilly landscape.</p>
<p>With the random search we have been randomly jumping around on this landscape. But seeing this image, we might also decide that we want to follow the path downhill; ultimately, our goal is to find the valley (the lowest loss). That is, the best value to try next should not be a random one but one downhill from where we are now.</p>
<p>This direction (“downhill”) is the slope of our hilly landscape, i.e.&nbsp;the gradient.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cmathrm%7Bd%7Df(x)%7D%7B%5Cmathrm%7Bd%7Dx%7D%20=%20%5Clim_%7Bh%5Cto0%7D%20%5Cfrac%7Bf(x+h)%20-%20f(x)%7D%7Bh%7D%0A"></p>
<p>Based on the formula above, we might decide to compute a gradient numerically using <a href="https://en.wikipedia.org/wiki/Finite_difference">finite differences</a>.</p>
<p>The problem is that we need to perform <em>many evaluations</em> of the loss to make it work (one per weight, which can be a lot for current frontier models). In addition, we add up errors because <img src="https://latex.codecogs.com/png.latex?h"> will be different from <img src="https://latex.codecogs.com/png.latex?0"> (truncation error) and because be have to work with machine precision and hence add rounding errors.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we compute numerical gradients, we have two main sources of error. One stems from the fact that <img src="https://latex.codecogs.com/png.latex?h"> in the euqation above is not exactly 0. This is known as truncation error. On the other hand, the finite difference equation leads to numberical problems (rounding errors) as two almost identical numbers are substracted and then divided by a very small number.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb2-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="co" style="color: #5E5E5E;"># Define the function and its exact derivative</span></span>
<span id="cb2-5"><span class="kw" style="color: #003B4F;">def</span> f(x):</span>
<span id="cb2-6">    <span class="cf" style="color: #003B4F;">return</span> x<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">3</span></span>
<span id="cb2-7"></span>
<span id="cb2-8"><span class="kw" style="color: #003B4F;">def</span> df_exact(x):</span>
<span id="cb2-9">    <span class="cf" style="color: #003B4F;">return</span> <span class="dv" style="color: #AD0000;">3</span><span class="op" style="color: #5E5E5E;">*</span>x<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb2-10"></span>
<span id="cb2-11"><span class="co" style="color: #5E5E5E;"># Point at which to evaluate the derivative</span></span>
<span id="cb2-12">x <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb2-13"></span>
<span id="cb2-14"><span class="co" style="color: #5E5E5E;"># Generate a range of h values (logarithmically spaced to cover small to larger values)</span></span>
<span id="cb2-15">h_values <span class="op" style="color: #5E5E5E;">=</span> np.logspace(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">400</span>)</span>
<span id="cb2-16">numerical_derivatives <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="co" style="color: #5E5E5E;"># Calculate numerical derivative using forward difference for each h</span></span>
<span id="cb2-19"><span class="cf" style="color: #003B4F;">for</span> h <span class="kw" style="color: #003B4F;">in</span> h_values:</span>
<span id="cb2-20">    numerical_derivative <span class="op" style="color: #5E5E5E;">=</span> (f(x<span class="op" style="color: #5E5E5E;">+</span>h) <span class="op" style="color: #5E5E5E;">-</span> f(x)) <span class="op" style="color: #5E5E5E;">/</span> h</span>
<span id="cb2-21">    numerical_derivatives.append(numerical_derivative)</span>
<span id="cb2-22"></span>
<span id="cb2-23"><span class="co" style="color: #5E5E5E;"># Calculate exact derivative</span></span>
<span id="cb2-24">exact_derivative <span class="op" style="color: #5E5E5E;">=</span> df_exact(x)</span>
<span id="cb2-25"></span>
<span id="cb2-26"><span class="co" style="color: #5E5E5E;"># Calculate errors</span></span>
<span id="cb2-27">errors <span class="op" style="color: #5E5E5E;">=</span> np.<span class="bu" style="color: null;">abs</span>(exact_derivative <span class="op" style="color: #5E5E5E;">-</span> np.array(numerical_derivatives))</span>
<span id="cb2-28"></span>
<span id="cb2-29"><span class="co" style="color: #5E5E5E;"># Plotting</span></span>
<span id="cb2-30">plt.figure()</span>
<span id="cb2-31">plt.loglog(h_values, errors, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'Absolute Error'</span>, marker<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'o'</span>, linestyle<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'-'</span>, markersize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, markevery<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>)</span>
<span id="cb2-32">plt.xlabel(<span class="st" style="color: #20794D;">'Step size $h$'</span>)</span>
<span id="cb2-33">plt.ylabel(<span class="st" style="color: #20794D;">'Absolute Error'</span>)</span>
<span id="cb2-34">plt.title(<span class="st" style="color: #20794D;">'Error in Numerical Derivative of $x^3$'</span>)</span>
<span id="cb2-35">plt.legend()</span>
<span id="cb2-36">plt.grid(<span class="va" style="color: #111111;">True</span>, which<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"both"</span>, linestyle<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'--'</span>)</span>
<span id="cb2-37">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://kjablonka.com/blog/posts/backprop/index_files/figure-html/cell-3-output-1.png" width="599" height="454"></p>
</div>
</div>
</div>
</div>
</section>
<section id="option-3-analytical-gradients" class="level3">
<h3 class="anchored" data-anchor-id="option-3-analytical-gradients">Option 3: Analytical gradients</h3>
<p>Obviously, we could save many evaluations when we could write down the derviates for a given functions. However, for our neural networks we cannot do this by hand.</p>
<p>The question is thus how we <em>efficiently</em> compute the gradient of function such as a neural network.</p>
</section>
</section>
<section id="evaluating-analytical-gradients-for-any-function-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-analytical-gradients-for-any-function-backpropagation">Evaluating analytical gradients for any function: Backpropagation</h2>
<section id="calculus-101-rules-for-computing-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="calculus-101-rules-for-computing-derivatives">Calculus 101: Rules for computing derivatives</h3>
<p>Let’s assume</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x,y)%20=%20xy%0A"></p>
<p>then the <em>partial derivates</em> are</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20=%20y%20%5Cquad%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D%20=%20x%0A"></p>
<p>An important rule for differentiation we will need to apply frequently, as it focusses on function composition, is the chain rule</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(g(f(x)))%5E%7B%5Cprime%7D=(g%20%5Ccirc%20f)%5E%7B%5Cprime%7D(x)=g%5E%7B%5Cprime%7D(f(x))%20f%5E%7B%5Cprime%7D(x)%0A"></p>
<p>with <img src="https://latex.codecogs.com/png.latex?g%20%5Ccirc%20f"> being function composition <img src="https://latex.codecogs.com/png.latex?x%20%5Cto%20f(x)%20%5Cto%20g(f(x))">.</p>
<p>In the multivariate case, we would write</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cmathrm%7Bd%7D%7D%7B%5Cmathrm%7Bd%7D%20t%7D%20f(x(t),%20y(t))=%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7D%20x%7D%7B%5Cmathrm%7B~d%7D%20t%7D+%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7D%20y%7D%7B%5Cmathrm%7B~d%7D%20t%7D.%0A"></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Intuitive understanding of chain rule
</div>
</div>
<div class="callout-body-container callout-body">
<p>How do you intuitively understand that? Let’s borrow from <a href="https://ia802808.us.archive.org/7/items/GeorgeSimmonsCalculusWithAnalyticGeometry1996McGrawHillScienceEngineeringMath/George%20Simmons%20-%20Calculus%20With%20Analytic%20Geometry%20%281996%2C%20McGraw-Hill%20Science_Engineering_Math%29.pdf">George F. Simmons</a>:</p>
<blockquote class="blockquote">
<p>If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.</p>
</blockquote>
<p>With</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x"> the position of the car</li>
<li><img src="https://latex.codecogs.com/png.latex?y"> the position of the bicycle</li>
<li><img src="https://latex.codecogs.com/png.latex?z"> the position of the walking man</li>
</ul>
<p>The rate of change in relative positions is given by terms like <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cmathrm%7Bd%7Dx%7D%7B%5Cmathrm%7Bd%7Dy%7D">, which gives us the change in relative position of bicycle and car. It we now aim to compute the rate of change of relative position of car to the walking man, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cmathrm%7Bd%7Dx%7D%7B%5Cmathrm%7Bd%7Dz%7D">, we find</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cmathrm%7Bd%7Dx%7D%7B%5Cmathrm%7Bd%7Dx%7D%20=%20%5Cfrac%7B%5Cmathrm%7Bd%7Dx%7D%7B%5Cmathrm%7Bd%7Dy%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Dy%7D%7B%5Cmathrm%7Bd%7Dz%7D%20=%20%5Cunderbrace%7B2%7D_%7B%5Ctext%7Bcar%20twice%20as%20fast%20as%20bicycle%7D%7D%20%5Ccdot%20%5Cunderbrace%7B4%7D_%7B%5Ctext%7Bbicycle%20is%20four%20times%20as%20fast%20as%20walking%20man%7D%7D%20=%208%0A"></p>
</div>
</div>
</section>
<section id="computing-derivatives-as-in-calculus-101" class="level3">
<h3 class="anchored" data-anchor-id="computing-derivatives-as-in-calculus-101">Computing derivatives as in calculus 101</h3>
<p>In neural networks, we nest functions. That is, will end up differentiating compound expression of the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%7B%5Cdisplaystyle%20h(x)=f(g(x))%7D%0A"></p>
<p>For instance, you might look at a simple regularized logistic regression:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%20=%20%5Cfrac%7B1%7D%7B2%7D%5Cleft(%5Csigma(wx%20+b)%20-t%20%5Cright)%5E2%20+%20%5Cfrac%7B%5Clambda%7D%7B2%7D%20w%5E2,%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is some activation function (e.g.&nbsp;the sigmoid).</p>
<p>If we now want to know what the influence of the weight <img src="https://latex.codecogs.com/png.latex?w"> is, we can differentiate the loss with respect to <img src="https://latex.codecogs.com/png.latex?w">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20w%7D%20%5Cleft%5B%5Cfrac%7B1%7D%7B2%7D%5Cleft(%5Csigma(wx%20+b)%20-t%20%5Cright)%5E2%20+%20%5Cfrac%7B%5Clambda%7D%7B2%7D%20w%5E2%20%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20w%7D%20%5Cleft(%5Csigma(wx%20+b)%20-t%20%5Cright)%5E2%20+%20%5Cfrac%7B%5Clambda%7D%7B2%7D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20w%7D%20w%5E2%20%5C%5C%0A&amp;=%20%5Cleft(%5Csigma(wx+b)%20-%20t%5Cright)%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20w%7D%5Cleft(%5Csigma(wx+b)-t%5Cright)%20+%20%5Clambda%20w%20%5C%5C%0A&amp;=%20%5Cleft(%5Csigma(wx+b)%20-%20t%5Cright)%5Csigma'(wx%20+b)%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20w%7D(wx+b)%20+%20%5Clambda%20w%20%5C%5C%0A&amp;=%20%5Cleft(%5Csigma(wx+b)%20-%20t%5Cright)%5Csigma'(wx%20+b)x%20+%20%5Clambda%20w%0A%5Cend%7Balign%7D%0A"></p>
<p>Puh! That was a lot of copying and pasting and quite error prone. And it might be quite costly to just directly evaluate such an expression (we might end up with an exponentially large expression, “expression swell”).</p>
<p>There must be a better way.</p>
</section>
<section id="making-it-efficient-with-caching" class="level3">
<h3 class="anchored" data-anchor-id="making-it-efficient-with-caching">Making it efficient with caching</h3>
<p>One thing that we can observe is that we need to do the same computation several times. For instance, <img src="https://latex.codecogs.com/png.latex?wx%20+b"> is evaluated two times. We code trade off space and time complexity by caching this using an intermediate variable.</p>
<p>If we do this systematically, we can very efficiently compute gradients – in a form that is symmetric to the computation of the function itself (and those with basically the same cost).</p>
<section id="general-computation-with-intermediate-values" class="level4">
<h4 class="anchored" data-anchor-id="general-computation-with-intermediate-values">General computation with intermediate values</h4>
<p>As a simple example, let’s start with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x,y,z)%20=%20(x+y)z%0A"></p>
<p>It can be convienient to introduce the following intermediate variable</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%20=%20(x%20+%20y)%0A"></p>
<p>We can then write</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af%20=%20pz%0A"></p>
<p>and also compute some partial derivatives</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20q%7D%20=%20z%20%5Cquad%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20z%7D%20=%20q%0A"></p>
<p>and we also know how to differentiate <img src="https://latex.codecogs.com/png.latex?p"> for <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20p%7D%7B%5Cpartial%20x%7D%20=%201%20%5Cquad%20%5Cfrac%7B%5Cpartial%20p%7D%7B%5Cpartial%20y%7D%20=1.%0A"></p>
<p>Using the <em>chain rule</em> we can combine those findings, as the chain rule states that we need to multiply the gradients to chain them:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20f(p,z)%7D%7B%5Cpartial%20x%7D%20=%20%5Cfrac%7B%5Cpartial%20f(p,%20x)%7D%7B%5Cpartial%20p%7D%20%20%5Cfrac%7B%5Cpartial%20p(x,y)%7D%7B%5Cpartial%20x%7D%0A"></p>
<p>This typically means that two numbers are multiplied.</p>
<p>If we try it for the example above we can use the following code. Note how we <em>cache</em> intermediate results (i.e.&nbsp;trade off time- vs.&nbsp;space-complexity).</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># the inputs we will use </span></span>
<span id="cb3-2">x <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb3-3">y <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb3-4">z <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span></span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;"># let's compute our intermediate terms</span></span>
<span id="cb3-7">t1 <span class="op" style="color: #5E5E5E;">=</span> x <span class="op" style="color: #5E5E5E;">+</span> y </span>
<span id="cb3-8">f <span class="op" style="color: #5E5E5E;">=</span> t1 <span class="op" style="color: #5E5E5E;">*</span> z</span></code></pre></div>
</div>
<p>Now, we can look at the derivatives we got above</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">dt1dx <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.</span></span>
<span id="cb4-2">dt1dy <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">dfdt1 <span class="op" style="color: #5E5E5E;">=</span> z</span>
<span id="cb4-5">dfdz <span class="op" style="color: #5E5E5E;">=</span> t1</span></code></pre></div>
</div>
<p>Now, we can use the chain rule to combine them</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">dfdx <span class="op" style="color: #5E5E5E;">=</span> dfdt1 <span class="op" style="color: #5E5E5E;">*</span> dt1dx</span>
<span id="cb5-2">dfdy <span class="op" style="color: #5E5E5E;">=</span> dfdt1 <span class="op" style="color: #5E5E5E;">*</span> dt1dy</span></code></pre></div>
</div>
<p>The sensitivity to <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?y">, and <img src="https://latex.codecogs.com/png.latex?z"> is hence</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="bu" style="color: null;">print</span>(dfdz, dfdy, dfdz)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3 -4.0 3</code></pre>
</div>
</div>
<p>Before we move ahead, realize what we did:</p>
<p>We computed gradients by recursively applying the chain rule, starting at the end:</p>
<ul>
<li>our computation graph is x -&gt; p -&gt; f</li>
<li>we first compute df/dp, then dp/dx. Chaining them gives us df/dx = df/dp dp/dx</li>
</ul>
<p>We can write this in a more general form as follows.</p>
<p>If we assume we have <img src="https://latex.codecogs.com/png.latex?N"> intermediate variables <img src="https://latex.codecogs.com/png.latex?t_N">, with <img src="https://latex.codecogs.com/png.latex?t_N"> being our output <img src="https://latex.codecogs.com/png.latex?f">, by definition we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cmathrm%7Bd%7D%7Bf%7D%7D%7B%5Cmathrm%7Bd%7Dt_N%7D%20=%201%0A"></p>
<p>For the other intermediate variables we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7D%20t_%7Bn-1%7D%7D%20&amp;=%20%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7Df_%7Bn%7D%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bn%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bn-1%7D%7D%20%5C%5C%0A%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7D%20t_%7Bn-2%7D%7D%20&amp;=%20%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7Df_%7Bn%7D%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bn%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bn-1%7D%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bn-1%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bn-2%7D%7D%20%5C%5C%0A%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7D%20t_%7Bn-3%7D%7D%20&amp;=%20%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7Df_%7Bn%7D%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bn%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bn-1%7D%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bn-1%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bn-2%7D%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bn-2%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bn-3%7D%7D%20%5C%5C%0A%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7D%20t_i%7D%20&amp;=%20%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7Df_%7Bn%7D%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bn%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bn-1%7D%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bn-1%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bn-2%7D%7D%20%5Cldots%20%5Cfrac%7B%5Cmathrm%7Bd%7Df_%7Bi+1%7D%7D%7B%5Cmathrm%7Bd%7Dt_%7Bi%7D%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>Note that many of the terms we computed can be reused.</p>
</section>
</section>
<section id="application-to-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="application-to-neural-networks">Application to neural networks</h3>
<p>Neural networks are more complicated circuits – nested functions.</p>
<p>Let’s assume a very simply case</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay=%5Cfrac%7B1%7D%7B1+%5Cexp%20(-(wx+b))%7D.%0A"></p>
<p>We can write it using the chaining of the following primitive operations (forming our computation graph).</p>
<p><img src="https://latex.codecogs.com/png.latex?%0At_1%20=%20wx%0A"> <img src="https://latex.codecogs.com/png.latex?%0At_2%20=%20t_1%20+%20b%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0At_3%20=%20%E2%88%92t_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0At_4%20=%20%5Cexp(t_3)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0At_5%20=%201%20+%20t_4%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0At_6%20=%201/t_5%0A"></p>
<p>(this list of evaluations is sometimes called evaluation trace or Wengert list).</p>
<p>As we would like again get the derivative w.r.t to the output like the loss</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AL%20=%20(t_6-y)%5E2,%0A"></p>
<p>which we can write down with some more evaluations</p>
<p><img src="https://latex.codecogs.com/png.latex?%0At_7%20=%20t_6-t%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0At_8%20=%20t_7%5E2.%0A"></p>
<p>We call this evaluation the <em>forward pass</em>.</p>
<p>The beauty of backprop is that the computation for the derivative follows the same structure as the computation of the function itself (and, for example, is not drastically more complex as one might expect). To see this, we can try out:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_8%7D%20&amp;=%201%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_7%7D%20&amp;=%202%20t_7%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_7%7D%7B%5Cpartial%20t_6%7D%20&amp;%20=%201%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_6%7D%7B%5Cpartial%20t_5%7D%20&amp;=%20%20-1/t_5%5E2%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_5%7D%7B%5Cpartial%20t_4%7D%20&amp;=%201%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_4%7D%7B%5Cpartial%20t_3%7D%20&amp;=%20%5Cexp(t_3)%20t_3%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_3%7D%7B%5Cpartial%20t_2%7D%20&amp;=%20-%201%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_2%7D%7B%5Cpartial%20t_1%7D%20&amp;=%201%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_1%7D%7B%5Cpartial%20w%7D%20&amp;=%20x%0A%5Cend%7Balign%7D%0A"></p>
<p>Armed with those partial derivatives, we can now multiply them to get the final goal – the derivative of the loss w.r.t. the weight (<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w%7D">).</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_6%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_7%7D%20%5Cfrac%7B%5Cpartial%20t_7%7D%7B%5Cpartial%20t_6%7D%20=%202%20t_7%20%5Ccdot%201%20=%202(t_6%20-y)%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_5%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_6%7D%20%5Cfrac%7B%5Cpartial%20t_6%7D%7B%5Cpartial%20t_5%7D%20=%202(t_6%20-y)%20%5Ccdot%20%20%5Cleft(-%5Cfrac%7B1%7D%7Bt_5%5E2%7D%20%5Cright)%20=%20%20-2/t_5%5E2%20(t_6%20-y)%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_4%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_5%7D%20%5Cfrac%7B%5Cpartial%20t_5%7D%7B%5Cpartial%20t_4%7D%20=%20-2/t_5%5E2%20(t_6%20-y)%20%5Ccdot%201%20=%20-2/t_5%5E2%20(t_6%20-y)%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_3%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_4%7D%20%5Cfrac%7B%5Cpartial%20t_4%7D%7B%5Cpartial%20t_3%7D%20=%20-2/t_5%5E2%20(t_6%20-y)%20%5Ccdot%20%5Cexp(t_3)%20t_3%20=%20-2/t_5%5E2%20(t_6%20-y)%20%5Ccdot%20%5Cexp(t_3)%20t_3%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_2%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_3%7D%20%5Cfrac%7B%5Cpartial%20t_3%7D%7B%5Cpartial%20t_2%7D%20=%20-2/t_5%5E2%20(t_6%20-y)%20%5Ccdot%20%5Cexp(t_3)%20t_3%20%5Ccdot%20-1%20=%202/t_5%5E2%20(t_6%20-y)%20%5Ccdot%20%5Cexp(t_3)%20t_3%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_1%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_2%7D%20%5Cfrac%7B%5Cpartial%20t_2%7D%7B%5Cpartial%20t_1%7D%20=%20%202/t_5%5E2%20(t_6%20-y)%20%5Ccdot%20%5Cexp(t_3)%20t_3%20%5C%5C%0A%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20w%7D%20&amp;=%20%5Cfrac%7B%5Cpartial%20t_8%7D%7B%5Cpartial%20t_1%7D%20%5Cfrac%7B%5Cpartial%20t_1%7D%7B%5Cpartial%20w%7D%20=%202/t_5%5E2%20(t_6%20-y)%20%5Ccdot%20%5Cexp(t_3)%20t_3%20%5Ccdot%20x%0A%5Cend%7Balign%7D%0A"></p>
<p>In practice, we would use autodifferentiation using a datastructure as follows to keep track of the computation graph.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;"># code taken from https://github.com/karpathy/micrograd/blob/master/trace_graph.ipynb</span></span>
<span id="cb8-2"><span class="im" style="color: #00769E;">from</span> graphviz <span class="im" style="color: #00769E;">import</span> Digraph</span>
<span id="cb8-3"></span>
<span id="cb8-4"><span class="kw" style="color: #003B4F;">def</span> trace(root):</span>
<span id="cb8-5">    nodes, edges <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(), <span class="bu" style="color: null;">set</span>()</span>
<span id="cb8-6">    <span class="kw" style="color: #003B4F;">def</span> build(v):</span>
<span id="cb8-7">        <span class="cf" style="color: #003B4F;">if</span> v <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> nodes:</span>
<span id="cb8-8">            nodes.add(v)</span>
<span id="cb8-9">            <span class="cf" style="color: #003B4F;">for</span> child <span class="kw" style="color: #003B4F;">in</span> v._prev:</span>
<span id="cb8-10">                edges.add((child, v))</span>
<span id="cb8-11">                build(child)</span>
<span id="cb8-12">    build(root)</span>
<span id="cb8-13">    <span class="cf" style="color: #003B4F;">return</span> nodes, edges</span>
<span id="cb8-14"></span>
<span id="cb8-15"><span class="kw" style="color: #003B4F;">def</span> draw_dot(root, <span class="bu" style="color: null;">format</span><span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'svg'</span>, rankdir<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'LR'</span>):</span>
<span id="cb8-16">    <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb8-17"><span class="co" style="color: #5E5E5E;">    format: png | svg | ...</span></span>
<span id="cb8-18"><span class="co" style="color: #5E5E5E;">    rankdir: TB (top to bottom graph) | LR (left to right)</span></span>
<span id="cb8-19"><span class="co" style="color: #5E5E5E;">    """</span></span>
<span id="cb8-20">    <span class="cf" style="color: #003B4F;">assert</span> rankdir <span class="kw" style="color: #003B4F;">in</span> [<span class="st" style="color: #20794D;">'LR'</span>, <span class="st" style="color: #20794D;">'TB'</span>]</span>
<span id="cb8-21">    nodes, edges <span class="op" style="color: #5E5E5E;">=</span> trace(root)</span>
<span id="cb8-22">    dot <span class="op" style="color: #5E5E5E;">=</span> Digraph(<span class="bu" style="color: null;">format</span><span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">format</span>, graph_attr<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">'rankdir'</span>: rankdir}) <span class="co" style="color: #5E5E5E;">#, node_attr={'rankdir': 'TB'})</span></span>
<span id="cb8-23">    </span>
<span id="cb8-24">    <span class="cf" style="color: #003B4F;">for</span> n <span class="kw" style="color: #003B4F;">in</span> nodes:</span>
<span id="cb8-25">        dot.node(name<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">str</span>(<span class="bu" style="color: null;">id</span>(n)), label <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"{ data </span><span class="sc" style="color: #5E5E5E;">%.4f</span><span class="st" style="color: #20794D;"> | grad </span><span class="sc" style="color: #5E5E5E;">%.4f</span><span class="st" style="color: #20794D;"> }"</span> <span class="op" style="color: #5E5E5E;">%</span> (n.data, n.grad), shape<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'record'</span>)</span>
<span id="cb8-26">        <span class="cf" style="color: #003B4F;">if</span> n._op:</span>
<span id="cb8-27">            dot.node(name<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">str</span>(<span class="bu" style="color: null;">id</span>(n)) <span class="op" style="color: #5E5E5E;">+</span> n._op, label<span class="op" style="color: #5E5E5E;">=</span>n._op)</span>
<span id="cb8-28">            dot.edge(<span class="bu" style="color: null;">str</span>(<span class="bu" style="color: null;">id</span>(n)) <span class="op" style="color: #5E5E5E;">+</span> n._op, <span class="bu" style="color: null;">str</span>(<span class="bu" style="color: null;">id</span>(n)))</span>
<span id="cb8-29">    </span>
<span id="cb8-30">    <span class="cf" style="color: #003B4F;">for</span> n1, n2 <span class="kw" style="color: #003B4F;">in</span> edges:</span>
<span id="cb8-31">        dot.edge(<span class="bu" style="color: null;">str</span>(<span class="bu" style="color: null;">id</span>(n1)), <span class="bu" style="color: null;">str</span>(<span class="bu" style="color: null;">id</span>(n2)) <span class="op" style="color: #5E5E5E;">+</span> n2._op)</span>
<span id="cb8-32">    </span>
<span id="cb8-33">    <span class="cf" style="color: #003B4F;">return</span> dot</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;"># taken from micrograd</span></span>
<span id="cb9-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb9-3"><span class="kw" style="color: #003B4F;">class</span> Value:</span>
<span id="cb9-4">    <span class="co" style="color: #5E5E5E;">""" stores a single scalar value and its gradient """</span></span>
<span id="cb9-5"></span>
<span id="cb9-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, data, _children<span class="op" style="color: #5E5E5E;">=</span>(), _op<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">''</span>):</span>
<span id="cb9-7">        <span class="va" style="color: #111111;">self</span>.data <span class="op" style="color: #5E5E5E;">=</span> data</span>
<span id="cb9-8">        <span class="va" style="color: #111111;">self</span>.grad <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb9-9">        <span class="co" style="color: #5E5E5E;"># internal variables used for autograd graph construction</span></span>
<span id="cb9-10">        <span class="va" style="color: #111111;">self</span>._backward <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span>: <span class="va" style="color: #111111;">None</span></span>
<span id="cb9-11">        <span class="va" style="color: #111111;">self</span>._prev <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(_children)</span>
<span id="cb9-12">        <span class="va" style="color: #111111;">self</span>._op <span class="op" style="color: #5E5E5E;">=</span> _op <span class="co" style="color: #5E5E5E;"># the op that produced this node, for graphviz / debugging / etc</span></span>
<span id="cb9-13"></span>
<span id="cb9-14">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__add__</span>(<span class="va" style="color: #111111;">self</span>, other):</span>
<span id="cb9-15">        other <span class="op" style="color: #5E5E5E;">=</span> other <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(other, Value) <span class="cf" style="color: #003B4F;">else</span> Value(other)</span>
<span id="cb9-16">        out <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="va" style="color: #111111;">self</span>.data <span class="op" style="color: #5E5E5E;">+</span> other.data, (<span class="va" style="color: #111111;">self</span>, other), <span class="st" style="color: #20794D;">'+'</span>)</span>
<span id="cb9-17"></span>
<span id="cb9-18">        <span class="co" style="color: #5E5E5E;"># propagate the gradient on out to parents</span></span>
<span id="cb9-19">        <span class="co" style="color: #5E5E5E;"># i.e. self and other </span></span>
<span id="cb9-20">        <span class="co" style="color: #5E5E5E;"># since out = self + other, then d(out)/dself = 1 and d(out)/dother = 1</span></span>
<span id="cb9-21">        <span class="co" style="color: #5E5E5E;"># so we can just add the gradient to both parents</span></span>
<span id="cb9-22">        <span class="kw" style="color: #003B4F;">def</span> _backward():</span>
<span id="cb9-23">            <span class="va" style="color: #111111;">self</span>.grad <span class="op" style="color: #5E5E5E;">=</span> out.grad</span>
<span id="cb9-24">            other.grad <span class="op" style="color: #5E5E5E;">=</span> out.grad</span>
<span id="cb9-25">        out._backward <span class="op" style="color: #5E5E5E;">=</span> _backward</span>
<span id="cb9-26"></span>
<span id="cb9-27">        <span class="cf" style="color: #003B4F;">return</span> out</span>
<span id="cb9-28"></span>
<span id="cb9-29">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__mul__</span>(<span class="va" style="color: #111111;">self</span>, other):</span>
<span id="cb9-30">        other <span class="op" style="color: #5E5E5E;">=</span> other <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(other, Value) <span class="cf" style="color: #003B4F;">else</span> Value(other)</span>
<span id="cb9-31">        out <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="va" style="color: #111111;">self</span>.data <span class="op" style="color: #5E5E5E;">*</span> other.data, (<span class="va" style="color: #111111;">self</span>, other), <span class="st" style="color: #20794D;">'*'</span>)</span>
<span id="cb9-32"></span>
<span id="cb9-33">        <span class="kw" style="color: #003B4F;">def</span> _backward():</span>
<span id="cb9-34">            <span class="va" style="color: #111111;">self</span>.grad <span class="op" style="color: #5E5E5E;">=</span> other.data <span class="op" style="color: #5E5E5E;">*</span> out.grad</span>
<span id="cb9-35">            other.grad <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.data <span class="op" style="color: #5E5E5E;">*</span> out.grad</span>
<span id="cb9-36">        out._backward <span class="op" style="color: #5E5E5E;">=</span> _backward</span>
<span id="cb9-37"></span>
<span id="cb9-38">        <span class="cf" style="color: #003B4F;">return</span> out</span>
<span id="cb9-39"></span>
<span id="cb9-40">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__pow__</span>(<span class="va" style="color: #111111;">self</span>, other):</span>
<span id="cb9-41">        <span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">isinstance</span>(other, (<span class="bu" style="color: null;">int</span>, <span class="bu" style="color: null;">float</span>)), <span class="st" style="color: #20794D;">"only supporting int/float powers for now"</span></span>
<span id="cb9-42">        out <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="va" style="color: #111111;">self</span>.data<span class="op" style="color: #5E5E5E;">**</span>other, (<span class="va" style="color: #111111;">self</span>,), <span class="ss" style="color: #20794D;">f'**</span><span class="sc" style="color: #5E5E5E;">{</span>other<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb9-43"></span>
<span id="cb9-44">        <span class="kw" style="color: #003B4F;">def</span> _backward():</span>
<span id="cb9-45">            <span class="va" style="color: #111111;">self</span>.grad <span class="op" style="color: #5E5E5E;">=</span> (other <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.data<span class="op" style="color: #5E5E5E;">**</span>(other<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)) <span class="op" style="color: #5E5E5E;">*</span> out.grad</span>
<span id="cb9-46">        out._backward <span class="op" style="color: #5E5E5E;">=</span> _backward</span>
<span id="cb9-47"></span>
<span id="cb9-48">        <span class="cf" style="color: #003B4F;">return</span> out</span>
<span id="cb9-49"></span>
<span id="cb9-50">    <span class="kw" style="color: #003B4F;">def</span> exp(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb9-51">        out <span class="op" style="color: #5E5E5E;">=</span> Value(np.exp(<span class="va" style="color: #111111;">self</span>.data), (<span class="va" style="color: #111111;">self</span>,), <span class="st" style="color: #20794D;">'exp'</span>)</span>
<span id="cb9-52"></span>
<span id="cb9-53">        <span class="kw" style="color: #003B4F;">def</span> _backward():</span>
<span id="cb9-54">            <span class="va" style="color: #111111;">self</span>.grad <span class="op" style="color: #5E5E5E;">=</span> np.exp(<span class="va" style="color: #111111;">self</span>.data) <span class="op" style="color: #5E5E5E;">*</span> out.grad</span>
<span id="cb9-55">        out._backward <span class="op" style="color: #5E5E5E;">=</span> _backward</span>
<span id="cb9-56"></span>
<span id="cb9-57">        <span class="cf" style="color: #003B4F;">return</span> out</span>
<span id="cb9-58"></span>
<span id="cb9-59">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__neg__</span>(<span class="va" style="color: #111111;">self</span>): <span class="co" style="color: #5E5E5E;"># -self</span></span>
<span id="cb9-60">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb9-61"></span>
<span id="cb9-62">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__radd__</span>(<span class="va" style="color: #111111;">self</span>, other): <span class="co" style="color: #5E5E5E;"># other + self</span></span>
<span id="cb9-63">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span> <span class="op" style="color: #5E5E5E;">+</span> other</span>
<span id="cb9-64"></span>
<span id="cb9-65">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__sub__</span>(<span class="va" style="color: #111111;">self</span>, other): <span class="co" style="color: #5E5E5E;"># self - other</span></span>
<span id="cb9-66">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span> <span class="op" style="color: #5E5E5E;">+</span> (<span class="op" style="color: #5E5E5E;">-</span>other)</span>
<span id="cb9-67"></span>
<span id="cb9-68">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__rsub__</span>(<span class="va" style="color: #111111;">self</span>, other): <span class="co" style="color: #5E5E5E;"># other - self</span></span>
<span id="cb9-69">        <span class="cf" style="color: #003B4F;">return</span> other <span class="op" style="color: #5E5E5E;">+</span> (<span class="op" style="color: #5E5E5E;">-</span><span class="va" style="color: #111111;">self</span>)</span>
<span id="cb9-70"></span>
<span id="cb9-71">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__rmul__</span>(<span class="va" style="color: #111111;">self</span>, other): <span class="co" style="color: #5E5E5E;"># other * self</span></span>
<span id="cb9-72">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span> <span class="op" style="color: #5E5E5E;">*</span> other</span>
<span id="cb9-73"></span>
<span id="cb9-74">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__truediv__</span>(<span class="va" style="color: #111111;">self</span>, other): <span class="co" style="color: #5E5E5E;"># self / other</span></span>
<span id="cb9-75">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span> <span class="op" style="color: #5E5E5E;">*</span> other<span class="op" style="color: #5E5E5E;">**-</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb9-76"></span>
<span id="cb9-77">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__rtruediv__</span>(<span class="va" style="color: #111111;">self</span>, other): <span class="co" style="color: #5E5E5E;"># other / self</span></span>
<span id="cb9-78">        <span class="cf" style="color: #003B4F;">return</span> other <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span><span class="op" style="color: #5E5E5E;">**-</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb9-79"></span>
<span id="cb9-80">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb9-81">        <span class="cf" style="color: #003B4F;">return</span> <span class="ss" style="color: #20794D;">f"Value(data=</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>data<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, grad=</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>grad<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">)"</span></span></code></pre></div>
</div>
<p>We can now write down our expression from before using the <code>Value</code> class</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;"># initialize some values</span></span>
<span id="cb10-2">w <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="fl" style="color: #AD0000;">2.0</span>)</span>
<span id="cb10-3">b <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="fl" style="color: #AD0000;">0.0</span>)</span>
<span id="cb10-4"></span>
<span id="cb10-5"><span class="co" style="color: #5E5E5E;"># define the input</span></span>
<span id="cb10-6">x <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="fl" style="color: #AD0000;">1.0</span>)</span>
<span id="cb10-7">target <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="fl" style="color: #AD0000;">10.0</span>)</span>
<span id="cb10-8"></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;"># define the computation</span></span>
<span id="cb10-10">t1 <span class="op" style="color: #5E5E5E;">=</span> w <span class="op" style="color: #5E5E5E;">*</span> x</span>
<span id="cb10-11">t2 <span class="op" style="color: #5E5E5E;">=</span> t1 <span class="op" style="color: #5E5E5E;">+</span> b</span>
<span id="cb10-12">t3 <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">*</span> t2</span>
<span id="cb10-13">t4 <span class="op" style="color: #5E5E5E;">=</span> t3.exp()</span>
<span id="cb10-14">t5 <span class="op" style="color: #5E5E5E;">=</span> t4 <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb10-15">t6 <span class="op" style="color: #5E5E5E;">=</span> t5<span class="op" style="color: #5E5E5E;">**</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb10-16"></span>
<span id="cb10-17">t7 <span class="op" style="color: #5E5E5E;">=</span> t6 <span class="op" style="color: #5E5E5E;">-</span> target</span>
<span id="cb10-18">t8 <span class="op" style="color: #5E5E5E;">=</span> t7<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb10-19"></span>
<span id="cb10-20">draw_dot(t8)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<p><img src="https://kjablonka.com/blog/posts/backprop/index_files/figure-html/cell-10-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>We need to seed the gradient of the loss</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">t8.grad <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.0</span></span></code></pre></div>
</div>
<p>Now, we can perform the backward pass by calling the <code>_backward</code> function of the loss node, which will in turn call the <code>_backward</code> functions of all its parents, and so on, until the entire graph has been visited.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;"># #| </span></span>
<span id="cb12-2">t8._backward()</span>
<span id="cb12-3"><span class="bu" style="color: null;">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 0 0 0 0 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="co" style="color: #5E5E5E;"># #| </span></span>
<span id="cb14-2">t7._backward()</span>
<span id="cb14-3"><span class="bu" style="color: null;">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 0 0 0 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;"># #| </span></span>
<span id="cb16-2">t6._backward()</span>
<span id="cb16-3"><span class="bu" style="color: null;">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 0 0 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="co" style="color: #5E5E5E;"># #| </span></span>
<span id="cb18-2">t5._backward()  </span>
<span id="cb18-3"><span class="bu" style="color: null;">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 14.149418952798422 0 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="co" style="color: #5E5E5E;"># #| </span></span>
<span id="cb20-2">t4._backward()</span>
<span id="cb20-3"><span class="bu" style="color: null;">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 14.149418952798422 1.9149156216104704 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="co" style="color: #5E5E5E;"># #| </span></span>
<span id="cb22-2">t3._backward()</span>
<span id="cb22-3"><span class="bu" style="color: null;">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 14.149418952798422 1.9149156216104704 0 0 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="co" style="color: #5E5E5E;"># #| </span></span>
<span id="cb24-2">t2._backward()</span>
<span id="cb24-3">w._backward()</span>
<span id="cb24-4"><span class="bu" style="color: null;">print</span>(t7.grad, t6.grad, t5.grad, t4.grad, t3.grad, w.grad, b.grad, x.grad)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-18.238405844044234 -18.238405844044234 14.149418952798422 14.149418952798422 1.9149156216104704 0 -1.9149156216104704 0</code></pre>
</div>
</div>
<p>To avoid calling the backward function multiple times, we can implement a <code>backprop</code> function that traverses the graph in reverse topological order and calls the <code>_backward</code> function of each node only once.</p>
<p>Topological sorting can be implemented using the following code</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">topo <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb26-2">visited <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>()</span>
<span id="cb26-3"><span class="kw" style="color: #003B4F;">def</span> build_topo(v):</span>
<span id="cb26-4">    <span class="cf" style="color: #003B4F;">if</span> v <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> visited:</span>
<span id="cb26-5">        visited.add(v)</span>
<span id="cb26-6">        <span class="cf" style="color: #003B4F;">for</span> child <span class="kw" style="color: #003B4F;">in</span> v._prev:</span>
<span id="cb26-7">            build_topo(child)</span>
<span id="cb26-8">        topo.append(v)</span></code></pre></div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Why does this sorting algorithm work?
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The algorithm is a depth-first search (DFS)</li>
<li>The deepest nodes are added to the <code>topo</code> list first</li>
<li>Recursiveness ensures that nodes another node depends on are added first (<code>topo.append</code> only happens after the recursive call)</li>
</ul>
<p>Note that this algorithm does not work for cyclic graphs.</p>
</div>
</div>
<p>Now, we can simply write</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="co" style="color: #5E5E5E;"># #| </span></span>
<span id="cb27-2"><span class="co" style="color: #5E5E5E;"># initialize some values</span></span>
<span id="cb27-3">w <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="fl" style="color: #AD0000;">2.0</span>)</span>
<span id="cb27-4">b <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="fl" style="color: #AD0000;">0.0</span>)</span>
<span id="cb27-5"></span>
<span id="cb27-6"><span class="co" style="color: #5E5E5E;"># define the input</span></span>
<span id="cb27-7">x <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="fl" style="color: #AD0000;">1.0</span>)</span>
<span id="cb27-8">target <span class="op" style="color: #5E5E5E;">=</span> Value(<span class="fl" style="color: #AD0000;">10.0</span>)</span>
<span id="cb27-9"></span>
<span id="cb27-10"><span class="co" style="color: #5E5E5E;"># define the computation</span></span>
<span id="cb27-11">t1 <span class="op" style="color: #5E5E5E;">=</span> w <span class="op" style="color: #5E5E5E;">*</span> x</span>
<span id="cb27-12">t2 <span class="op" style="color: #5E5E5E;">=</span> t1 <span class="op" style="color: #5E5E5E;">+</span> b</span>
<span id="cb27-13">t3 <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">*</span> t2</span>
<span id="cb27-14">t4 <span class="op" style="color: #5E5E5E;">=</span> t3.exp()</span>
<span id="cb27-15">t5 <span class="op" style="color: #5E5E5E;">=</span> t4 <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb27-16">t6 <span class="op" style="color: #5E5E5E;">=</span> t5<span class="op" style="color: #5E5E5E;">**</span>(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb27-17"></span>
<span id="cb27-18">t7 <span class="op" style="color: #5E5E5E;">=</span> t6 <span class="op" style="color: #5E5E5E;">-</span> target</span>
<span id="cb27-19">t8 <span class="op" style="color: #5E5E5E;">=</span> t7<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span></span></code></pre></div>
</div>
<p>And now call the topological sorting and then <code>_backward</code> for all nodes</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">t8.grad <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb28-2"></span>
<span id="cb28-3">build_topo(t8)</span>
<span id="cb28-4"></span>
<span id="cb28-5"><span class="cf" style="color: #003B4F;">for</span> v <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">reversed</span>(topo):</span>
<span id="cb28-6">    v._backward()</span>
<span id="cb28-7"></span>
<span id="cb28-8">w.grad</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>-1.9149156216104704</code></pre>
</div>
</div>
<p>Note that we had to reverse the topological ordering because the deepest dependent of <code>t8</code> was first and we need to work backwards.</p>
</section>
</section>
<section id="lecture" class="level2">
<h2 class="anchored" data-anchor-id="lecture">Lecture</h2>
<p>If you prefer watching a short video over reading you can see me go through the gist of backprop in the following video.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0;">
<iframe src="https://www.loom.com/embed/579ab50060044464832777e6650180f3?sid=0412526a-5a1e-4e24-ab37-691214804dc5" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
</iframe>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ol type="1">
<li><p><a href="https://karpathy.github.io/neuralnets/">Andrej Karpathy “Hacker’s guide to Neural Networks”</a> inspired the comparison between random search and gradient descent. The same ideas are used in the <a href="https://cs231n.github.io/optimization-1/">cs231n lecture notes</a> since he taught this class. The chain rule example is taken from the <a href="https://cs231n.github.io/optimization-2/">c231n lecture notes</a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=PaCmpygFfXo">Andrej Karparthy recorded a lecture in which he builds an autodiff system from scratch</a> and it inspired many parts of the notebooks, some parts (the <code>Value</code> class) are taken from his lecture.</p></li>
<li><p><a href="https://mml-book.github.io/">Deisenroth et al.&nbsp;“Mathematics of Machine Learning”</a> has a beautiful chapter about backprop and autodiff.</p></li>
<li><p><a href="https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6">Mark Saroufim “Automatic Differentiation Step by Step”</a> has an intuitive explaination of dual numbers and has a good resource section, including <a href="https://www.youtube.com/watch?v=Rs0uRQJdIcg&amp;list=WL&amp;index=8&amp;t=149s"></a></p></li>
<li><p><a href="http://arxiv.org/abs/1502.05767">Automatic Differentiation in Machine Learning: a Survey</a> is a great survey that clarifies many terms.</p></li>
<li><p><a href="http://neuralnetworksanddeeplearning.com/chap2.html">Michael Nielsen’s book</a> highlights some of the “hidden” assumptions.</p></li>
<li><p><a href="https://e2eml.school/how_backpropagation_works">Brandon Rohrer</a> has a very intuitive of the chain rule in terms of the shower rate (similar to the bicycle/car/man example above).</p></li>
<li><p><a href="https://dlsyscourse.org/lectures/">Deep Learning Systems Lecture at CMU</a> has a detailed slides on the algorithmic details behind autodiff.</p></li>
<li><p><a href="https://github.com/MikeInnes/diff-zoo/tree/master/src">Differentiation for Hackers</a> has nice Julia code that showcases what makes autodiff special (and different from symbolic and numeric differentiation).</p></li>
<li><p><a href="https://theoryandpractice.org/stats-ds-book/autodiff-tutorial.html">Kyle Cranmer</a> has a useful intro to autodiff. I took the <code>sympy</code> example from there.</p></li>
</ol>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further reading</h2>
<section id="who-invented-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="who-invented-backpropagation">Who “invented” backpropagation</h3>
<p>As with many popular things, there is some debate on “who was first”. You can find some discussion on this <a href="https://people.idsia.ch/~juergen/who-invented-backpropagation.html#BP1">here</a>.</p>
<section id="original-backprop-paper" class="level4">
<h4 class="anchored" data-anchor-id="original-backprop-paper">“Original” Backprop Paper</h4>
<p>In the context of training neural networks, backpropagation was popularized in a beatiful paper by <a href="https://www.nature.com/articles/323533a0">David E. Rumelhart et al.</a> It is beautiful and you should read it.</p>
</section>
</section>
<section id="backpropagation-and-lagrangian" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-and-lagrangian">Backpropagation and Lagrangian</h3>
<p>As <a href="https://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/">this blog post by Tim Viera</a> and <a href="https://arc.net/l/quote/mjznlhvx">this paper by Yann LeCun</a> show, the intermediate variables can be recovered by rephrasing the optimization as a constrained optimization using the Lagrangian framework.</p>
</section>
<section id="forward-vs.-reverse-mode-autodiff" class="level3">
<h3 class="anchored" data-anchor-id="forward-vs.-reverse-mode-autodiff">Forward vs.&nbsp;reverse mode autodiff</h3>
<p>If we have a computation graph as follows</p>
<p><code>x -&gt; a -&gt; b -&gt; y</code></p>
<p>we can compute the derivative of the output with respect to the input as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cmathrm%7Bd%7Dy%7D%7B%5Cmathrm%7Bd%7Dx%7D%20=%20%5Cfrac%7B%5Cmathrm%7Bd%7Dy%7D%7B%5Cmathrm%7Bd%7Db%7D%5Cfrac%7B%5Cmathrm%7Bd%7Db%7D%7B%5Cmathrm%7Bd%7Da%7D%20%5Cfrac%7B%5Cmathrm%7Bd%7Da%7D%7B%5Cmathrm%7Bd%7Dx%7D%0A"></p>
<p>since multiplication is associative, we can choose between computing</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cmathrm%7Bd%7Dy%7D%7B%5Cmathrm%7Bd%7Dx%7D%20=%20%5Cleft(%20%5Cfrac%7B%5Cmathrm%7Bd%7Dy%7D%7B%5Cmathrm%7Bd%7Db%7D%5Cfrac%7B%5Cmathrm%7Bd%7Db%7D%7B%5Cmathrm%7Bd%7Da%7D%20%5Cright)%20%5Cfrac%7B%5Cmathrm%7Bd%7Da%7D%7B%5Cmathrm%7Bd%7Dx%7D%0A"></p>
<p>and <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cmathrm%7Bd%7Dy%7D%7B%5Cmathrm%7Bd%7Dx%7D%20=%20%20%5Cfrac%7B%5Cmathrm%7Bd%7Dy%7D%7B%5Cmathrm%7Bd%7Db%7D%5Cleft(%5Cfrac%7B%5Cmathrm%7Bd%7Db%7D%7B%5Cmathrm%7Bd%7Da%7D%20%20%5Cfrac%7B%5Cmathrm%7Bd%7Da%7D%7B%5Cmathrm%7Bd%7Dx%7D%20%5Cright)%0A"></p>
<p>The first mode is called “reverse mode” autodiff as the gradient flow is opposite to the data flow. The second mode is called “forward mode” autodiff as the order of computation is the same for the gradient computation as for the computation of the function itself.</p>
<p>Backpropagation is a special case of reverse mode autodiff.</p>
<p>Which mode is more efficient depends on whether the input dimension is smaller than the output dimension. If the output dimension is smaller than the input dimension (which is the case for training neural networks) the reverse mode is more efficient as only one application of the reverse mode is needed to compute the gradients.</p>
<p>The forward mode, however is of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO(n)%7D">, where <img src="https://latex.codecogs.com/png.latex?n"> is the number of inputs. If the number of inputs is small (or even just one) and the number of outputs is large, e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%20%5Cto%20%5Cmathbb%7BR%5Em%7D">, then the forward mode will be more efficient.</p>
</section>
<section id="symbolic-differentiation-vs.-numerical-differentiation-vs.-autodiff" class="level3">
<h3 class="anchored" data-anchor-id="symbolic-differentiation-vs.-numerical-differentiation-vs.-autodiff">Symbolic differentiation vs.&nbsp;numerical differentiation vs.&nbsp;autodiff</h3>
<ul>
<li>Numerical differentiation involves computing a term like <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x_i%7D%20%5Capprox%20%5Cfrac%7Bf(x+h)%20-%20f(x)%7D%7Bh%7D"> for a small <img src="https://latex.codecogs.com/png.latex?h">. While this is might be relatively easy to implement, but requires <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO(n)%7D"> evaluations for <img src="https://latex.codecogs.com/png.latex?n"> gradients, and can be numerically unstable (dividing by small number, subtracting two numbers of almost the same value).</li>
<li>Symbolic differentation can be performed with systems like Maple, Sympy, or Mathematica. This gives us <em>expressions</em> for the derivatives, which might grow exponentially large (in blind application).</li>
</ul>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="im" style="color: #00769E;">import</span> sympy </span>
<span id="cb30-2">x <span class="op" style="color: #5E5E5E;">=</span> sympy.symbols(<span class="st" style="color: #20794D;">'x'</span>)</span>
<span id="cb30-3"></span>
<span id="cb30-4"><span class="kw" style="color: #003B4F;">def</span> base_function(x): </span>
<span id="cb30-5">    <span class="cf" style="color: #003B4F;">return</span> x<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">3</span><span class="op" style="color: #5E5E5E;">*</span>x <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">4</span></span></code></pre></div>
</details>
</div>
<ul>
<li>Autodiff can easily deal with control flows</li>
</ul>
</section>
<section id="dual-numbers" class="level3">
<h3 class="anchored" data-anchor-id="dual-numbers">Dual numbers</h3>
<p>Dual numbers are numbers of the form <img src="https://latex.codecogs.com/png.latex?v+%5Cdot%7Bv%7D%5Cepsilon">, where <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> has the special property that it is non-zero and <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%5E2%20=%200">.</p>
<p>They behave as one might expect:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(v+%5Cdot%7Bv%7D%5Cepsilon)%20+%20(u%20+%20%5Cdot%7Bu%7D%5Cepsilon)%20=%20(v%20+%20u)%20+%20(%5Cdot%7Bv%7D%20+%20%5Cdot%7Bu%7D)%5Cepsilon%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(v+%5Cdot%7Bv%7D%5Cepsilon)(u+%5Cdot%7Bu%7D%5Cepsilon)%20=%20(vu)%20+%20(v%5Cdot%7Bu%7D%20+%20%5Cdot%7Bu%7Dv)%5Cepsilon%0A"></p>
<p>Now, keep in mind that the Tyalor series of a function $f(x)</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%20f(a)%20+%20f'(a)(x-a)%20+%20%5Cfrac%7Bf''(a)%7D%7B2!%7D%20(x-a)%5E2%20+%20%5Cfrac%7Bf'''(a)%7D%7B3!%7D%20(x-a)%5E3%0A"></p>
<p>Now, if <img src="https://latex.codecogs.com/png.latex?x%20=%20a+%5Cdot%7Bv%7D%5Cepsilon"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(a%20+%20%5Cdot%7Bv%7D%5Cepsilon)%20=%20f(a)%20+%20f'(a)(a%20+%20%5Cdot%7Bv%7D%5Cepsilon%20-a)%20+%20%20%5Cfrac%7Bf''(a)%7D%7B2!%7D%20(a%20+%20%5Cdot%7Bv%7D%5Cepsilon%20-a)%5E2%20+%20%5Cfrac%7Bf'''(a)%7D%7B3!%7D%20(a%20+%20%5Cdot%7Bv%7D%5Cepsilon%20-a)%5E3%0A"></p>
<p>not that, per definition, all terms with <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%5E2"> or higher powers will vanish. Therefore, we will be left with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af(a%20+%20%5Cdot%7Bv%7D%5Cepsilon)%20=%20f(a)%20+%20f'(a)%5Cdot%7Bv%7D%5Cepsilon%0A"></p>
<p>That is, we can do something like</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft.%20%5Cfrac%7B%5Cmathrm%7Bd%7Df%7D%7B%5Cmathrm%7Bd%7Dx%7D%5Cright%7C_%7Bx=a%7D%20=%20%5Ctext%7Bepsilon%20coefficient%7D(%5Ctext%7Bdual%20version%7D(f)(a+1%5Cepsilon))%0A"></p>
<p>This means that we directly compute f(x) and the derivative (scaled by <img src="https://latex.codecogs.com/png.latex?%5Cdot%7Bv%7D">). Thus, we can simulatanously compute the values of functions and derivatives. A naiive implementation might look as follows</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="im" style="color: #00769E;">import</span> math </span>
<span id="cb31-2"><span class="kw" style="color: #003B4F;">class</span> DualNumber:</span>
<span id="cb31-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, real, dual):</span>
<span id="cb31-4">        <span class="va" style="color: #111111;">self</span>.real <span class="op" style="color: #5E5E5E;">=</span> real  <span class="co" style="color: #5E5E5E;"># Real part</span></span>
<span id="cb31-5">        <span class="va" style="color: #111111;">self</span>.dual <span class="op" style="color: #5E5E5E;">=</span> dual  <span class="co" style="color: #5E5E5E;"># Dual part (coefficient of epsilon)</span></span>
<span id="cb31-6"></span>
<span id="cb31-7">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb31-8">        <span class="cf" style="color: #003B4F;">return</span> <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>real<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> + </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>dual<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">ε"</span></span>
<span id="cb31-9">    </span>
<span id="cb31-10">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__add__</span>(<span class="va" style="color: #111111;">self</span>, other):</span>
<span id="cb31-11">        <span class="co" style="color: #5E5E5E;"># Addition with another DualNumber or scalar</span></span>
<span id="cb31-12">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(other, DualNumber):</span>
<span id="cb31-13">            <span class="cf" style="color: #003B4F;">return</span> DualNumber(<span class="va" style="color: #111111;">self</span>.real <span class="op" style="color: #5E5E5E;">+</span> other.real, <span class="va" style="color: #111111;">self</span>.dual <span class="op" style="color: #5E5E5E;">+</span> other.dual)</span>
<span id="cb31-14">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb31-15">            <span class="cf" style="color: #003B4F;">return</span> DualNumber(<span class="va" style="color: #111111;">self</span>.real <span class="op" style="color: #5E5E5E;">+</span> other, <span class="va" style="color: #111111;">self</span>.dual)</span>
<span id="cb31-16"></span>
<span id="cb31-17">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__mul__</span>(<span class="va" style="color: #111111;">self</span>, other):</span>
<span id="cb31-18">        <span class="co" style="color: #5E5E5E;"># Multiplication with another DualNumber or scalar</span></span>
<span id="cb31-19">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(other, DualNumber):</span>
<span id="cb31-20">            <span class="cf" style="color: #003B4F;">return</span> DualNumber(<span class="va" style="color: #111111;">self</span>.real <span class="op" style="color: #5E5E5E;">*</span> other.real, <span class="va" style="color: #111111;">self</span>.real <span class="op" style="color: #5E5E5E;">*</span> other.dual <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.dual <span class="op" style="color: #5E5E5E;">*</span> other.real)</span>
<span id="cb31-21">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb31-22">            <span class="cf" style="color: #003B4F;">return</span> DualNumber(<span class="va" style="color: #111111;">self</span>.real <span class="op" style="color: #5E5E5E;">*</span> other, <span class="va" style="color: #111111;">self</span>.dual <span class="op" style="color: #5E5E5E;">*</span> other)</span>
<span id="cb31-23">    </span>
<span id="cb31-24">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__radd__</span>(<span class="va" style="color: #111111;">self</span>, other):</span>
<span id="cb31-25">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.<span class="fu" style="color: #4758AB;">__add__</span>(other)</span>
<span id="cb31-26">    </span>
<span id="cb31-27">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__rmul__</span>(<span class="va" style="color: #111111;">self</span>, other):</span>
<span id="cb31-28">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.<span class="fu" style="color: #4758AB;">__mul__</span>(other)</span>
<span id="cb31-29">        </span>
<span id="cb31-30">    <span class="kw" style="color: #003B4F;">def</span> exp(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb31-31">        <span class="co" style="color: #5E5E5E;"># Exponential function</span></span>
<span id="cb31-32">        exp_real <span class="op" style="color: #5E5E5E;">=</span> math.exp(<span class="va" style="color: #111111;">self</span>.real)</span>
<span id="cb31-33">        <span class="cf" style="color: #003B4F;">return</span> DualNumber(exp_real, exp_real <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.dual)</span>
<span id="cb31-34">    </span>
<span id="cb31-35">    <span class="kw" style="color: #003B4F;">def</span> square(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb31-36">        <span class="co" style="color: #5E5E5E;"># Squaring the dual number</span></span>
<span id="cb31-37">        <span class="cf" style="color: #003B4F;">return</span> DualNumber(<span class="va" style="color: #111111;">self</span>.real<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.real <span class="op" style="color: #5E5E5E;">*</span> <span class="va" style="color: #111111;">self</span>.dual)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="kw" style="color: #003B4F;">def</span> complex_function(x):</span>
<span id="cb32-2">    <span class="cf" style="color: #003B4F;">return</span> x.square() <span class="op" style="color: #5E5E5E;">*</span> x.exp() <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">3</span><span class="op" style="color: #5E5E5E;">*</span>x</span>
<span id="cb32-3"></span>
<span id="cb32-4"><span class="co" style="color: #5E5E5E;"># Correcting the differentiation at x = 1</span></span>
<span id="cb32-5">x <span class="op" style="color: #5E5E5E;">=</span> DualNumber(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb32-6">result <span class="op" style="color: #5E5E5E;">=</span> complex_function(x)</span>
<span id="cb32-7"></span>
<span id="cb32-8">result.real, result.dual</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>(5.718281828459045, 11.154845485377136)</code></pre>
</div>
</div>
<p>Which is correct if we check using <a href="https://www.wolframalpha.com/input?i=what+is+the+derivative+of+x%5E2+*+exp(x)+++3x+at+x%3D1">WolframAlpha</a>.</p>
</section>
<section id="differentiating-complex-programs" class="level3">
<h3 class="anchored" data-anchor-id="differentiating-complex-programs">Differentiating complex programs</h3>
<p>Autodiff, and thus differentiable programs, are now becoming a first-class citizen in programming languages—see, for example, the <a href="https://github.com/apple/swift/blob/main/docs/DifferentiableProgramming.md">differentiable programming manifesto</a>.</p>
<p>In the field of computational materials science a few nice examples include</p>
<ul>
<li><a href="https://github.com/jax-md/jax-md">jax-md</a>: Which allows one to differentia through full MD simulations, to do things like <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2024083118">the design of kinetic pathways</a></li>
<li><a href="https://doi.org/10.1063/5.0137103">optimization of a Hückel model implemented in jax</a></li>
<li><a href="https://www.nature.com/articles/s41524-023-01080-x">inverse design of pores</a></li>
</ul>


<!-- -->

</section>
</section>

 ]]></description>
  <category>machine-learning</category>
  <guid>https://kjablonka.com/blog/posts/backprop/index.html</guid>
  <pubDate>Thu, 22 Feb 2024 23:00:00 GMT</pubDate>
</item>
</channel>
</rss>
