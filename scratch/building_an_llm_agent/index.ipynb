{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Building an LLM agent from scratch'\n",
    "description: <i>Developing an understanding for how LLM-based agents work.</i>\n",
    "image: \"\"\n",
    "sidebar: false\n",
    "categories:\n",
    "  - machine-learning\n",
    "  - llm\n",
    "date: \"5/2/2024\"\n",
    "format:\n",
    "  html:\n",
    "    code-tools: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "from litellm.caching import Cache\n",
    "import re\n",
    "from rdkit import Chem\n",
    "litellm.cache = Cache()\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "LLM-powered agents have caught a lot of an attention. \n",
    "They are interesting, because they allow us to couple the flexibility of LLMs with the power of robust tools or knowledge bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In the chemical sciences, this approach has been popularized by [ChemCrow](https://arxiv.org/abs/2304.05376) and [Coscientist](https://www.nature.com/articles/s41586-023-06792-0).\n",
    "In those systems, the LLMs had access to tools such as reaction planner and a cloud laboratory and, in this way, could plan and perform experiments autonomously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "While it might seem that these systems are very complex, they are are surprisingly simple.\n",
    "Unfortunately, this simplicity is sometimes hidden below layers of abstractions in libraries and frameworks.\n",
    "\n",
    "In this post, we will implement a simple agent from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to answer simple questions about molecules (such as the number of hydrogen bond donors) reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply prompt an LLM to answer the question about hydrogen bond donors, it might give us something like the completion shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule  = \"[C@H]([C@@H]([C@@H](C(=O)[O-])O)O)[C@H]C(=O)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "OpenAIException - Traceback (most recent call last):\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py\", line 414, in completion\n    raise e\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py\", line 350, in completion\n    openai_client = openai(\n                    ^^^^^^^\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/openai/_client.py\", line 104, in __init__\n    raise openaiError(\nopenai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py:414\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider)\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py:350\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     openai_client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m    351\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m    352\u001b[0m         base_url\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m    353\u001b[0m         http_client\u001b[38;5;241m=\u001b[39mlitellm\u001b[38;5;241m.\u001b[39mclient_session,\n\u001b[1;32m    354\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    355\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    356\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/openai/_client.py:104\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/main.py:1029\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1025\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1026\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m   1027\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[1;32m   1028\u001b[0m     )\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/main.py:1002\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai_chat_completions\u001b[38;5;241m.\u001b[39mcompletion(\n\u001b[1;32m   1003\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1004\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   1005\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1006\u001b[0m         model_response\u001b[38;5;241m=\u001b[39mmodel_response,\n\u001b[1;32m   1007\u001b[0m         print_verbose\u001b[38;5;241m=\u001b[39mprint_verbose,\n\u001b[1;32m   1008\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   1009\u001b[0m         api_base\u001b[38;5;241m=\u001b[39mapi_base,\n\u001b[1;32m   1010\u001b[0m         acompletion\u001b[38;5;241m=\u001b[39macompletion,\n\u001b[1;32m   1011\u001b[0m         logging_obj\u001b[38;5;241m=\u001b[39mlogging,\n\u001b[1;32m   1012\u001b[0m         optional_params\u001b[38;5;241m=\u001b[39moptional_params,\n\u001b[1;32m   1013\u001b[0m         litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,\n\u001b[1;32m   1014\u001b[0m         logger_fn\u001b[38;5;241m=\u001b[39mlogger_fn,\n\u001b[1;32m   1015\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1016\u001b[0m         custom_prompt_dict\u001b[38;5;241m=\u001b[39mcustom_prompt_dict,\n\u001b[1;32m   1017\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,  \u001b[38;5;66;03m# pass AsyncOpenAI, OpenAI client\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[1;32m   1019\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py:422\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, model, messages, print_verbose, api_key, api_base, acompletion, logging_obj, optional_params, litellm_params, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, message\u001b[38;5;241m=\u001b[39mtraceback\u001b[38;5;241m.\u001b[39mformat_exc())\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py\", line 414, in completion\n    raise e\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py\", line 350, in completion\n    openai_client = OpenAI(\n                    ^^^^^^^\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/openai/_client.py\", line 104, in __init__\n    raise OpenAIError(\nopenai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m9/_txh68y946s4pxy1x2wnd3lh0000gn/T/ipykernel_54520/2910079435.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m completion(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-4-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     messages = [\n\u001b[1;32m      4\u001b[0m         {\n",
      "\u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3213\u001b[0m                     if (\n\u001b[1;32m   3214\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m   3216\u001b[0m                         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3213\u001b[0m                     if (\n\u001b[1;32m   3214\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m   3216\u001b[0m                         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/main.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2197\u001b[0m             )\n\u001b[1;32m   2198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2200\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2201\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   2202\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m             \u001b[0moriginal_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   9116\u001b[0m         ):\n\u001b[1;32m   9117\u001b[0m             \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_all_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9118\u001b[0m         \u001b[0;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9120\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   9116\u001b[0m         ):\n\u001b[1;32m   9117\u001b[0m             \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_all_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9118\u001b[0m         \u001b[0;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9120\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: OpenAIException - Traceback (most recent call last):\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py\", line 414, in completion\n    raise e\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/litellm/llms/openai.py\", line 350, in completion\n    openai_client = openai(\n                    ^^^^^^^\n  File \"/Users/kevinmaikjablonka/miniconda3/envs/gpttutorial/lib/python3.11/site-packages/openai/_client.py\", line 104, in __init__\n    raise openaiError(\nopenai.openaiError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n"
     ]
    }
   ],
   "source": [
    "completion(\n",
    "    model='gpt-4-turbo', \n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'message': f\"What is the number of hydrogen bond donors in the molecule {molecule}?\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x12aeaff40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chem.MolFromSmiles(molecule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## MRKL and ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "One of the most common ways of building LLM powered agents is using the [MRKL](https://arxiv.org/pdf/2205.00445) architecture implemented using the [ReAct](https://arxiv.org/pdf/2210.03629) framework.\n",
    "\n",
    "MRKL describes in a very general way systems that augment LLMs with external knowledge sources and symbolic reasoning. \n",
    "ReAct is a specific prompt that implements MRKL by: \n",
    "\n",
    "- Prompting the model to think \n",
    "- Prompting the model to act \n",
    "- Prompting the model to observe\n",
    "\n",
    "This is inspired by chain-of-thought prompting, which has been shown to be effective in improving the performance of LLMs on a variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Using the ReAct prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "By reading the ReAct paper (or digging [very deep into Langchain's codebase](https://smith.langchain.com/hub/hwchase17/react)), we find that the following text is at the heart of the ReAct framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_PROMPT=\"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "\n",
    "Thought: you should always think about what to do\n",
    "\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "\n",
    "Action Input: the input to the action\n",
    "\n",
    "Observation: the result of the action\n",
    "\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "\n",
    "Thought: I now know the final answer\n",
    "\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Thought:{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we might now be tempted to do is to just send this prompt with a question to OpenAI..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > stop token\n",
    "- > actually calling tool \n",
    "- > abstracting stuff away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Beyond hard-coding prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- As always, there is an [awesome blogpost by Lilian Weng](https://lilianweng.github.io/posts/2023-06-23-agent/). \n",
    "- This blog post was heavily inspired by [Colin Eberhardt's post on implementing LangChain in 100 lines of code](https://blog.scottlogic.com/2023/05/04/langchain-mini.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpttutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
